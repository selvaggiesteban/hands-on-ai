# Orchestrator Principal - Hands On AI (Full Authority System)
# Sistema de control centralizado con acceso total al disco local (Windows 10).
# Integra Comandos, Procesos, Automatizaciones y Templates con UI/UX optimizada.

import os
import sys
import json
import shutil
import asyncio
import subprocess
import glob
from datetime import datetime

# Configuraci√≥n de rutas del sistema
SYSTEM_ROOT = os.path.dirname(os.path.abspath(__file__))
TOOLS_DIR = os.path.join(SYSTEM_ROOT, 'tools')
KB_DIR = os.path.join(SYSTEM_ROOT, 'knowledge_base')
TEMPLATES_DIR = os.path.join(KB_DIR, 'templates')
ENV_FILE = os.path.join(SYSTEM_ROOT, '.env')

# Agregar tools al path
sys.path.insert(0, TOOLS_DIR)

# Intentar importar IA
try:
    from ai_wrapper.multi_model import MultiModelProcessor, MultiModelConfig
    from ai_wrapper.chat_manager import ChatManager
    from ai_wrapper.providers.base import Message
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False

class Orchestrator:
    def __init__(self):
        self.active_root = SYSTEM_ROOT
        self.ai = None
        self.chat_manager = None
        self._init_ai()
        self._init_ui()
        self._init_chat_manager()

    def _init_ai(self):
        if AI_AVAILABLE:
            try:
                # Recargar configuraci√≥n desde .env si existe
                config = MultiModelConfig(parallel_execution=True)
                self.ai = MultiModelProcessor(config)
            except Exception as e:
                self.ai = None

    def _init_chat_manager(self):
        if AI_AVAILABLE:
            # Historial persistente en el proyecto activo
            history_path = os.path.join(self.active_root, "chat_history.json")
            self.chat_manager = ChatManager(history_path)

    def _init_ui(self):
        os.system('cls' if os.name == 'nt' else 'clear')

    # =========================================================================
    #  INTERFAZ Y MEN√öS (UI/UX)
    # =========================================================================

    def show_help(self):
        print("\n" + "‚ïê"*70)
        print("üìñ GU√çA DE OPERACI√ìN - HANDS ON AI (AI-POWERED)")
        print("‚ïê"*70)
        
        print("\nüõ†Ô∏è  [ Comandos ]")
        print("  /root      - Cambiar el directorio ra√≠z de trabajo.")
        print("  /config    - Ver y editar Configuraci√≥n de IA (Keys/Modelos).")
        print("  /chat      - üÜï Iniciar una nueva conversaci√≥n (Borrar memoria).")
        print("  /exit      - Cerrar el orquestador.")

        print("\n‚öôÔ∏è  [ Procesos Inteligentes ]")
        print("  /init      - Deep Research + Memoria -> Generar Plan y Metadatos.")
        print("  /audit     - IA Auditor: Compara C√≥digo vs. Plan.")
        print("  /print     - Ver el Plan Maestro generado.")

        print("\nüìÑ [ Templates Adaptativos ]")
        print("  /templates - Generar entregables adaptados con IA.")

        print("\n‚ö° [ Automatizaciones ]")
        print("  /sync      - Sincronizar Knowledge Base.")
        print("  /run       - Ejecutar Agentes.")

        print("\nüí¨ [ Chat con Contexto ]")
        print("  Escribe directamente para chatear. Usa /chat para reiniciar.")
        print("‚ïê"*70)

    def main_loop(self):
        print("¬°Hola! Sistema Hands-On AI listo. /help para opciones.")
        
        while True:
            try:
                current_dir_name = os.path.basename(self.active_root)
                ai_status = "üü¢" if (self.ai and self.ai.get_available_providers()) else "üî¥"
                
                prompt = input(f"\nüìÇ ({current_dir_name}) {ai_status} > ").strip()
                
                if not prompt: continue
                if prompt.lower() in ['/exit', 'exit', 'salir']:
                    print("üëã ¬°Hasta pronto!")
                    break
                
                asyncio.run(self.handle_prompt(prompt))
            
            except KeyboardInterrupt:
                print("\n‚õî Cancelado.")
            except Exception as e:
                print(f"‚ùå Error: {e}")
            except EOFError:
                break

    async def handle_prompt(self, prompt):
        cmd = prompt.lower().split()[0] if prompt.startswith('/') else None

        if cmd == '/help':      self.show_help()
        elif cmd == '/root':    self.cmd_root()
        elif cmd == '/config':  self.cmd_config()
        elif cmd == '/chat':    self.cmd_new_chat()
        elif cmd == '/init':    await self.cmd_init()
        elif cmd == '/print':   self.cmd_print()
        elif cmd == '/audit':   await self.cmd_audit()
        elif cmd == '/sync':    self.cmd_sync()
        elif cmd == '/run':     self.cmd_run_agent()
        elif cmd == '/templates': await self.cmd_templates()
        elif not prompt.startswith('/'):
            await self.chat_ai(prompt)
        else:
            print(f"‚ö†Ô∏è Comando '{cmd}' no reconocido.")

    # =========================================================================
    #  LOGICA DE NEGOCIO (CORE)
    # =========================================================================

    def cmd_root(self):
        print(f"\nüìç Actual: {self.active_root}")
        path = input("Nueva Ruta > ").strip().strip('"').strip("'")
        if os.path.exists(path) and os.path.isdir(path):
            self.active_root = os.path.abspath(path)
            print(f"‚úÖ Root cambiado a: {self.active_root}")
            self._init_chat_manager() # Recargar memoria del nuevo contexto
        else:
            print("‚ùå Ruta inv√°lida.")

    def cmd_new_chat(self):
        """Reinicia la memoria de conversaci√≥n."""
        if self.chat_manager:
            self.chat_manager.clear_history()
            print("\nüßπ Memoria de conversaci√≥n borrada. Iniciando nuevo chat.")
        else:
            print("‚ö†Ô∏è Chat Manager no inicializado.")

    def cmd_config(self):
        """Configuraci√≥n interactiva de IA."""
        # Cargar configuraci√≥n actual para mostrarla
        current_provider = "Ninguno"
        current_model = "N/A"
        has_key = "NO"

        if os.path.exists(ENV_FILE):
            with open(ENV_FILE, 'r') as f:
                for line in f:
                    if line.startswith("AI_PROVIDER="): current_provider = line.split("=")[1].strip()
                    if line.startswith("AI_MODEL="): current_model = line.split("=")[1].strip()
                    if line.startswith("AI_API_KEY="): 
                        key = line.split("=")[1].strip()
                        if key: has_key = "S√ç (***)"

        print("\nüîê --- Configuraci√≥n de Inteligencia Artificial (Enero 2026) ---")
        print(f"   Estado Actual: {current_provider} | Modelo: {current_model} | Key Configurada: {has_key}")
        print("----------------------------------------------------------------")
        print("Seleccione NUEVO proveedor principal:")
        print("1. OpenAI")
        print("2. Anthropic")
        print("3. Google Gemini")
        
        sel = input("\nOpci√≥n [1-3] (Enter para mantener actual y salir): ").strip()
        if not sel: 
            print("‚úÖ Configuraci√≥n mantenida.")
            return

        provider = ""
        model = ""
        
        if sel == '1':
            provider = "OPENAI"
            print("\nModelos OpenAI:")
            print("  1. o1 (Full Reasoning)")
            print("  2. o1-mini")
            print("  3. gpt-4o (Omni Stable)")
            print("  4. gpt-4o-mini")
            print("  5. Otro (Ingresar ID manual)")
            m = input("Modelo [1-5]: ").strip()
            model_map = {'1': 'o1', '2': 'o1-mini', '3': 'gpt-4o', '4': 'gpt-4o-mini'}
            if m == '5': model = input("ID del modelo: ").strip()
            else: model = model_map.get(m, 'o1')
            
        elif sel == '2':
            provider = "ANTHROPIC"
            print("\nModelos Anthropic:")
            print("  1. claude-opus-4-5-20251101")
            print("  2. claude-sonnet-4-5-20250929")
            print("  3. claude-haiku-4-5-20251001")
            print("  4. claude-3-5-sonnet-latest")
            print("  5. Otro (Ingresar ID manual)")
            m = input("Modelo [1-5]: ").strip()
            model_map = {
                '1': 'claude-opus-4-5-20251101', 
                '2': 'claude-sonnet-4-5-20250929', 
                '3': 'claude-haiku-4-5-20251001', 
                '4': 'claude-3-5-sonnet-latest'
            }
            if m == '5': model = input("ID del modelo: ").strip()
            else: model = model_map.get(m, 'claude-sonnet-4-5-20250929')
            
        elif sel == '3':
            provider = "GEMINI"
            print("\nüîç Consultando modelos disponibles en Google AI...")
            # Intentar listar modelos reales si hay key
            temp_key = input(" (Opcional) Ingrese API KEY temporal para ver lista, o Enter para saltar: ").strip()
            
            real_models = []
            if temp_key and AI_AVAILABLE:
                try:
                    import google.generativeai as genai
                    genai.configure(api_key=temp_key)
                    for m in genai.list_models():
                        if 'generateContent' in m.supported_generation_methods:
                            real_models.append(m.name.replace('models/', ''))
                except: pass

            if real_models:
                print("\n‚úÖ Modelos Disponibles para tu cuenta:")
                for i, rm in enumerate(real_models, 1):
                    print(f"  {i}. {rm}")
                m = input(f"Modelo [1-{len(real_models)}]: ").strip()
                if m.isdigit() and 1 <= int(m) <= len(real_models):
                    model = real_models[int(m)-1]
                else:
                    model = "gemini-1.5-flash"
            else:
                print("\nModelos Google (Est√°ndar):")
                print("  1. gemini-1.5-pro (Recomendado)")
                print("  2. gemini-1.5-flash (R√°pido)")
                print("  3. gemini-1.0-pro")
                print("  4. Otro (Manual)")
                m = input("Modelo [2]: ").strip()
                model_map = {'1': 'gemini-1.5-pro', '2': 'gemini-1.5-flash', '3': 'gemini-1.0-pro'}
                if m == '4': model = input("ID del modelo: ").strip()
                else: model = model_map.get(m, 'gemini-1.5-flash')
        
        else:
            print("‚ùå Selecci√≥n inv√°lida.")
            return

        api_key = input(f"\nüîë Ingrese su API KEY para {provider}: ").strip()
        if not api_key:
            print("‚ùå API Key vac√≠a. Cancelado.")
            return

        # Guardar en .env
        try:
            with open(ENV_FILE, 'w', encoding='utf-8') as f:
                f.write(f"AI_PROVIDER={provider}\n")
                f.write(f"AI_MODEL={model}\n")
                f.write(f"AI_API_KEY={api_key}\n")
            
            print(f"\n‚úÖ Configuraci√≥n guardada en {ENV_FILE}")
            print(f"   Proveedor: {provider}")
            print(f"   Modelo: {model}")
            
            # Recargar IA
            print("üîÑ Reiniciando motor de IA...")
            self._init_ai()
            if self.ai and self.ai.get_available_providers():
                print("üü¢ IA Conectada y Lista.")
            else:
                print("üî¥ IA Configurada pero NO Conectada (Revise Key o Dependencias).")
                
        except Exception as e:
            print(f"‚ùå Error guardando configuraci√≥n: {e}")

    async def cmd_init(self):
        """
        PROCESO CENTRAL DE INTELIGENCIA
        1. Deep Research (Escaneo de archivos)
        2. Context Memory (Historial de Chat)
        3. Strategic Planning (Generaci√≥n de JSONs)
        """
        print(f"\nüöÄ [IA] Iniciando An√°lisis Profundo en: {self.active_root}")
        
        if not self.ai:
            print("‚ùå IA no activa. Use /config.")
            return

        # 1. Scaffolding Base
        target_meta = os.path.join(self.active_root, 'project_meta')
        if not os.path.exists(target_meta):
            print("üì¶ Desplegando estructura base...")
            try:
                shutil.copytree(os.path.join(SYSTEM_ROOT, 'project_meta'), target_meta)
            except: pass

        # 2. Recolecci√≥n de Datos (Research + Memory)
        print("üîç Deep Research: Leyendo c√≥digo fuente...")
        code_context = self._scan_directory(self.active_root)
        
        print("üß† Memory Recall: Consultando historial de conversaci√≥n...")
        memory_context = ""
        if self.chat_manager:
            msgs = self.chat_manager.get_full_history()
            # Tomar los √∫ltimos 10 turnos relevantes
            relevant_msgs = msgs[-10:] if len(msgs) > 10 else msgs
            memory_context = "\n".join([f"{m.role.upper()}: {m.content}" for m in relevant_msgs])

        # 3. Prompt de Ingenier√≠a
        print("ü§ñ Razonando Estrategia (Code + Context)...")
        prompt = f"""
        ACT√öA COMO UN CTO Y ARQUITECTO DE SOFTWARE PRINCIPAL.
        
        CONTEXTO DEL C√ìDIGO FUENTE (ESTADO ACTUAL):
        {code_context[:40000]}
        
        CONTEXTO DE LA CONVERSACI√ìN CON EL USUARIO (PREFERENCIAS Y OBJETIVOS):
        {memory_context}
        
        TAREA:
        Actualiza los metadatos del proyecto ('product-overview' y 'plan') reflejando la realidad del c√≥digo 
        PERO alineada con los deseos expresados por el usuario en el chat.
        
        Si el usuario pidi√≥ algo (ej: "Usar Python") y no est√° en el c√≥digo, agr√©galo como Tarea Pendiente en el Plan.

        FORMATO JSON √öNICO:
        {{
            "overview": {{ "title": "...", "description": "...", "goals": [] }},
            "plan": {{
                "techStack": {{ "languages": [], "frameworks": [] }},
                "phases": [
                    {{ "name": "Fase 1: An√°lisis", "tasks": ["..."] }},
                    {{ "name": "Fase 2: Implementaci√≥n", "tasks": ["..."] }}
                ]
            }}
        }}
        """
        
        try:
            resp = await self.ai.process_single(prompt)
            data = self._extract_json(resp.content)
            
            if data:
                self._save_json(data.get('overview'), os.path.join(target_meta, 'product_overview', 'product-overview.json'))
                self._save_json(data.get('plan'), os.path.join(target_meta, 'planning', 'plan.json'))
                print("‚úÖ [IA] Estrategia Actualizada y Sincronizada.")
                self.cmd_print()
            else:
                print("‚ùå La IA no gener√≥ una estructura v√°lida.")
        except Exception as e:
            print(f"‚ùå Error cr√≠tico en IA: {e}")

    async def cmd_audit(self):
        """Auditor√≠a T√©cnica con IA"""
        print("\nüïµÔ∏è [IA Auditor] Iniciando revisi√≥n de consistencia...")
        
        plan_path = os.path.join(self.active_root, 'project_meta', 'planning', 'plan.json')
        if not os.path.exists(plan_path):
            print("‚ö†Ô∏è No existe plan.json. Ejecuta /init.")
            return

        code_context = self._scan_directory(self.active_root, max_chars=20000)
        with open(plan_path, 'r', encoding='utf-8') as f:
            plan_content = f.read()

        prompt = f"""
        ERES UN AUDITOR T√âCNICO RIGUROSO.
        
        PLAN PROMETIDO (plan.json):
        {plan_content}
        
        REALIDAD DEL C√ìDIGO:
        {code_context}
        
        TAREA:
        Genera un reporte de auditor√≠a breve.
        1. ¬øQu√© tecnolog√≠as prometidas NO est√°n instaladas/detectadas?
        2. ¬øQu√© tareas marcadas como hechas parecen incompletas?
        3. Calificaci√≥n de cumplimiento (0-100%).
        """
        
        if self.ai:
            resp = await self.ai.process_single(prompt)
            print("\nüìä --- REPORTE DE AUDITOR√çA IA ---")
            print(resp.content)
            print("-----------------------------------")

    async def cmd_templates(self):
        """Generaci√≥n + Adaptaci√≥n IA"""
        print("\nüìÑ --- Templates ---")
        if not os.path.exists(TEMPLATES_DIR):
            print("‚ùå Error: No templates dir.")
            return

        items = os.listdir(TEMPLATES_DIR)
        for i, t in enumerate(items, 1): print(f"{i}. {t}")
        
        sel = input("Template # o 'A': ").strip().upper()
        targets = items if sel == 'A' else ([items[int(sel)-1]] if sel.isdigit() else [])
        
        for t in targets:
            src = os.path.join(TEMPLATES_DIR, t)
            dst = os.path.join(self.active_root, t)
            if not os.path.exists(dst):
                shutil.copytree(src, dst)
                print(f"‚úÖ Copiado: {t}")
                
                # ADAPTACI√ìN IA
                do_adapt = input(f"ü§ñ ¬øDeseas que la IA adapte '{t}' a tu proyecto? (s/n): ").lower()
                if do_adapt == 's' and self.ai:
                    print("   ‚ú® Adaptando contenido...")
                    # Leer archivos clave del template copiado (ej. READMEs o .md)
                    for root, _, files in os.walk(dst):
                        for file in files:
                            if file.endswith('.md'):
                                fpath = os.path.join(root, file)
                                with open(fpath, 'r', encoding='utf-8') as f: content = f.read()
                                
                                prompt = f"ADAPTA este template al proyecto actual (Contexto: {os.path.basename(self.active_root)}).\n\nTEMPLATE:\n{content[:5000]}"
                                resp = await self.ai.process_single(prompt)
                                
                                with open(fpath, 'w', encoding='utf-8') as f: f.write(resp.content)
                                print(f"   ‚ú® {file} reescrito por IA.")

    # --- Helpers y Chat Standard ---
    
    async def chat_ai(self, prompt):
        if not self.ai: return print("‚ùå Configura IA primero.")
        
        if self.chat_manager: self.chat_manager.add_message('user', prompt)
        
        # Inyecci√≥n de Contexto Profundo
        msgs = []
        # 1. System Prompt con info del entorno
        sys_p = f"Proyecto: {self.active_root}. Eres un asistente experto Hands-On AI."
        
        # 2. Historial Reciente
        if self.chat_manager:
            hist = self.chat_manager.get_context(limit=10)
            for h in hist: msgs.append(Message(role=h['role'], content=h['content']))
        else:
            msgs.append(Message(role='user', content=prompt))
            
        try:
            resp = await self.ai.process_single(messages=msgs, system_prompt=sys_p)
            if self.chat_manager: self.chat_manager.add_message('assistant', resp.content)
            print(f"\nü§ñ {resp.content}\n")
        except Exception as e:
            print(f"‚ùå Error: {e}")

    def cmd_print(self):
        """Genera un reporte legible del Plan Maestro, sin JSON."""
        plan_path = os.path.join(self.active_root, 'project_meta', 'planning', 'plan.json')
        overview_path = os.path.join(self.active_root, 'project_meta', 'product_overview', 'product-overview.json')

        if not os.path.exists(plan_path) or not os.path.exists(overview_path):
            print("‚ùå No se han generado los metadatos. Ejecuta /init primero.")
            return

        try:
            with open(overview_path, 'r', encoding='utf-8') as f:
                overview_data = json.load(f)
            
            with open(plan_path, 'r', encoding='utf-8') as f:
                plan_data = json.load(f)

            title = overview_data.get('title', 'Proyecto Sin T√≠tulo')
            description = overview_data.get('description', 'Sin descripci√≥n.')
            
            phases = plan_data.get('phases', [])
            stack = plan_data.get('techStack', {})

            print("\n" + "‚ïê"*60)
            print(f"üìã PLAN MAESTRO: {title}")
            print("‚ïê"*60)
            print(f"\nüéØ OBJETIVO: {description}\n")

            if stack:
                print("üõ†Ô∏è  STACK TECNOL√ìGICO PROPUESTO:")
                for category, techs in stack.items():
                    tech_list = ", ".join(techs) if isinstance(techs, list) else techs
                    print(f"  ‚Ä¢ {category.capitalize()}: {tech_list}")
                print("‚îÄ"*60)

            print("üìÖ FASES DE IMPLEMENTACI√ìN:")
            if not phases:
                print("  No se han definido fases de implementaci√≥n.")
            for i, p in enumerate(phases, 1):
                name = p.get('name', 'Fase sin nombre')
                tasks = p.get('tasks', [])
                print(f"\nüîπ FASE {i}: {name}")
                if tasks:
                    for t in tasks: print(f"   [ ] {t}")
                else:
                    print("   - No hay tareas definidas para esta fase.")
            print("‚ïê"*60)
            
        except Exception as e:
            print(f"‚ùå Error al generar el reporte del plan: {e}")
    
    def cmd_sync(self):
        # (Stub)
        print("üîÑ Sync ejecutado.")

    def cmd_run_agent(self):
        # (Stub)
        print("üïµÔ∏è Agentes listos.")

    def _scan_directory(self, path, max_chars=50000):
        buffer = []
        total = 0
        ignore = ['.git', 'node_modules', '__pycache__', 'dist', 'project_meta']
        for root, dirs, files in os.walk(path):
            dirs[:] = [d for d in dirs if d not in ignore]
            for file in files:
                if file.endswith(('.py', '.js', '.md', '.json', '.txt')):
                    try:
                        with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                            c = f.read()
                            buffer.append(f"FILE {file}:\n{c}\n")
                            total += len(c)
                            if total > max_chars: return "\n".join(buffer)
                    except: pass
        return "\n".join(buffer)

    def _extract_json(self, text):
        try:
            if "```json" in text: text = text.split("```json")[1].split("```")[0]
            elif "```" in text: text = text.split("```")[1].split("```")[0]
            return json.loads(text.strip())
        except: return None

    def _save_json(self, data, path):
        if data:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, 'w', encoding='utf-8') as f: json.dump(data, f, indent=2)

if __name__ == "__main__":
    app = Orchestrator()
    try:
        app.main_loop()
    except KeyboardInterrupt: pass
