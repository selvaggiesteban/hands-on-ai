#!/usr/bin/env python3
"""
SEO Content Generator - Advanced SEO Analysis and Content Creation Tool
Analyzes websites and generates complete SEO content strategies including:
- Website analysis from sitemaps and URL lists
- Business brief processing and completion
- Competitive research via Google/Bing scraping
- SEO-optimized content generation
- Comprehensive Excel reports with 14 specialized sheets

Supports two operation modes:
1. New Site Mode: Process business brief and generate complete SEO strategy
2. Existing Site Mode: Analyze current site and generate optimization recommendations

Required packages: requests, beautifulsoup4, lxml, openpyxl, python-docx, PyPDF2,
selenium, webdriver-manager, nltk, textblob
"""

import sys
import subprocess

def install_requirements():
    """Install required packages if not available"""
    required_packages = [
        ("requests", "requests>=2.25.1"),
        ("bs4", "beautifulsoup4>=4.9.3"),
        ("lxml", "lxml>=4.6.3"),
        ("openpyxl", "openpyxl>=3.0.0"),
        ("docx", "python-docx>=0.8.11"),
        ("PyPDF2", "PyPDF2>=3.0.1"),
        ("selenium", "selenium>=4.15.0"),
        ("webdriver_manager", "webdriver-manager>=4.0.1"),
        ("nltk", "nltk>=3.8.1"),
        ("textblob", "textblob>=0.17.1")
    ]

    missing_packages = []

    for import_name, package_name in required_packages:
        try:
            __import__(import_name)
        except ImportError:
            missing_packages.append(package_name)

    if missing_packages:
        print(f"Installing missing packages: {', '.join(missing_packages)}")
        try:
            subprocess.check_call([
                sys.executable, "-m", "pip", "install"
            ] + missing_packages)
            print("Packages installed successfully!")
        except subprocess.CalledProcessError as e:
            print(f"Error installing packages: {e}")
            print("Please install manually with: pip install " + " ".join(missing_packages))
            sys.exit(1)

# Install requirements before importing other modules
install_requirements()

import requests
import urllib3
import xml.etree.ElementTree as ET
import argparse
import os
import time
from urllib.parse import urlparse, urljoin

# Disable SSL warnings for sites with certificate errors
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from bs4 import BeautifulSoup
from collections import Counter, defaultdict
from datetime import datetime
import re
import json
import logging
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.cell.cell import MergedCell

# New imports for enhanced functionality
from docx import Document
import PyPDF2
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import nltk
from textblob import TextBlob
import random
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SEOContentGenerator:
    def __init__(self, delay=1):
        self.delay = delay
        self.analyzed_urls = []
        self.content_data = []
        self.keywords = Counter()
        self.topics = defaultdict(list)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        # Disable SSL certificate verification for sites with certificate errors
        self.session.verify = False

        # Cache configuration
        self.cache_dir = Path('.seo_cache')
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_rescrape_minutes = 60   # Re-scrape every: 60 minutes (OBLIGATORIO)
        self.cache_cleanup_hours = 24      # Delete cache after: 24 hours
        self.current_domain = None         # Current domain being analyzed
        self.domain_cache_dir = None       # Cache directory for current domain

        # New properties for content generation
        self.operation_mode = None  # 'new_site' or 'existing_site'
        self.business_data = {}
        self.brief_template = self._load_brief_template()
        self.competitive_analysis = {}
        self.generated_content = {}
        self.serp_data = {}
        self.driver = None  # Selenium WebDriver

        # URL Classification and Filtering
        self.url_classifications = {}  # Store URL type classifications
        self.site_ctas = []  # Store detected CTAs from site

    def _load_brief_template(self):
        """Load the embedded brief template"""
        return {
            'general_info': {
                'nombre_comercial': '',
                'descripcion_corta': '',
                'actividad_principal': '',
                'cobertura_geografica': '',
                'medios_pago': '',
                'diferenciales': [],
                'objetivo_principal': '',
                'cta_global': ''
            },
            'locations': [],
            'social_media': {
                'instagram': '',
                'facebook': '',
                'linkedin': '',
                'twitter': '',
                'pinterest': '',
                'tiktok': ''
            },
            'brand': {
                'logo': '',
                'colores': '',
                'tipografias': ''
            },
            'pages': {
                'home': {},
                'sobre_nosotros': {},
                'servicios': {},
                'blog': {},
                'contacto': {}
            },
            'blog_posts': []
        }

    def _should_exclude_url(self, url):
        """Filter out WordPress/Elementor non-content URLs"""
        url_lower = url.lower()

        # Exclude patterns (WordPress, Elementor, taxonomies, archives)
        exclude_patterns = [
            '?elementor_library=',
            '/elementor-',
            '/category/',
            '/tag/',
            '/author/',
            '/search',
            '/page/',
            '/feed/',
            '/wp-json/',
            '/wp-content/',
            '/wp-includes/',
            'sitemap',
            '/attachment/',
            '?p=',
            '?page_id=',
            '/trackback',
            '/comment-page-',
            '/print/',
            '/embed/',
            '?replytocom='
        ]

        # Check if URL matches any exclude pattern
        for pattern in exclude_patterns:
            if pattern in url_lower:
                return True

        # Exclude archive patterns like /2025/04/ without meaningful slug
        archive_pattern = r'/\d{4}/\d{2}/?$'
        if re.search(archive_pattern, url):
            return True

        return False

    def _classify_url_type(self, url):
        """Classify URL by type based on friendly URL patterns"""
        url_lower = url.lower()
        parsed = urlparse(url)
        path = parsed.path.lower()

        # Homepage
        if path in ['/', '', '/index.html', '/index.php']:
            return 'homepage'

        # Blog/Articles - improved detection
        blog_indicators = ['/blog/', '/articulo/', '/post/', '/noticia/', '/news/']
        if any(indicator in path for indicator in blog_indicators):
            return 'blog'

        # Date pattern in URL (typically blog posts) like /2024/05/post-title
        if re.search(r'/\d{4}/\d{2}/.+', path):
            return 'blog'

        # Services
        service_indicators = ['/servicio', '/service', '/tratamiento', '/treatment', '/especialidad']
        if any(indicator in path for indicator in service_indicators):
            return 'service'

        # Products
        product_indicators = ['/producto', '/product', '/tienda', '/shop', '/catalog']
        if any(indicator in path for indicator in product_indicators):
            return 'product'

        # About/Company pages
        about_indicators = ['/sobre-', '/nosotros', '/about', '/empresa', '/quienes', '/equipo', '/team']
        if any(indicator in path for indicator in about_indicators):
            return 'about'

        # Contact
        contact_indicators = ['/contacto', '/contact', '/cita', '/appointment', '/reserva']
        if any(indicator in path for indicator in contact_indicators):
            return 'contact'

        # Legal pages
        legal_indicators = ['/aviso', '/legal', '/privacy', '/privacidad', '/terminos', '/terms', '/cookies']
        if any(indicator in path for indicator in legal_indicators):
            return 'legal'

        # FAQ/Help
        if any(word in path for word in ['/faq', '/ayuda', '/help', '/preguntas']):
            return 'faq'

        # Default: general page
        return 'page'

    def _classify_all_urls(self, urls):
        """Classify all URLs and store in url_classifications"""
        classified = {
            'homepage': [],
            'blog': [],
            'service': [],
            'product': [],
            'about': [],
            'contact': [],
            'faq': [],
            'legal': [],
            'page': []
        }

        for url in urls:
            if not self._should_exclude_url(url):
                url_type = self._classify_url_type(url)
                classified[url_type].append(url)
                self.url_classifications[url] = url_type

        # Log classification summary
        logger.info("ðŸ“Š URL Classification Summary:")
        for url_type, url_list in classified.items():
            if url_list:
                logger.info(f"  - {url_type.title()}: {len(url_list)} URLs")

        return classified

    def _get_industry_knowledge_banks(self):
        """Knowledge banks for 20+ industries with common questions, services, and terminology"""
        return {
            'fisioterapia': {
                'keywords': ['fisioterapia', 'fisioterapeuta', 'rehabilitaciÃ³n', 'lesiÃ³n', 'dolor', 'tratamiento', 'terapia', 'recuperaciÃ³n'],
                'services': ['rehabilitaciÃ³n', 'masajes', 'electroterapia', 'ejercicios terapÃ©uticos', 'vendaje funcional', 'punciÃ³n seca'],
                'common_questions': [
                    'Â¿CuÃ¡ntas sesiones necesito?',
                    'Â¿CuÃ¡nto dura una sesiÃ³n de fisioterapia?',
                    'Â¿El tratamiento es doloroso?',
                    'Â¿CuÃ¡nto tiempo tarda la recuperaciÃ³n?',
                    'Â¿Aceptan seguro mÃ©dico?'
                ],
                'h2_templates': [
                    'Tipos de {keyword} que tratamos',
                    'Proceso de recuperaciÃ³n y rehabilitaciÃ³n',
                    'TÃ©cnicas de {keyword} avanzada',
                    'Beneficios del tratamiento {keyword}',
                    'Precio y coste de las sesiones'
                ]
            },
            'dentista': {
                'keywords': ['dental', 'dentista', 'odontologÃ­a', 'dientes', 'sonrisa', 'ortodoncia', 'implante'],
                'services': ['ortodoncia', 'implantes', 'blanqueamiento', 'endodoncia', 'periodoncia', 'cirugÃ­a oral'],
                'common_questions': [
                    'Â¿Duele el tratamiento dental?',
                    'Â¿CuÃ¡nto cuesta un implante dental?',
                    'Â¿Cada cuÃ¡nto debo ir al dentista?',
                    'Â¿CuÃ¡nto dura el blanqueamiento dental?',
                    'Â¿Aceptan financiaciÃ³n?'
                ],
                'h2_templates': [
                    'Tratamientos de {keyword} disponibles',
                    'Precio de {keyword} en [ciudad]',
                    'TecnologÃ­a dental de Ãºltima generaciÃ³n',
                    'Â¿Por quÃ© elegir nuestra clÃ­nica dental?',
                    'Primera consulta gratuita'
                ]
            },
            'abogado': {
                'keywords': ['abogado', 'legal', 'jurÃ­dico', 'derecho', 'abogacÃ­a', 'asesorÃ­a', 'bufete'],
                'services': ['derecho civil', 'derecho penal', 'derecho laboral', 'divorcio', 'herencias', 'reclamaciones'],
                'common_questions': [
                    'Â¿CuÃ¡nto cobra un abogado?',
                    'Â¿CuÃ¡nto dura el proceso legal?',
                    'Â¿Ofrecen primera consulta gratuita?',
                    'Â¿QuÃ© documentos necesito?',
                    'Â¿Trabajan con tarifa plana?'
                ],
                'h2_templates': [
                    'EspecializaciÃ³n en {keyword}',
                    'Casos de Ã©xito en {keyword}',
                    'Proceso y plazos del trÃ¡mite legal',
                    'Honorarios y formas de pago',
                    'Contacta con nuestro despacho'
                ]
            },
            'restaurante': {
                'keywords': ['restaurante', 'comida', 'gastronomÃ­a', 'menÃº', 'cocina', 'chef', 'reserva'],
                'services': ['comida a domicilio', 'eventos privados', 'menÃº del dÃ­a', 'catering', 'reservas'],
                'common_questions': [
                    'Â¿CuÃ¡l es el horario del restaurante?',
                    'Â¿Tienen menÃº del dÃ­a?',
                    'Â¿Se puede reservar mesa?',
                    'Â¿Tienen opciones veganas/vegetarianas?',
                    'Â¿Hacen comida para llevar?'
                ],
                'h2_templates': [
                    'Nuestra carta y especialidades',
                    'MenÃº del dÃ­a {keyword}',
                    'Reserva tu mesa en [restaurante]',
                    'Eventos y celebraciones privadas',
                    'Delivery y comida a domicilio'
                ]
            },
            'gimnasio': {
                'keywords': ['gimnasio', 'fitness', 'entrenamiento', 'crossfit', 'musculaciÃ³n', 'cardio'],
                'services': ['entrenamiento personal', 'clases dirigidas', 'nutriciÃ³n', 'spinning', 'yoga', 'pilates'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta la cuota mensual?',
                    'Â¿Tienen entrenador personal?',
                    'Â¿CuÃ¡l es el horario del gimnasio?',
                    'Â¿Hay clases dirigidas incluidas?',
                    'Â¿Ofrecen clase de prueba gratuita?'
                ],
                'h2_templates': [
                    'Instalaciones y equipamiento del gimnasio',
                    'Clases dirigidas de {keyword}',
                    'Entrenamiento personalizado',
                    'Planes y tarifas mensuales',
                    'Horarios y ubicaciÃ³n'
                ]
            },
            'psicologia': {
                'keywords': ['psicÃ³logo', 'psicologÃ­a', 'terapia', 'ansiedad', 'depresiÃ³n', 'coaching'],
                'services': ['terapia individual', 'terapia de pareja', 'terapia infantil', 'coaching', 'ansiedad', 'depresiÃ³n'],
                'common_questions': [
                    'Â¿CuÃ¡nto dura una sesiÃ³n de psicologÃ­a?',
                    'Â¿CuÃ¡ntas sesiones voy a necesitar?',
                    'Â¿Es confidencial la terapia?',
                    'Â¿Hacen terapia online?',
                    'Â¿CuÃ¡nto cuesta una consulta?'
                ],
                'h2_templates': [
                    'Tipos de terapia {keyword}',
                    'Tratamiento para {keyword}',
                    'Beneficios de la terapia psicolÃ³gica',
                    'MetodologÃ­a y enfoque terapÃ©utico',
                    'Reserva tu primera consulta'
                ]
            },
            'inmobiliaria': {
                'keywords': ['inmobiliaria', 'piso', 'casa', 'alquiler', 'venta', 'vivienda', 'propiedad'],
                'services': ['venta de pisos', 'alquiler', 'tasaciÃ³n', 'gestiÃ³n de inmuebles', 'inversiÃ³n'],
                'common_questions': [
                    'Â¿CuÃ¡nto cobran de comisiÃ³n?',
                    'Â¿Tienen pisos en alquiler?',
                    'Â¿Hacen tasaciones gratuitas?',
                    'Â¿CuÃ¡nto tiempo tarda en venderse una casa?',
                    'Â¿Ayudan con la hipoteca?'
                ],
                'h2_templates': [
                    'Pisos en venta en [ciudad]',
                    'Alquiler de viviendas {keyword}',
                    'Proceso de compra paso a paso',
                    'TasaciÃ³n gratuita de tu propiedad',
                    'Por quÃ© elegirnos como tu inmobiliaria'
                ]
            },
            'veterinario': {
                'keywords': ['veterinario', 'veterinaria', 'mascota', 'perro', 'gato', 'animal'],
                'services': ['consulta veterinaria', 'vacunaciÃ³n', 'cirugÃ­a', 'urgencias', 'peluquerÃ­a canina'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta la consulta veterinaria?',
                    'Â¿Tienen servicio de urgencias?',
                    'Â¿Hacen cirugÃ­as?',
                    'Â¿CuÃ¡ndo debo vacunar a mi mascota?',
                    'Â¿Atienden urgencias 24 horas?'
                ],
                'h2_templates': [
                    'Servicios veterinarios para {keyword}',
                    'Urgencias veterinarias 24h',
                    'VacunaciÃ³n y prevenciÃ³n',
                    'Precio de consulta y tratamientos',
                    'Cuidado integral de tu mascota'
                ]
            },
            'hotel': {
                'keywords': ['hotel', 'alojamiento', 'habitaciÃ³n', 'hospedaje', 'turismo', 'resort'],
                'services': ['habitaciones', 'spa', 'restaurante', 'piscina', 'eventos', 'wifi'],
                'common_questions': [
                    'Â¿CuÃ¡l es el horario de check-in?',
                    'Â¿Tienen wifi gratuito?',
                    'Â¿Admiten mascotas?',
                    'Â¿El desayuno estÃ¡ incluido?',
                    'Â¿Se puede cancelar la reserva?'
                ],
                'h2_templates': [
                    'Habitaciones y suites disponibles',
                    'Servicios e instalaciones del hotel',
                    'UbicaciÃ³n y puntos de interÃ©s cercanos',
                    'Tarifas y ofertas especiales',
                    'Reserva tu estancia en [hotel]'
                ]
            },
            'taller_mecanico': {
                'keywords': ['taller', 'mecÃ¡nico', 'reparaciÃ³n', 'coche', 'vehÃ­culo', 'auto'],
                'services': ['revisiÃ³n', 'reparaciÃ³n motor', 'cambio aceite', 'neumÃ¡ticos', 'ITV', 'chapa y pintura'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta la revisiÃ³n del coche?',
                    'Â¿Hacen presupuesto gratuito?',
                    'Â¿CuÃ¡nto tarda la reparaciÃ³n?',
                    'Â¿Ofrecen servicio de grÃºa?',
                    'Â¿Tienen coche de sustituciÃ³n?'
                ],
                'h2_templates': [
                    'Servicios de reparaciÃ³n {keyword}',
                    'RevisiÃ³n completa del vehÃ­culo',
                    'Presupuesto sin compromiso',
                    'Precio y tiempo de reparaciÃ³n',
                    'Por quÃ© confiar en nuestro taller'
                ]
            },
            'academia': {
                'keywords': ['academia', 'clases', 'formaciÃ³n', 'curso', 'profesor', 'aprender'],
                'services': ['clases particulares', 'preparaciÃ³n exÃ¡menes', 'refuerzo escolar', 'idiomas', 'online'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuestan las clases?',
                    'Â¿Tienen clases online?',
                    'Â¿CuÃ¡l es el horario?',
                    'Â¿Hacen preparaciÃ³n de exÃ¡menes?',
                    'Â¿Ofrecen clase de prueba gratuita?'
                ],
                'h2_templates': [
                    'Cursos y clases de {keyword}',
                    'MetodologÃ­a de enseÃ±anza',
                    'Horarios y grupos disponibles',
                    'Precio de matrÃ­cula y mensualidad',
                    'InscrÃ­bete en nuestra academia'
                ]
            },
            'estetica': {
                'keywords': ['estÃ©tica', 'belleza', 'tratamiento', 'facial', 'corporal', 'depilaciÃ³n'],
                'services': ['depilaciÃ³n lÃ¡ser', 'tratamientos faciales', 'masajes', 'manicura', 'peeling'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta la depilaciÃ³n lÃ¡ser?',
                    'Â¿CuÃ¡ntas sesiones necesito?',
                    'Â¿Es doloroso el tratamiento?',
                    'Â¿Tienen promociones?',
                    'Â¿Se puede financiar?'
                ],
                'h2_templates': [
                    'Tratamientos de {keyword} avanzados',
                    'DepilaciÃ³n lÃ¡ser definitiva',
                    'Precio y paquetes de sesiones',
                    'TecnologÃ­a y equipamiento',
                    'Reserva tu cita de valoraciÃ³n'
                ]
            },
            'peluqueria': {
                'keywords': ['peluquerÃ­a', 'peluquero', 'corte', 'pelo', 'tinte', 'peinado'],
                'services': ['corte', 'color', 'mechas', 'alisado', 'tratamiento capilar', 'peinado'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta un corte de pelo?',
                    'Â¿Necesito cita previa?',
                    'Â¿Hacen alisado brasileÃ±o?',
                    'Â¿Tienen tratamientos para el cabello?',
                    'Â¿CuÃ¡l es el precio del tinte?'
                ],
                'h2_templates': [
                    'Servicios de peluquerÃ­a y belleza',
                    'Corte y color de pelo',
                    'Tratamientos capilares profesionales',
                    'Precio de servicios',
                    'Reserva tu cita online'
                ]
            },
            'consultoria': {
                'keywords': ['consultorÃ­a', 'consultor', 'asesorÃ­a', 'negocio', 'empresa', 'estrategia'],
                'services': ['consultorÃ­a estratÃ©gica', 'anÃ¡lisis de negocio', 'transformaciÃ³n digital', 'auditorÃ­a'],
                'common_questions': [
                    'Â¿CÃ³mo funciona la consultorÃ­a?',
                    'Â¿CuÃ¡nto cobran?',
                    'Â¿CuÃ¡nto dura el proceso?',
                    'Â¿En quÃ© sectores trabajan?',
                    'Â¿Ofrecen primera consulta gratuita?'
                ],
                'h2_templates': [
                    'Servicios de consultorÃ­a {keyword}',
                    'MetodologÃ­a y proceso de trabajo',
                    'Casos de Ã©xito y resultados',
                    'Sectores en los que trabajamos',
                    'Contacta con nuestros consultores'
                ]
            },
            'arquitectura': {
                'keywords': ['arquitecto', 'arquitectura', 'proyecto', 'diseÃ±o', 'construcciÃ³n', 'reforma'],
                'services': ['proyectos arquitectÃ³nicos', 'reformas', 'interiorismo', 'planos', 'licencias'],
                'common_questions': [
                    'Â¿CuÃ¡nto cobra un arquitecto?',
                    'Â¿CuÃ¡nto tiempo tarda el proyecto?',
                    'Â¿Gestionan las licencias?',
                    'Â¿Hacen reformas integrales?',
                    'Â¿Ofrecen servicio de interiorismo?'
                ],
                'h2_templates': [
                    'Proyectos de {keyword} a medida',
                    'Reformas integrales y rehabilitaciÃ³n',
                    'Proceso de diseÃ±o arquitectÃ³nico',
                    'Honorarios y presupuesto',
                    'Solicita tu consulta gratuita'
                ]
            },
            'fotografo': {
                'keywords': ['fotÃ³grafo', 'fotografÃ­a', 'fotos', 'sesiÃ³n', 'boda', 'reportaje'],
                'services': ['bodas', 'reportajes', 'sesiones familiares', 'fotografÃ­a corporativa', 'eventos'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta una sesiÃ³n de fotos?',
                    'Â¿CuÃ¡ndo recibo las fotos?',
                    'Â¿Incluyen retoque?',
                    'Â¿CuÃ¡ntas fotos entregÃ¡is?',
                    'Â¿Hacen fotografÃ­a de bodas?'
                ],
                'h2_templates': [
                    'Servicios de {keyword} profesional',
                    'Paquetes y precios de sesiones',
                    'Portfolio de trabajos realizados',
                    'Â¿QuÃ© incluye la sesiÃ³n fotogrÃ¡fica?',
                    'Reserva tu sesiÃ³n de fotos'
                ]
            },
            'contabilidad': {
                'keywords': ['gestorÃ­a', 'contabilidad', 'fiscal', 'laboral', 'contable', 'asesorÃ­a'],
                'services': ['gestiÃ³n fiscal', 'gestiÃ³n laboral', 'contabilidad', 'declaraciones', 'nÃ³minas'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta una gestorÃ­a?',
                    'Â¿QuÃ© servicios incluye?',
                    'Â¿Gestionan la declaraciÃ³n de la renta?',
                    'Â¿Atienden autÃ³nomos?',
                    'Â¿CuÃ¡nto tardan en procesar un trÃ¡mite?'
                ],
                'h2_templates': [
                    'Servicios de gestorÃ­a y {keyword}',
                    'AsesorÃ­a fiscal para autÃ³nomos y empresas',
                    'GestiÃ³n laboral y nÃ³minas',
                    'Tarifas y cuotas mensuales',
                    'Solicita presupuesto sin compromiso'
                ]
            },
            'nutricionista': {
                'keywords': ['nutriciÃ³n', 'nutricionista', 'dieta', 'alimentaciÃ³n', 'peso', 'adelgazar'],
                'services': ['dieta personalizada', 'nutriciÃ³n deportiva', 'pÃ©rdida de peso', 'educaciÃ³n nutricional'],
                'common_questions': [
                    'Â¿CuÃ¡nto cuesta la consulta de nutriciÃ³n?',
                    'Â¿CuÃ¡ntas visitas necesito?',
                    'Â¿Hacen dietas personalizadas?',
                    'Â¿Atienden nutriciÃ³n deportiva?',
                    'Â¿Tienen consulta online?'
                ],
                'h2_templates': [
                    'Servicios de {keyword} personalizados',
                    'Plan nutricional a medida',
                    'PÃ©rdida de peso saludable',
                    'Precio y seguimiento nutricional',
                    'Pide tu cita con nutricionista'
                ]
            },
            'fontanero': {
                'keywords': ['fontanero', 'fontanerÃ­a', 'instalaciÃ³n', 'reparaciÃ³n', 'fuga', 'tuberÃ­a'],
                'services': ['reparaciones', 'instalaciÃ³n', 'urgencias', 'desatascos', 'calefacciÃ³n'],
                'common_questions': [
                    'Â¿CuÃ¡nto cobra un fontanero?',
                    'Â¿Tienen servicio de urgencias?',
                    'Â¿CuÃ¡nto tardan en venir?',
                    'Â¿Hacen presupuesto gratuito?',
                    'Â¿Atienden los fines de semana?'
                ],
                'h2_templates': [
                    'Servicios de {keyword} profesional',
                    'Urgencias 24 horas disponibles',
                    'Reparaciones e instalaciones',
                    'Precio y presupuesto sin compromiso',
                    'Llama ahora y resolvemos tu problema'
                ]
            },
            'electricista': {
                'keywords': ['electricista', 'electricidad', 'instalaciÃ³n', 'luz', 'cuadro elÃ©ctrico'],
                'services': ['instalaciones elÃ©ctricas', 'reparaciones', 'cuadros elÃ©ctricos', 'certificados', 'urgencias'],
                'common_questions': [
                    'Â¿CuÃ¡nto cobra un electricista?',
                    'Â¿Hacen certificados elÃ©ctricos?',
                    'Â¿Atienden urgencias?',
                    'Â¿Hacen presupuesto gratuito?',
                    'Â¿CuÃ¡nto tiempo tarda la instalaciÃ³n?'
                ],
                'h2_templates': [
                    'Servicios de instalaciones {keyword}',
                    'Reparaciones elÃ©ctricas urgentes',
                    'Certificados y boletines elÃ©ctricos',
                    'Precio y presupuesto de trabajos',
                    'Contacta con electricistas profesionales'
                ]
            }
        }

    def _detect_business_niche(self, page):
        """
        Detect business niche/industry from URL structure, title, and content

        Args:
            page: Dict with 'url', 'title', 'h1', 'body_text', 'meta_description'

        Returns:
            tuple: (niche_name, confidence_score)
        """
        url = page.get('url', '').lower()
        title = page.get('title', '').lower()
        h1 = page.get('h1', [''])[0].lower() if page.get('h1') else ''
        meta = page.get('meta_description', '').lower()
        body = page.get('body_text', '').lower()

        # Combine all text for analysis
        all_text = f"{url} {title} {h1} {meta} {body}"

        industry_banks = self._get_industry_knowledge_banks()

        # Score each industry
        scores = {}
        for niche, data in industry_banks.items():
            score = 0
            keywords = data['keywords']

            # Check presence of industry keywords
            for keyword in keywords:
                if keyword in url:
                    score += 10  # URL is strongest signal
                if keyword in title:
                    score += 8
                if keyword in h1:
                    score += 6
                if keyword in meta:
                    score += 4
                # Count occurrences in body (cap at 5)
                body_count = min(body.count(keyword), 5)
                score += body_count * 2

            scores[niche] = score

        # Get top niche
        if not scores or max(scores.values()) == 0:
            return ('general', 0)

        top_niche = max(scores, key=scores.get)
        confidence = scores[top_niche]

        return (top_niche, confidence)

    def _parse_url_structure(self, url):
        """
        Parse URL structure to extract semantic meaning from path segments

        Args:
            url: Full URL string

        Returns:
            dict: {
                'section': First path segment (servicios, blog, etc.),
                'category': Second path segment,
                'subcategory': Third path segment,
                'slug': Final segment,
                'segments': All segments as list
            }
        """
        parsed = urlparse(url)
        path = parsed.path.strip('/')

        if not path:
            return {
                'section': 'homepage',
                'category': None,
                'subcategory': None,
                'slug': 'homepage',
                'segments': []
            }

        segments = path.split('/')

        return {
            'section': segments[0] if len(segments) > 0 else None,
            'category': segments[1] if len(segments) > 1 else None,
            'subcategory': segments[2] if len(segments) > 2 else None,
            'slug': segments[-1],  # Last segment
            'segments': segments
        }

    def select_operation_mode(self):
        """Interactive mode selection"""
        print("\n" + "="*60)
        print("ðŸš€ SEO CONTENT GENERATOR")
        print("="*60)
        print("\nSeleccione el modo de operaciÃ³n:")
        print("\n1. ðŸ“„ SITIO NUEVO - Trabajar con brief de negocio")
        print("   â€¢ Procesar archivo de brief (.txt, .docx, .pdf)")
        print("   â€¢ Generar estrategia SEO completa")
        print("   â€¢ Crear contenido desde cero")
        print("\n2. ðŸ” SITIO EXISTENTE - Analizar sitio actual")
        print("   â€¢ Analizar sitemap existente")
        print("   â€¢ Optimizar pÃ¡ginas seleccionadas")
        print("   â€¢ Mejorar contenido actual")
        print("\n" + "-"*60)

        while True:
            try:
                choice = input("\nIngrese su opciÃ³n (1 o 2): ").strip()
                if choice == '1':
                    self.operation_mode = 'new_site'
                    print("\nâœ… Modo seleccionado: SITIO NUEVO")
                    return 'new_site'
                elif choice == '2':
                    self.operation_mode = 'existing_site'
                    print("\nâœ… Modo seleccionado: SITIO EXISTENTE")
                    return 'existing_site'
                else:
                    print("âŒ OpciÃ³n invÃ¡lida. Por favor ingrese 1 o 2.")
            except KeyboardInterrupt:
                print("\n\nðŸ‘‹ OperaciÃ³n cancelada por el usuario.")
                sys.exit(0)

    def process_brief_file(self, file_path=None):
        """Process business brief from txt, docx, or pdf file"""
        if not file_path:
            print("\nðŸ“„ PROCESAMIENTO DE BRIEF DE NEGOCIO")
            print("-" * 50)
            file_path = input("Ingrese la ruta del archivo de brief (.txt, .docx, .pdf): ").strip().strip('"')

        if not os.path.exists(file_path):
            print(f"âŒ Error: El archivo '{file_path}' no existe.")
            return None

        file_ext = Path(file_path).suffix.lower()
        content = ""

        try:
            if file_ext == '.txt':
                content = self._read_txt_file(file_path)
            elif file_ext == '.docx':
                content = self._read_docx_file(file_path)
            elif file_ext == '.pdf':
                content = self._read_pdf_file(file_path)
            else:
                print(f"âŒ Formato de archivo no soportado: {file_ext}")
                print("ðŸ“‹ Formatos soportados: .txt, .docx, .pdf")
                return None

            logger.info(f"Brief loaded from: {file_path}")
            print(f"âœ… Brief cargado exitosamente desde: {os.path.basename(file_path)}")

            # Parse brief content
            self.business_data = self._parse_brief_content(content)
            return content

        except Exception as e:
            logger.error(f"Error processing brief file: {e}")
            print(f"âŒ Error procesando el archivo: {e}")
            return None

    def _read_txt_file(self, file_path):
        """Read text file"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def _read_docx_file(self, file_path):
        """Read Word document"""
        doc = Document(file_path)
        content = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                content.append(paragraph.text.strip())
        return '\n'.join(content)

    def _read_pdf_file(self, file_path):
        """Read PDF file"""
        content = []
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text = page.extract_text()
                if text.strip():
                    content.append(text.strip())
        return '\n'.join(content)

    def _parse_brief_content(self, content):
        """Parse brief content and extract business data"""
        business_data = self.brief_template.copy()

        # Basic parsing using regex patterns
        patterns = {
            'nombre_comercial': r'nombre\s*comercial[:\-]?\s*(.+?)(?:\n|$)',
            'descripcion_corta': r'descripci[Ã³o]n\s*corta[:\-]?\s*(.+?)(?:\n|$)',
            'actividad_principal': r'actividad\s*principal[:\-]?\s*(.+?)(?:\n|$)',
            'cobertura_geografica': r'cobertura\s*geogr[Ã¡a]fica[:\-]?\s*(.+?)(?:\n|$)',
            'objetivo_principal': r'objetivo\s*principal[:\-]?\s*(.+?)(?:\n|$)',
        }

        content_lower = content.lower()

        for field, pattern in patterns.items():
            match = re.search(pattern, content_lower, re.IGNORECASE | re.MULTILINE)
            if match:
                business_data['general_info'][field] = match.group(1).strip()

        # Extract social media links
        social_patterns = {
            'instagram': r'instagram[:\-]?\s*(.+?)(?:\n|$)',
            'facebook': r'facebook[:\-]?\s*(.+?)(?:\n|$)',
            'linkedin': r'linkedin[:\-]?\s*(.+?)(?:\n|$)',
        }

        for platform, pattern in social_patterns.items():
            match = re.search(pattern, content_lower, re.IGNORECASE | re.MULTILINE)
            if match:
                business_data['social_media'][platform] = match.group(1).strip()

        return business_data

    def validate_brief_completeness(self):
        """Validate brief completeness and request missing data"""
        print("\nðŸ” VALIDACIÃ“N DE COMPLETITUD DEL BRIEF")
        print("-" * 50)

        missing_fields = []
        required_fields = {
            'general_info': {
                'nombre_comercial': 'Nombre comercial del negocio',
                'actividad_principal': 'Actividad principal del negocio',
                'cobertura_geografica': 'Cobertura geogrÃ¡fica (ciudades/barrios)',
                'objetivo_principal': 'Objetivo principal del sitio web'
            }
        }

        # Check missing required fields
        for section, fields in required_fields.items():
            for field, description in fields.items():
                if not self.business_data.get(section, {}).get(field):
                    missing_fields.append((section, field, description))

        if missing_fields:
            print(f"ðŸ“‹ Se detectaron {len(missing_fields)} campos obligatorios faltantes:")
            self.request_missing_data(missing_fields)
        else:
            print("âœ… El brief estÃ¡ completo con todos los campos obligatorios.")

        # Optional fields with smart defaults
        self._fill_optional_fields()

    def request_missing_data(self, missing_fields):
        """Interactively request missing data from user"""
        print("\nðŸ“ SOLICITUD DE DATOS FALTANTES")
        print("-" * 50)

        for section, field, description in missing_fields:
            while True:
                try:
                    value = input(f"\nâ€¢ {description}: ").strip()
                    if value:
                        if section not in self.business_data:
                            self.business_data[section] = {}
                        self.business_data[section][field] = value
                        print(f"  âœ… Guardado: {value}")
                        break
                    else:
                        print("  âš ï¸  Este campo es obligatorio. Por favor ingrese un valor.")
                except KeyboardInterrupt:
                    print("\n\nðŸ‘‹ OperaciÃ³n cancelada por el usuario.")
                    sys.exit(0)

        print(f"\nâœ… Todos los campos obligatorios han sido completados.")

    def _fill_optional_fields(self):
        """Fill optional fields with smart defaults if missing"""
        defaults = {
            'general_info': {
                'descripcion_corta': f"Servicios profesionales de {self.business_data.get('general_info', {}).get('actividad_principal', 'nuestro negocio')}",
                'medios_pago': 'Efectivo, tarjetas de crÃ©dito/dÃ©bito, transferencias bancarias',
                'cta_global': 'ContÃ¡ctanos'
            }
        }

        for section, fields in defaults.items():
            for field, default_value in fields.items():
                if not self.business_data.get(section, {}).get(field):
                    if section not in self.business_data:
                        self.business_data[section] = {}
                    self.business_data[section][field] = default_value

    def setup_webdriver(self):
        """Setup Selenium WebDriver with Chrome"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            # Ignore SSL certificate errors
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--allow-insecure-localhost')

            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            logger.info("WebDriver initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Error setting up WebDriver: {e}")
            print(f"âš ï¸  No se pudo inicializar el navegador. Usando modo bÃ¡sico sin investigaciÃ³n competitiva.")
            return False

    def search_competitors(self, keywords):
        """Perform competitive research via Google and Bing"""
        if not self.driver and not self.setup_webdriver():
            return {}

        print(f"\nðŸ” INVESTIGACIÃ“N COMPETITIVA")
        print("-" * 50)
        print(f"ðŸŽ¯ Analizando keywords: {', '.join(keywords[:3])}...")

        all_competitors = {}
        search_engines = ['google', 'bing']

        for engine in search_engines:
            print(f"\nðŸ“Š Buscando en {engine.upper()}...")

            for keyword in keywords[:3]:  # Limit to top 3 keywords
                try:
                    competitors = self._search_single_keyword(engine, keyword)
                    if competitors:
                        all_competitors[f"{engine}_{keyword}"] = competitors
                        print(f"  âœ… {keyword}: {len(competitors)} competidores encontrados")

                    # Delay between searches
                    time.sleep(random.uniform(2, 4))

                except Exception as e:
                    logger.error(f"Error searching {keyword} on {engine}: {e}")
                    print(f"  âŒ Error buscando '{keyword}' en {engine}")

        self.competitive_analysis = all_competitors
        return all_competitors

    def _search_single_keyword(self, engine, keyword):
        """Search single keyword on specified engine"""
        competitors = []
        max_pages = 3  # Reduced from 5 for efficiency

        for page in range(max_pages):
            try:
                if engine == 'google':
                    url = f"https://www.google.com/search?q={keyword}&start={page * 10}"
                else:  # bing
                    url = f"https://www.bing.com/search?q={keyword}&first={page * 10 + 1}"

                self.driver.get(url)
                time.sleep(random.uniform(1, 2))

                # Extract search results
                page_competitors = self._extract_serp_results(engine)
                competitors.extend(page_competitors)

                if len(competitors) >= 15:  # Limit total results
                    break

            except Exception as e:
                logger.error(f"Error on page {page + 1} for {keyword}: {e}")
                break

        return competitors[:15]  # Return top 15 results

    def _extract_serp_results(self, engine):
        """Extract search results from SERP"""
        results = []

        try:
            if engine == 'google':
                # Google result selectors
                result_elements = self.driver.find_elements(By.CSS_SELECTOR, 'div.g')

                for element in result_elements:
                    try:
                        title_elem = element.find_element(By.CSS_SELECTOR, 'h3')
                        title = title_elem.text if title_elem else ""

                        link_elem = element.find_element(By.CSS_SELECTOR, 'a')
                        url = link_elem.get_attribute('href') if link_elem else ""

                        desc_elem = element.find_element(By.CSS_SELECTOR, '[data-sncf="1"]')
                        description = desc_elem.text if desc_elem else ""

                        if title and url and not url.startswith('https://www.google.com'):
                            results.append({
                                'title': title[:100],
                                'url': url,
                                'description': description[:200],
                                'engine': engine
                            })
                    except:
                        continue

            else:  # bing
                # Bing result selectors
                result_elements = self.driver.find_elements(By.CSS_SELECTOR, '.b_algo')

                for element in result_elements:
                    try:
                        title_elem = element.find_element(By.CSS_SELECTOR, 'h2 a')
                        title = title_elem.text if title_elem else ""
                        url = title_elem.get_attribute('href') if title_elem else ""

                        desc_elem = element.find_element(By.CSS_SELECTOR, '.b_caption p')
                        description = desc_elem.text if desc_elem else ""

                        if title and url and not url.startswith('https://www.bing.com'):
                            results.append({
                                'title': title[:100],
                                'url': url,
                                'description': description[:200],
                                'engine': engine
                            })
                    except:
                        continue

        except Exception as e:
            logger.error(f"Error extracting SERP results: {e}")

        return results

    def analyze_serp_patterns(self):
        """Analyze SERP patterns to identify keyword opportunities"""
        if not self.competitive_analysis:
            return {}

        print(f"\nðŸ“Š ANÃLISIS DE PATRONES SERP")
        print("-" * 50)

        pattern_analysis = {
            'title_patterns': Counter(),
            'description_patterns': Counter(),
            'url_patterns': Counter(),
            'common_keywords': Counter(),
            'content_gaps': [],
            'competitor_insights': {}
        }

        all_titles = []
        all_descriptions = []
        all_urls = []

        # Collect all SERP data
        for search_key, competitors in self.competitive_analysis.items():
            for competitor in competitors:
                if competitor.get('title'):
                    all_titles.append(competitor['title'].lower())
                if competitor.get('description'):
                    all_descriptions.append(competitor['description'].lower())
                if competitor.get('url'):
                    all_urls.append(competitor['url'].lower())

        # Analyze title patterns
        title_words = []
        for title in all_titles:
            words = re.findall(r'\b\w+\b', title)
            title_words.extend([word for word in words if len(word) > 3])

        pattern_analysis['title_patterns'] = Counter(title_words).most_common(20)

        # Analyze description patterns
        desc_words = []
        for desc in all_descriptions:
            words = re.findall(r'\b\w+\b', desc)
            desc_words.extend([word for word in words if len(word) > 3])

        pattern_analysis['description_patterns'] = Counter(desc_words).most_common(20)

        # Analyze common phrases
        all_text = ' '.join(all_titles + all_descriptions)
        phrases = self._extract_phrases(all_text)
        pattern_analysis['common_keywords'] = Counter(phrases).most_common(15)

        # Identify content gaps
        pattern_analysis['content_gaps'] = self._identify_content_gaps(pattern_analysis)

        self.serp_data = pattern_analysis

        print(f"âœ… AnÃ¡lisis completado:")
        print(f"  â€¢ {len(pattern_analysis['title_patterns'])} patrones de tÃ­tulos")
        print(f"  â€¢ {len(pattern_analysis['description_patterns'])} patrones de descripciones")
        print(f"  â€¢ {len(pattern_analysis['common_keywords'])} keywords principales")

        return pattern_analysis

    def _extract_phrases(self, text):
        """Extract meaningful phrases from text"""
        # Simple phrase extraction (2-3 words)
        words = re.findall(r'\b\w+\b', text.lower())
        phrases = []

        for i in range(len(words) - 1):
            if len(words[i]) > 3 and len(words[i + 1]) > 3:
                phrase = f"{words[i]} {words[i + 1]}"
                phrases.append(phrase)

        # 3-word phrases
        for i in range(len(words) - 2):
            if all(len(word) > 3 for word in words[i:i+3]):
                phrase = f"{words[i]} {words[i + 1]} {words[i + 2]}"
                phrases.append(phrase)

        return phrases

    def _identify_content_gaps(self, pattern_analysis):
        """Identify content gaps and opportunities"""
        gaps = []

        # Analyze missing common terms
        top_terms = [term for term, count in pattern_analysis['common_keywords'][:10]]

        business_activity = self.business_data.get('general_info', {}).get('actividad_principal', '').lower()
        business_location = self.business_data.get('general_info', {}).get('cobertura_geografica', '').lower()

        # Suggest location-based content
        if business_location:
            location_keywords = [
                f"{business_activity} {business_location}",
                f"mejor {business_activity} {business_location}",
                f"{business_activity} cerca de {business_location}"
            ]
            gaps.extend(location_keywords)

        # Suggest service-based content
        if business_activity:
            service_keywords = [
                f"costo {business_activity}",
                f"precio {business_activity}",
                f"como elegir {business_activity}"
            ]
            gaps.extend(service_keywords)

        return gaps[:10]

    def generate_seo_content(self):
        """Generate complete SEO content based on business data and competitive analysis"""
        print(f"\nðŸŽ¨ GENERACIÃ“N DE CONTENIDO SEO")
        print("-" * 50)

        # Extract main keywords from competitive analysis
        main_keywords = self._extract_main_keywords()

        # Generate content for each page type
        self.generated_content = {
            'pages': {},
            'blog_posts': [],
            'faqs': {},
            'keywords_assignment': {}
        }

        page_types = ['home', 'sobre_nosotros', 'servicios', 'blog', 'contacto']

        for page_type in page_types:
            print(f"  ðŸ“„ Generando contenido para: {page_type.replace('_', ' ').title()}")
            page_content = self._generate_page_content(page_type, main_keywords)
            self.generated_content['pages'][page_type] = page_content

        # Generate blog posts
        print(f"  ðŸ“ Generando artÃ­culos de blog...")
        blog_posts = self._generate_blog_posts(main_keywords)
        self.generated_content['blog_posts'] = blog_posts

        # Generate FAQs for each page
        print(f"  â“ Generando FAQs...")
        faqs = self._generate_faqs_for_pages(main_keywords)
        self.generated_content['faqs'] = faqs

        print(f"âœ… Contenido generado:")
        print(f"  â€¢ {len(self.generated_content['pages'])} pÃ¡ginas principales")
        print(f"  â€¢ {len(self.generated_content['blog_posts'])} artÃ­culos de blog")
        print(f"  â€¢ {len(self.generated_content['faqs'])} sets de FAQs")

        return self.generated_content

    def _extract_main_keywords(self):
        """Extract main keywords from business data and competitive analysis"""
        keywords = []

        # Business-based keywords
        business_info = self.business_data.get('general_info', {})
        activity = business_info.get('actividad_principal', '')
        location = business_info.get('cobertura_geografica', '')

        if activity:
            keywords.extend([
                activity.lower(),
                f"{activity.lower()} profesional",
                f"servicios {activity.lower()}"
            ])

        if location:
            keywords.extend([
                f"{activity.lower()} {location.lower()}",
                f"mejor {activity.lower()} {location.lower()}"
            ])

        # Add keywords from competitive analysis
        if self.serp_data.get('common_keywords'):
            comp_keywords = [kw for kw, count in self.serp_data['common_keywords'][:5]]
            keywords.extend(comp_keywords)

        return keywords[:10]

    def _generate_page_content(self, page_type, keywords):
        """Generate content for a specific page type"""
        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Nuestro Negocio')
        activity = self.business_data.get('general_info', {}).get('actividad_principal', 'nuestros servicios')
        location = self.business_data.get('general_info', {}).get('cobertura_geografica', '')

        # Select main keyword for this page
        main_keyword = keywords[0] if keywords else activity

        content = {
            'main_keyword': main_keyword,
            'title_seo': '',
            'meta_description': '',
            'h1': '',
            'slug': '',
            'h2_structure': [],
            'paragraphs': [],
            'cta': self.business_data.get('general_info', {}).get('cta_global', 'ContÃ¡ctanos')
        }

        if page_type == 'home':
            content.update({
                'title_seo': f"{business_name} - {activity.title()} {location}",
                'meta_description': f"Servicios profesionales de {activity} en {location}. {business_name} ofrece soluciones de calidad. Â¡ContÃ¡ctanos hoy!",
                'h1': f"{business_name} - {activity.title()} {location}",
                'slug': '/',
                'h2_structure': [
                    f"Â¿Por quÃ© elegir nuestros servicios de {main_keyword}?",
                    f"Nuestros servicios de {activity}",
                    f"Experiencia en {main_keyword} {location}",
                    f"Proceso de trabajo en {activity}",
                    f"Beneficios de trabajar con {business_name}",
                    "Testimonios de clientes satisfechos",
                    "Ãreas de cobertura",
                    "Preguntas Frecuentes"
                ]
            })

        elif page_type == 'sobre_nosotros':
            content.update({
                'title_seo': f"Sobre {business_name} - Expertos en {activity.title()}",
                'meta_description': f"Conoce la historia de {business_name}, nuestro equipo y experiencia en {activity}. MÃ¡s de X aÃ±os brindando servicios de calidad.",
                'h1': f"Sobre {business_name} - Expertos en {activity.title()}",
                'slug': '/sobre-nosotros',
                'h2_structure': [
                    f"Historia de {business_name}",
                    f"Nuestro equipo especializado en {main_keyword}",
                    "MisiÃ³n y valores",
                    f"Experiencia en {activity}",
                    "Certificaciones y reconocimientos",
                    "Nuestro compromiso con la calidad",
                    "Clientes satisfechos",
                    "Preguntas Frecuentes"
                ]
            })

        elif page_type == 'servicios':
            content.update({
                'title_seo': f"Servicios de {activity.title()} - {business_name} {location}",
                'meta_description': f"Descubre todos nuestros servicios de {activity} en {location}. Calidad, experiencia y resultados garantizados. Â¡Cotiza ahora!",
                'h1': f"Servicios de {activity.title()} - {business_name}",
                'slug': '/servicios',
                'h2_structure': [
                    f"Servicios principales de {main_keyword}",
                    f"Proceso de {activity}",
                    "GarantÃ­as y calidad",
                    "Precios y cotizaciones",
                    f"Ãreas de especializaciÃ³n en {main_keyword}",
                    "Materiales y herramientas",
                    "Tiempos de entrega",
                    "Preguntas Frecuentes"
                ]
            })

        elif page_type == 'blog':
            content.update({
                'title_seo': f"Blog - Consejos y Tips sobre {activity.title()}",
                'meta_description': f"ArtÃ­culos, consejos y novedades sobre {activity}. Mantente informado con los expertos de {business_name}.",
                'h1': f"Blog - Todo sobre {activity.title()}",
                'slug': '/blog',
                'h2_structure': [
                    f"Ãšltimos artÃ­culos sobre {main_keyword}",
                    f"Consejos profesionales de {activity}",
                    "Tendencias del sector",
                    "Casos de Ã©xito",
                    f"GuÃ­as prÃ¡cticas de {main_keyword}",
                    "Novedades del mercado",
                    "ArtÃ­culos destacados",
                    "Preguntas Frecuentes"
                ]
            })

        elif page_type == 'contacto':
            content.update({
                'title_seo': f"Contacto - {business_name} {location}",
                'meta_description': f"Contacta con {business_name} para {activity} en {location}. TelÃ©fono, WhatsApp, email. Â¡Respuesta rÃ¡pida garantizada!",
                'h1': f"Contacta con {business_name}",
                'slug': '/contacto',
                'h2_structure': [
                    "InformaciÃ³n de contacto",
                    f"Solicita tu cotizaciÃ³n de {main_keyword}",
                    "Horarios de atenciÃ³n",
                    "UbicaciÃ³n y cobertura",
                    f"Formulario de contacto para {activity}",
                    "Redes sociales",
                    "Tiempo de respuesta",
                    "Preguntas Frecuentes"
                ]
            })

        # Ensure titles and descriptions meet SEO requirements
        content['title_seo'] = content['title_seo'][:60]
        content['meta_description'] = content['meta_description'][:160]

        return content

    def _generate_blog_posts(self, keywords):
        """Generate 4 blog post templates"""
        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Nuestro Negocio')
        activity = self.business_data.get('general_info', {}).get('actividad_principal', 'nuestros servicios')
        location = self.business_data.get('general_info', {}).get('cobertura_geografica', '')

        blog_posts = []

        # Blog post templates
        post_templates = [
            {
                'type': 'guide',
                'title_base': f"GuÃ­a completa de {activity}",
                'topic': f"Todo lo que necesitas saber sobre {activity}"
            },
            {
                'type': 'tips',
                'title_base': f"10 consejos para elegir {activity}",
                'topic': f"Tips profesionales para {activity}"
            },
            {
                'type': 'costs',
                'title_base': f"Â¿CuÃ¡nto cuesta {activity} en {location}?",
                'topic': f"Precios y factores que influyen en {activity}"
            },
            {
                'type': 'trends',
                'title_base': f"Tendencias 2024 en {activity}",
                'topic': f"Novedades y futuro de {activity}"
            }
        ]

        for i, template in enumerate(post_templates):
            main_keyword = keywords[i % len(keywords)] if keywords else activity

            post = {
                'title_seo': template['title_base'][:60],
                'meta_description': f"{template['topic']} en {location}. Consejos de expertos de {business_name}. Â¡Lee mÃ¡s!"[:160],
                'h1': template['title_base'],
                'slug': f"/blog/{template['type']}-{activity.replace(' ', '-').lower()}",
                'main_keyword': main_keyword,
                'h2_structure': [
                    f"IntroducciÃ³n a {main_keyword}",
                    f"Beneficios de {main_keyword}",
                    f"Tipos de {main_keyword}",
                    f"CÃ³mo elegir {main_keyword}",
                    f"Errores comunes en {main_keyword}",
                    f"Recomendaciones de expertos",
                    "Conclusiones",
                    "Preguntas Frecuentes"
                ],
                'type': template['type']
            }

            blog_posts.append(post)

        return blog_posts

    def _generate_faqs_for_pages(self, keywords):
        """Generate FAQs for each page"""
        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Nuestro Negocio')
        activity = self.business_data.get('general_info', {}).get('actividad_principal', 'nuestros servicios')
        location = self.business_data.get('general_info', {}).get('cobertura_geografica', '')

        faqs = {}

        # Common FAQ patterns
        common_faqs = [
            {
                'question': f"Â¿QuÃ© incluye el servicio de {activity}?",
                'answer': f"Nuestro servicio de {activity} incluye evaluaciÃ³n inicial, desarrollo del trabajo, garantÃ­a y soporte post-servicio."
            },
            {
                'question': f"Â¿CuÃ¡nto tiempo toma el {activity}?",
                'answer': f"El tiempo de {activity} depende del alcance del proyecto. Generalmente entre 1-4 semanas."
            },
            {
                'question': f"Â¿Ofrecen garantÃ­a en {activity}?",
                'answer': f"SÃ­, todos nuestros servicios de {activity} incluyen garantÃ­a de calidad y satisfacciÃ³n."
            },
            {
                'question': f"Â¿Atienden en {location}?",
                'answer': f"SÃ­, brindamos servicios de {activity} en toda la zona de {location} y alrededores."
            },
            {
                'question': f"Â¿CÃ³mo solicitar una cotizaciÃ³n de {activity}?",
                'answer': f"Puedes contactarnos por telÃ©fono, WhatsApp o nuestro formulario web para cotizar {activity}."
            }
        ]

        # Assign FAQs to pages
        for page_type in ['home', 'sobre_nosotros', 'servicios', 'blog', 'contacto']:
            faqs[page_type] = common_faqs.copy()

        return faqs

    def generate_complete_brief_markdown(self, output_file):
        """Generate complete brief in markdown format"""
        print(f"\nðŸ“ GENERANDO BRIEF COMPLETO")
        print("-" * 50)

        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Nuestro Negocio')
        activity = self.business_data.get('general_info', {}).get('actividad_principal', 'nuestros servicios')

        # Build complete brief markdown
        brief_md = f"""# Brief Completo de SEO - {business_name}
*Generado automÃ¡ticamente el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*

## ðŸ“‹ Datos Generales del Negocio

### InformaciÃ³n BÃ¡sica
- **Nombre comercial:** {self.business_data.get('general_info', {}).get('nombre_comercial', '')}
- **DescripciÃ³n corta:** {self.business_data.get('general_info', {}).get('descripcion_corta', '')}
- **Actividad principal:** {self.business_data.get('general_info', {}).get('actividad_principal', '')}
- **Cobertura geogrÃ¡fica:** {self.business_data.get('general_info', {}).get('cobertura_geografica', '')}
- **Medios de pago:** {self.business_data.get('general_info', {}).get('medios_pago', '')}
- **Objetivo principal:** {self.business_data.get('general_info', {}).get('objetivo_principal', '')}
- **CTA global:** {self.business_data.get('general_info', {}).get('cta_global', '')}

### Redes Sociales
- **Instagram:** {self.business_data.get('social_media', {}).get('instagram', '')}
- **Facebook:** {self.business_data.get('social_media', {}).get('facebook', '')}
- **LinkedIn:** {self.business_data.get('social_media', {}).get('linkedin', '')}

## ðŸŽ¯ Estrategia de Palabras Clave

### Keywords Principales Identificadas
"""

        # Add keyword research results
        if self.serp_data.get('common_keywords'):
            brief_md += "\n**Top Keywords de la Competencia:**\n"
            for i, (keyword, count) in enumerate(self.serp_data['common_keywords'][:10], 1):
                brief_md += f"{i}. **{keyword}** ({count} menciones)\n"

        # Add generated content for each page
        brief_md += "\n\n## ðŸ“„ Contenido SEO Generado\n\n"

        for page_type, content in self.generated_content.get('pages', {}).items():
            page_title = page_type.replace('_', ' ').title()
            brief_md += f"### {page_title}\n\n"
            brief_md += f"- **TÃ­tulo SEO:** {content.get('title_seo', '')}\n"
            brief_md += f"- **Meta Description:** {content.get('meta_description', '')}\n"
            brief_md += f"- **H1:** {content.get('h1', '')}\n"
            brief_md += f"- **Slug:** {content.get('slug', '')}\n"
            brief_md += f"- **Keyword Principal:** {content.get('main_keyword', '')}\n"

            brief_md += "\n**Estructura de H2:**\n"
            for i, h2 in enumerate(content.get('h2_structure', []), 1):
                brief_md += f"{i}. {h2}\n"
            brief_md += "\n"

        # Add blog posts
        brief_md += "## ðŸ“ ArtÃ­culos de Blog Sugeridos\n\n"
        for i, post in enumerate(self.generated_content.get('blog_posts', []), 1):
            brief_md += f"### ArtÃ­culo {i}: {post.get('title_seo', '')}\n"
            brief_md += f"- **Meta Description:** {post.get('meta_description', '')}\n"
            brief_md += f"- **Keyword Principal:** {post.get('main_keyword', '')}\n"
            brief_md += f"- **Slug:** {post.get('slug', '')}\n\n"

        # Add competitive analysis summary
        if self.competitive_analysis:
            brief_md += "## ðŸ” Resumen de AnÃ¡lisis Competitivo\n\n"
            brief_md += f"- **Total de competidores analizados:** {sum(len(competitors) for competitors in self.competitive_analysis.values())}\n"
            brief_md += f"- **Motores de bÃºsqueda consultados:** Google, Bing\n"
            brief_md += f"- **Keywords investigadas:** {len(self.competitive_analysis)} tÃ©rminos\n"

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(brief_md)
            print(f"âœ… Brief completo guardado en: {output_file}")
            logger.info(f"Complete brief saved to: {output_file}")
        except Exception as e:
            logger.error(f"Error saving complete brief: {e}")
            print(f"âŒ Error guardando brief: {e}")

    def generate_expanded_excel_report(self, output_file):
        """Generate Excel report with 4 sheets: Brief + 3 template sheets"""
        print(f"\nðŸ“Š GENERANDO REPORTE EXCEL EXPANDIDO")
        print("-" * 50)

        try:
            wb = Workbook()

            # Remove default sheet
            if "Sheet" in wb.sheetnames:
                wb.remove(wb["Sheet"])

            # Create 4 sheets: Brief + Template structure
            self._create_business_brief_sheet(wb)  # Sheet 1: Brief with scraped data
            self._create_link_building_sheet(wb)   # Sheet 2: Link building
            self._create_blog_sheet(wb)            # Sheet 3: Blog
            self._create_seo_onpage_sheet(wb)      # Sheet 4: SEO On-Page

            # Save workbook
            wb.save(output_file)
            print(f"âœ… Excel expandido guardado en: {output_file}")
            logger.info(f"Expanded Excel report saved to: {output_file}")

        except Exception as e:
            logger.error(f"Error generating expanded Excel report: {e}")
            print(f"âŒ Error generando Excel: {e}")

    def generate_phase1_excel(self, output_file):
        """
        FASE 1: Genera Excel con datos base (sin recomendaciones IA)
        - Extrae datos del sitio (URL, H1, H2, FAQs existentes)
        - Deja columna 'Palabra clave principal' VACÃA (usuario la completa)
        - Deja columnas de recomendaciones IA VACÃAS (se generan en Fase 2)
        """
        print(f"\nðŸ“Š GENERANDO EXCEL FASE 1 (SIN RECOMENDACIONES IA)")
        print("-" * 50)

        try:
            wb = Workbook()

            # Remove default sheet
            if "Sheet" in wb.sheetnames:
                wb.remove(wb["Sheet"])

            # Create sheets with Phase 1 logic (no AI recommendations)
            self._create_business_brief_sheet(wb)
            self._create_link_building_sheet(wb)
            self._create_blog_sheet_phase1(wb)      # Phase 1: No AI content
            self._create_seo_onpage_sheet_phase1(wb)  # Phase 1: No AI content

            # Save workbook
            wb.save(output_file)
            print(f"âœ… Excel Fase 1 guardado en: {output_file}")
            logger.info(f"Phase 1 Excel saved to: {output_file}")

        except Exception as e:
            logger.error(f"Error generating Phase 1 Excel: {e}")
            print(f"âŒ Error generando Excel Fase 1: {e}")

    def generate_phase2_excel(self, excel_file):
        """
        FASE 2: Lee Excel editado por usuario y genera recomendaciones IA
        - Lee keywords definidas por usuario en columna 7
        - Genera contenido IA solo para filas con keyword definida
        - Actualiza columnas verdes con recomendaciones
        """
        print(f"\nðŸ¤– GENERANDO CONTENIDO IA FASE 2")
        print("-" * 50)

        try:
            if not os.path.exists(excel_file):
                logger.error(f"Excel file not found: {excel_file}")
                print(f"âŒ Error: Archivo no encontrado: {excel_file}")
                return

            # Load existing Excel
            wb = load_workbook(excel_file)

            processed_total = 0
            skipped_total = 0

            # Process Blog sheet
            if "Blog" in wb.sheetnames:
                print("ðŸ“ Procesando hoja 'Blog'...")
                ws = wb["Blog"]
                processed, skipped = self._process_phase2_sheet(ws, "Blog")
                processed_total += processed
                skipped_total += skipped
                print(f"   âœ… {processed} pÃ¡ginas procesadas")
                print(f"   â­ï¸  {skipped} pÃ¡ginas sin keyword (omitidas)")

            # Process SEO On-Page sheet
            if "SEO On-Page" in wb.sheetnames:
                print("ðŸ“ Procesando hoja 'SEO On-Page'...")
                ws = wb["SEO On-Page"]
                processed, skipped = self._process_phase2_sheet(ws, "SEO On-Page")
                processed_total += processed
                skipped_total += skipped
                print(f"   âœ… {processed} pÃ¡ginas procesadas")
                print(f"   â­ï¸  {skipped} pÃ¡ginas sin keyword (omitidas)")

            # Save updated Excel
            wb.save(excel_file)
            print(f"\nâœ… Excel Fase 2 guardado en: {excel_file}")
            print(f"ðŸ“Š Total procesado: {processed_total} pÃ¡ginas")
            print(f"â­ï¸  Total omitido: {skipped_total} pÃ¡ginas (sin keyword)")
            logger.info(f"Phase 2 Excel saved: {excel_file} ({processed_total} processed, {skipped_total} skipped)")

        except Exception as e:
            logger.error(f"Error generating Phase 2 Excel: {e}")
            print(f"âŒ Error generando Excel Fase 2: {e}")
            import traceback
            traceback.print_exc()

    def _process_phase2_sheet(self, ws, sheet_name):
        """
        Procesa una hoja en Fase 2 (Blog o SEO On-Page)
        Lee keywords del usuario y genera contenido IA
        """
        processed = 0
        skipped = 0

        # Start from row 2 (skip header)
        for row in range(2, ws.max_row + 1):
            url = ws.cell(row, 5).value  # Column 5: URL
            user_keyword = ws.cell(row, 7).value  # Column 7: Palabra clave principal

            # Skip if no keyword defined by user
            if not user_keyword or str(user_keyword).strip() == "":
                skipped += 1
                continue

            user_keyword = str(user_keyword).strip()

            # Get existing data
            h1 = ws.cell(row, 8).value or ""  # Column 8: H1
            h2 = ws.cell(row, 10).value or ""  # Column 10: H2
            existing_faqs = ws.cell(row, 12).value or ""  # Column 12: FAQs

            # Find page data for contextual generation
            page = self._find_page_by_url(url) if url else None

            # Detect niche for contextual recommendations
            niche, confidence = ('general', 0)
            if page:
                niche, confidence = self._detect_business_niche(page)
                logger.info(f"   ðŸ§  Niche detected: '{niche}' (confidence: {confidence})")

            # Generate AI recommendations using user-defined keyword
            logger.info(f"   ðŸ”„ Generando contenido para keyword: '{user_keyword}'")

            url_recommendation = self._generate_seo_url_recommendation(url, user_keyword) if url else ""
            h1_recommendation = self._generate_seo_h1_recommendation(h1, user_keyword)
            h2_recommendation = self._generate_seo_h2_recommendation(h2, user_keyword, page)
            faqs_recommendation = self._generate_seo_faqs_recommendation(existing_faqs, user_keyword, page)

            # Update recommendation columns (green columns)
            ws.cell(row, 6, value=url_recommendation)  # Column 6: âœ… URL Optimizada
            ws.cell(row, 9, value=h1_recommendation)   # Column 9: âœ… H1 Optimizado
            ws.cell(row, 11, value=h2_recommendation)  # Column 11: âœ… H2 Optimizados
            ws.cell(row, 13, value=faqs_recommendation)  # Column 13: âœ… FAQs Optimizadas

            processed += 1

        return processed, skipped

    def _find_page_by_url(self, url):
        """Encuentra datos de pÃ¡gina por URL en content_data"""
        if not url or not self.content_data:
            return None

        url = str(url).strip()
        for page in self.content_data:
            page_url = page.get('url', '').strip()
            if page_url == url:
                return page

        return None

    def run_enhanced_analysis(self, sitemap_path=None, url_list_path=None, output_file="analisis-mejorado.md", excel_output="analisis-mejorado.xlsx"):
        """Enhanced analysis for existing sites with content generation capabilities"""
        print(f"\nðŸ” ANÃLISIS MEJORADO DE SITIO EXISTENTE")
        print("="*60)

        # Run original analysis first
        self.run_analysis(sitemap_path, url_list_path, output_file, excel_output)

        # Extract business data from existing content if possible
        if self.content_data:
            self._extract_business_data_from_content()

        # Perform competitive research based on extracted keywords
        main_keywords = [kw for kw, count in self.keywords.most_common(3)]
        if main_keywords:
            print(f"\nðŸ” Realizando investigaciÃ³n competitiva...")
            self.search_competitors(main_keywords)
            self.analyze_serp_patterns()

        # Generate additional content recommendations
        self.generate_seo_content()

        # Generate enhanced Excel report
        self.generate_expanded_excel_report(excel_output)

        print(f"\nâœ… AnÃ¡lisis mejorado completado:")
        print(f"  ðŸ“ Reporte markdown: {output_file}")
        print(f"  ðŸ“Š Excel expandido: {excel_output}")

    def _extract_business_data_from_content(self):
        """Extract comprehensive business data from existing website content via web scraping"""
        # Initialize data structures
        if not self.business_data.get('general_info'):
            self.business_data['general_info'] = {}
        if not self.business_data.get('social_media'):
            self.business_data['social_media'] = {}
        if not self.business_data.get('contact'):
            self.business_data['contact'] = {}
        if not self.business_data.get('services'):
            self.business_data['services'] = {}

        # Collect all data from scraped pages
        all_titles = [page.get('title', '') for page in self.content_data if page.get('title')]
        all_content = [page.get('body_text', '') for page in self.content_data if page.get('body_text')]
        all_meta_descriptions = [page.get('meta_description', '') for page in self.content_data if page.get('meta_description')]
        all_h1 = []
        all_h2 = []
        for page in self.content_data:
            if page.get('h1'):
                all_h1.extend(page['h1'])
            if page.get('h2'):
                all_h2.extend(page['h2'])

        combined_text = ' '.join(all_titles + all_content + all_meta_descriptions).lower()

        # 1. Extract business name
        if not self.business_data['general_info'].get('nombre_comercial'):
            # Try from home page title first
            for page in self.content_data:
                url = page.get('url', '').lower()
                if url.endswith('/') or url.split('/')[-1] in ['index.html', 'home', '']:
                    title = page.get('title', '')
                    if title:
                        # Remove common suffixes
                        clean_name = re.split(r'[-|â€“â€”:]', title)[0].strip()
                        self.business_data['general_info']['nombre_comercial'] = clean_name
                        break

            # Fallback: use first non-generic title
            if not self.business_data['general_info'].get('nombre_comercial'):
                for title in all_titles[:3]:
                    if title and len(title.split()) <= 6 and not any(generic in title.lower() for generic in ['home', 'inicio', 'blog', 'contacto', 'aviso', 'polÃ­tica']):
                        self.business_data['general_info']['nombre_comercial'] = title.split('-')[0].strip()
                        break

        # 2. Extract short description (from meta descriptions or first paragraph)
        if not self.business_data['general_info'].get('descripcion_corta'):
            if all_meta_descriptions:
                # Use the longest, most descriptive meta description
                best_desc = max([desc for desc in all_meta_descriptions if len(desc) > 50],
                              key=len, default='')
                if best_desc:
                    self.business_data['general_info']['descripcion_corta'] = best_desc[:300]

            # If still no description, generate one from content
            if not self.business_data['general_info'].get('descripcion_corta'):
                self.business_data['general_info']['descripcion_corta'] = self._generate_description_from_content()

        # 3. Extract main activity (from about page, services, or most common topics)
        if not self.business_data['general_info'].get('actividad_principal'):
            # Look for "sobre nosotros" or "about" pages
            about_content = []
            for page in self.content_data:
                url = page.get('url', '').lower()
                if any(keyword in url for keyword in ['sobre', 'about', 'nosotros', 'quienes', 'empresa']):
                    about_content.append(page.get('body_text', ''))

            if about_content:
                combined_about = ' '.join(about_content)
                # Extract first meaningful sentence (simplified)
                sentences = combined_about.split('.')
                for sentence in sentences[:5]:
                    if len(sentence) > 30 and len(sentence) < 200:
                        self.business_data['general_info']['actividad_principal'] = sentence.strip() + '.'
                        break

            # If still no activity, generate from content analysis
            if not self.business_data['general_info'].get('actividad_principal'):
                self.business_data['general_info']['actividad_principal'] = self._generate_main_activity_from_content()

        # 4. Extract service zones (zonas de servicio)
        if not self.business_data['general_info'].get('zonas_servicio'):
            # Look for country/city names in content
            location_keywords = []
            spanish_cities = ['madrid', 'barcelona', 'valencia', 'sevilla', 'bilbao', 'mÃ¡laga', 'valladolid', 'zaragoza']
            countries = ['espaÃ±a', 'mexico', 'argentina', 'colombia', 'chile', 'perÃº']

            for location in spanish_cities + countries:
                if location in combined_text:
                    location_keywords.append(location.title())

            if location_keywords:
                self.business_data['general_info']['zonas_servicio'] = ', '.join(set(location_keywords[:3]))

        # 5. Extract payment methods
        if not self.business_data['general_info'].get('medios_pago'):
            payment_keywords = {
                'tarjeta': 'Tarjeta de crÃ©dito/dÃ©bito',
                'paypal': 'PayPal',
                'transferencia': 'Transferencia bancaria',
                'efectivo': 'Efectivo',
                'bizum': 'Bizum',
                'stripe': 'Stripe'
            }
            found_payments = []
            for keyword, payment_name in payment_keywords.items():
                if keyword in combined_text:
                    found_payments.append(payment_name)

            if found_payments:
                self.business_data['general_info']['medios_pago'] = ', '.join(found_payments[:4])

        # 6. Extract differentiators (from unique selling points)
        if not self.business_data['general_info'].get('diferenciales'):
            differentiator_keywords = ['Ãºnico', 'exclusiv', 'diferente', 'especializado', 'lÃ­der', 'mejor', 'experto', 'profesional']
            differentiators = []

            for page in self.content_data:
                text = page.get('body_text', '').lower()
                for keyword in differentiator_keywords:
                    if keyword in text:
                        # Extract sentence containing keyword
                        sentences = text.split('.')
                        for sentence in sentences:
                            if keyword in sentence and len(sentence) > 20 and len(sentence) < 150:
                                differentiators.append(sentence.strip())
                                break

            if differentiators:
                self.business_data['general_info']['diferenciales'] = list(set(differentiators[:3]))

            # If no differentiators found, generate from content analysis
            if not self.business_data['general_info'].get('diferenciales'):
                self.business_data['general_info']['diferenciales'] = self._generate_differentiators_from_content()

        # 7. Extract social media links
        social_patterns = {
            'instagram': r'instagram\.com/([a-zA-Z0-9._]+)',
            'facebook': r'facebook\.com/([a-zA-Z0-9._]+)',
            'linkedin': r'linkedin\.com/(company|in)/([a-zA-Z0-9._-]+)',
            'twitter': r'twitter\.com/([a-zA-Z0-9._]+)',
            'youtube': r'youtube\.com/(channel|c|user)/([a-zA-Z0-9._-]+)'
        }

        for page in self.content_data:
            body_text = page.get('body_text', '')
            for platform, pattern in social_patterns.items():
                if not self.business_data['social_media'].get(platform):
                    match = re.search(pattern, body_text)
                    if match:
                        if platform == 'linkedin':
                            self.business_data['social_media'][platform] = f"https://linkedin.com/{match.group(1)}/{match.group(2)}"
                        elif platform == 'youtube':
                            self.business_data['social_media'][platform] = f"https://youtube.com/{match.group(1)}/{match.group(2)}"
                        else:
                            self.business_data['social_media'][platform] = f"https://{platform}.com/{match.group(1)}"

        # 8. Extract contact information
        # Email
        if not self.business_data['contact'].get('email'):
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            for page in self.content_data:
                text = page.get('body_text', '')
                email_match = re.search(email_pattern, text)
                if email_match:
                    email = email_match.group(0)
                    # Exclude common generic emails in examples
                    if not any(ex in email.lower() for ex in ['example', 'correo', 'email', 'test']):
                        self.business_data['contact']['email'] = email
                        break

        # Phone
        if not self.business_data['contact'].get('telefono'):
            phone_patterns = [
                r'\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}',
                r'\d{3}[-.\s]?\d{3}[-.\s]?\d{3}'
            ]
            for page in self.content_data:
                url = page.get('url', '').lower()
                if 'contacto' in url or 'contact' in url:
                    text = page.get('body_text', '')
                    for pattern in phone_patterns:
                        phone_match = re.search(pattern, text)
                        if phone_match:
                            self.business_data['contact']['telefono'] = phone_match.group(0)
                            break
                    if self.business_data['contact'].get('telefono'):
                        break

        # 9. Extract services/products
        if not self.business_data['services'].get('lista_servicios'):
            services = []
            # Look in services/products pages
            for page in self.content_data:
                url = page.get('url', '').lower()
                if any(keyword in url for keyword in ['servicio', 'service', 'producto', 'product']):
                    h2_list = page.get('h2', [])
                    h3_list = page.get('h3', [])
                    # H2 and H3 often list services
                    services.extend([h.strip() for h in h2_list if len(h.strip()) > 5 and len(h.strip()) < 100])
                    services.extend([h.strip() for h in h3_list if len(h.strip()) > 5 and len(h.strip()) < 100])

            if services:
                self.business_data['services']['lista_servicios'] = list(set(services[:10]))

        # 10. Extract main objective/CTA
        if not self.business_data['general_info'].get('objetivo_principal'):
            cta_keywords = ['contacta', 'solicita', 'reserva', 'compra', 'suscrÃ­be', 'regÃ­strate', 'descarga', 'prueba']
            for page in self.content_data:
                text = page.get('body_text', '').lower()
                for keyword in cta_keywords:
                    if keyword in text:
                        # Find common CTAs
                        if 'contacta' in text or 'solicita' in text:
                            self.business_data['general_info']['objetivo_principal'] = 'Generar leads / consultas'
                            break
                        elif 'compra' in text or 'reserva' in text:
                            self.business_data['general_info']['objetivo_principal'] = 'Ventas online'
                            break
                        elif 'suscrÃ­be' in text or 'regÃ­strate' in text:
                            self.business_data['general_info']['objetivo_principal'] = 'CaptaciÃ³n de registros'
                            break
                if self.business_data['general_info'].get('objetivo_principal'):
                    break

        # 11. Extract mission (misiÃ³n)
        if not self.business_data['general_info'].get('mision'):
            for page in self.content_data:
                text = page.get('body_text', '')
                # Look for mission keywords
                if 'misiÃ³n' in text.lower() or 'nuestra misiÃ³n' in text.lower():
                    # Extract paragraph after "misiÃ³n"
                    sentences = text.split('.')
                    for i, sentence in enumerate(sentences):
                        if 'misiÃ³n' in sentence.lower() and i < len(sentences) - 1:
                            mission_text = sentences[i] + '.' + (sentences[i+1] if i+1 < len(sentences) else '')
                            if len(mission_text) > 30 and len(mission_text) < 300:
                                self.business_data['general_info']['mision'] = mission_text.strip()
                                break
                if self.business_data['general_info'].get('mision'):
                    break

            # If no mission found, generate from content
            if not self.business_data['general_info'].get('mision'):
                self.business_data['general_info']['mision'] = self._generate_mission_from_content()

        # 12. Extract vision (visiÃ³n)
        if not self.business_data['general_info'].get('vision'):
            for page in self.content_data:
                text = page.get('body_text', '')
                if 'visiÃ³n' in text.lower() or 'nuestra visiÃ³n' in text.lower():
                    sentences = text.split('.')
                    for i, sentence in enumerate(sentences):
                        if 'visiÃ³n' in sentence.lower() and i < len(sentences) - 1:
                            vision_text = sentences[i] + '.' + (sentences[i+1] if i+1 < len(sentences) else '')
                            if len(vision_text) > 30 and len(vision_text) < 300:
                                self.business_data['general_info']['vision'] = vision_text.strip()
                                break
                if self.business_data['general_info'].get('vision'):
                    break

            # If no vision found, generate from content
            if not self.business_data['general_info'].get('vision'):
                self.business_data['general_info']['vision'] = self._generate_vision_from_content()

        # 13. Extract values (valores)
        if not self.business_data['general_info'].get('valores'):
            values_list = []
            for page in self.content_data:
                text = page.get('body_text', '')
                if 'valores' in text.lower() or 'nuestros valores' in text.lower():
                    # Try to extract values from H2/H3 near "valores"
                    h2_list = page.get('h2', [])
                    h3_list = page.get('h3', [])

                    # Common value keywords
                    value_keywords = ['integridad', 'honestidad', 'compromiso', 'excelencia', 'innovaciÃ³n',
                                    'transparencia', 'responsabilidad', 'respeto', 'calidad', 'confianza']

                    for h in h2_list + h3_list:
                        if any(keyword in h.lower() for keyword in value_keywords):
                            values_list.append(h.strip())

                    if values_list:
                        self.business_data['general_info']['valores'] = ', '.join(values_list[:5])
                        break

            # If no explicit values found, generate SEO-optimized values based on content analysis
            if not values_list and self.content_data:
                values_list = self._generate_seo_values_from_content()
                if values_list:
                    self.business_data['general_info']['valores'] = ', '.join(values_list)

        logger.info(f"Extracted business data from {len(self.content_data)} scraped pages")
        logger.info(f"Business name: {self.business_data.get('general_info', {}).get('nombre_comercial', 'Not found')}")
        logger.info(f"Services found: {len(self.business_data.get('services', {}).get('lista_servicios', []))}")
        logger.info(f"Social media: {list(self.business_data.get('social_media', {}).keys())}")

    def _generate_description_from_content(self):
        """Generate SEO-optimized short description from content analysis"""
        combined_text = ' '.join([page.get('body_text', '') for page in self.content_data[:3]])[:500]
        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Este negocio')

        # Extract key service words from H2/H3
        service_words = []
        for page in self.content_data[:3]:
            service_words.extend(page.get('h2', [])[:2])
            service_words.extend(page.get('h3', [])[:2])

        service_context = ', '.join(service_words[:3]) if service_words else 'servicios especializados'

        return f"{business_name} ofrece {service_context.lower()} con atenciÃ³n personalizada, calidad garantizada y compromiso con la excelencia en cada proyecto."

    def _generate_main_activity_from_content(self):
        """Generate main activity description from content analysis"""
        # Analyze most common H2 titles and keywords
        h2_titles = []
        for page in self.content_data:
            h2_titles.extend(page.get('h2', []))

        if h2_titles:
            # Use most common H2 as activity indicator
            from collections import Counter
            common_topics = Counter(h2_titles).most_common(3)
            topic = common_topics[0][0] if common_topics else 'servicios profesionales'
        else:
            topic = 'servicios especializados'

        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'La empresa')
        return f"{business_name} se especializa en {topic.lower()}, brindando soluciones integrales y personalizadas para satisfacer las necesidades de cada cliente."

    def _generate_differentiators_from_content(self):
        """Generate business differentiators from content analysis"""
        differentiators = []

        combined_text = ' '.join([page.get('body_text', '') for page in self.content_data]).lower()

        # Define differentiator patterns
        if 'experiencia' in combined_text or 'aÃ±os' in combined_text:
            differentiators.append('Amplia experiencia en el sector con resultados comprobados')

        if 'personalizado' in combined_text or 'medida' in combined_text:
            differentiators.append('AtenciÃ³n personalizada y soluciones a medida para cada cliente')

        if 'calidad' in combined_text or 'excelencia' in combined_text:
            differentiators.append('Compromiso con la calidad y excelencia en cada proyecto')

        if 'equipo' in combined_text or 'profesional' in combined_text:
            differentiators.append('Equipo altamente capacitado de profesionales especializados')

        # Default if nothing found
        if not differentiators:
            differentiators = [
                'Enfoque centrado en resultados y satisfacciÃ³n del cliente',
                'InnovaciÃ³n constante en metodologÃ­as y procesos',
                'Transparencia y comunicaciÃ³n fluida en cada etapa del proyecto'
            ]

        return differentiators[:3]

    def _generate_mission_from_content(self):
        """Generate mission statement from content analysis"""
        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Nuestra empresa')
        activity = self.business_data.get('general_info', {}).get('actividad_principal', '')

        if activity:
            core_service = activity.split('.')[0].lower().replace(business_name.lower(), '').strip()
        else:
            core_service = 'brindar servicios de excelencia'

        return f"La misiÃ³n de {business_name} es {core_service}, superando las expectativas de nuestros clientes mediante la innovaciÃ³n, el compromiso y la calidad en cada proyecto que emprendemos."

    def _generate_vision_from_content(self):
        """Generate vision statement from content analysis"""
        business_name = self.business_data.get('general_info', {}).get('nombre_comercial', 'Nuestra organizaciÃ³n')

        combined_text = ' '.join([page.get('body_text', '') for page in self.content_data]).lower()

        # Determine industry context
        if any(word in combined_text for word in ['tecnologÃ­a', 'digital', 'software', 'web']):
            sector = 'en el sector tecnolÃ³gico'
        elif any(word in combined_text for word in ['consultorÃ­a', 'asesorÃ­a', 'gestiÃ³n']):
            sector = 'en consultorÃ­a empresarial'
        elif any(word in combined_text for word in ['educaciÃ³n', 'formaciÃ³n', 'cursos']):
            sector = 'en el Ã¡mbito educativo'
        else:
            sector = 'en nuestro sector'

        return f"Ser reconocidos como lÃ­deres {sector}, destacÃ¡ndonos por nuestra innovaciÃ³n, calidad de servicio y compromiso inquebrantable con el Ã©xito de nuestros clientes."

    def _generate_seo_values_from_content(self):
        """Generate SEO-optimized business values based on content analysis"""
        values_list = []

        # Analyze content to infer values
        combined_text = ' '.join([page.get('body_text', '') for page in self.content_data]).lower()

        # Define value inference patterns based on content themes
        value_patterns = {
            'Calidad y Excelencia': ['mejor', 'calidad', 'premium', 'superior', 'destacado', 'Ã³ptimo', 'excelente'],
            'Compromiso con el Cliente': ['cliente', 'satisfacciÃ³n', 'atenciÃ³n', 'servicio', 'apoyo', 'asesoramiento'],
            'InnovaciÃ³n y TecnologÃ­a': ['innovador', 'tecnologÃ­a', 'moderno', 'avanzado', 'digital', 'actualizado'],
            'Experiencia y Profesionalismo': ['experiencia', 'aÃ±os', 'profesional', 'experto', 'especializado', 'trayectoria'],
            'Transparencia y Confianza': ['confianza', 'transparente', 'honesto', 'seguro', 'garantÃ­a', 'certificado'],
            'Responsabilidad Social': ['sostenible', 'responsable', 'comunidad', 'social', 'medio ambiente', 'ecolÃ³gico'],
            'Eficiencia y Resultados': ['eficiente', 'efectivo', 'rÃ¡pido', 'resultados', 'optimizado', 'productivo']
        }

        # Score each value based on keyword frequency
        value_scores = {}
        for value, keywords in value_patterns.items():
            score = sum(combined_text.count(keyword) for keyword in keywords)
            if score > 0:
                value_scores[value] = score

        # Get top 4-5 values based on content
        if value_scores:
            sorted_values = sorted(value_scores.items(), key=lambda x: x[1], reverse=True)
            values_list = [value for value, score in sorted_values[:5]]

        # If still no values found, provide default professional values
        if not values_list:
            business_name = self.business_data.get('general_info', {}).get('nombre_comercial', '')
            activity = self.business_data.get('general_info', {}).get('actividad_principal', '')

            # Generate contextual default values
            if any(word in activity.lower() for word in ['tecnologÃ­a', 'software', 'digital', 'web']):
                values_list = ['InnovaciÃ³n TecnolÃ³gica', 'Calidad en el Desarrollo', 'AtenciÃ³n Personalizada', 'Resultados Medibles']
            elif any(word in activity.lower() for word in ['consultorÃ­a', 'asesorÃ­a', 'servicios']):
                values_list = ['Experiencia Comprobada', 'Compromiso con el Cliente', 'Soluciones Personalizadas', 'Transparencia']
            elif any(word in activity.lower() for word in ['comercio', 'venta', 'tienda', 'productos']):
                values_list = ['Calidad en Productos', 'AtenciÃ³n al Cliente', 'Precios Competitivos', 'Confianza y Seguridad']
            else:
                values_list = ['Excelencia Profesional', 'Compromiso con la Calidad', 'OrientaciÃ³n al Cliente', 'Integridad y Confianza']

        return values_list

    def _detect_main_keyword_with_ai(self, page):
        """Detect main keyword using AI analysis of title, meta description, and content"""
        title = page.get('title', '')
        meta_description = page.get('meta_description', '')
        body_text = page.get('body_text', '')
        h1 = page.get('h1', [''])[0] if page.get('h1') else ''

        # Combine all text sources for analysis
        combined_text = f"{title} {meta_description} {h1} {body_text}".lower()

        # Extract word frequency (excluding common stop words)
        from collections import Counter
        import re

        # Extended stop words: Spanish, English, navigation terms, months
        stop_words = {
            # Spanish stop words
            'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'haber', 'por', 'con', 'su',
            'para', 'como', 'estar', 'tener', 'le', 'lo', 'todo', 'pero', 'mÃ¡s', 'hacer', 'o', 'poder',
            'decir', 'este', 'ir', 'otro', 'ese', 'si', 'me', 'ya', 'ver', 'porque', 'dar', 'cuando',
            'Ã©l', 'muy', 'sin', 'vez', 'mucho', 'saber', 'quÃ©', 'sobre', 'mi', 'alguno', 'mismo', 'yo',
            'tambiÃ©n', 'hasta', 'aÃ±o', 'dos', 'querer', 'entre', 'asÃ­', 'primero', 'desde', 'grande',
            # English stop words
            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 'for', 'not', 'on',
            'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they', 'we',
            'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their', 'is',
            'was', 'are', 'has', 'been', 'www', 'com', 'http', 'https',
            # Navigation and common WordPress terms
            'inicio', 'home', 'contacto', 'contact', 'servicios', 'services', 'nosotros', 'about',
            'empresa', 'company', 'privacidad', 'privacy', 'aviso', 'legal', 'terminos', 'terms',
            'cookies', 'politica', 'policy', 'categoria', 'category', 'archivo', 'archive', 'tag',
            'autor', 'author', 'buscar', 'search', 'pagina', 'page', 'menu', 'sidebar', 'widget',
            'footer', 'header', 'navigation', 'navegacion',
            # Months in Spanish
            'enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto',
            'septiembre', 'octubre', 'noviembre', 'diciembre',
            # Months in English
            'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',
            'september', 'october', 'november', 'december',
            # Generic filler words
            'aqui', 'alli', 'here', 'there', 'cuando', 'where', 'what', 'quien', 'como', 'donde',
            'cual', 'quien', 'cuanto', 'cuantos'
        }

        # Tokenize and count words
        words = re.findall(r'\b[a-zÃ¡Ã©Ã­Ã³ÃºÃ±]{3,}\b', combined_text)
        filtered_words = [word for word in words if word not in stop_words and len(word) > 3]

        word_counts = Counter(filtered_words)

        # Score keywords based on position and frequency
        keyword_scores = {}

        for word, count in word_counts.most_common(20):
            score = count

            # Boost score if word appears in title (x3)
            if word in title.lower():
                score *= 3

            # Boost score if word appears in meta description (x2)
            if word in meta_description.lower():
                score *= 2

            # Boost score if word appears in H1 (x2.5)
            if word in h1.lower():
                score *= 2.5

            keyword_scores[word] = score

        # Get top keyword
        if keyword_scores:
            sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
            main_keyword = sorted_keywords[0][0]

            # Quality guard: check if keyword is suspicious (navigation/month term)
            suspicious_terms = {
                'inicio', 'contacto', 'servicios', 'nosotros', 'home', 'contact', 'about',
                'enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto',
                'septiembre', 'octubre', 'noviembre', 'diciembre', 'january', 'february',
                'march', 'april', 'june', 'july', 'august', 'september', 'october', 'november', 'december'
            }

            # If main keyword is suspicious, try next best keyword
            if main_keyword in suspicious_terms and len(sorted_keywords) > 1:
                main_keyword = sorted_keywords[1][0]

            # Check for 2-word phrases (bigrams) with the main keyword
            bigram_pattern = rf'\b\w+\s+{main_keyword}\b|\b{main_keyword}\s+\w+\b'
            bigrams = re.findall(bigram_pattern, combined_text)

            if bigrams:
                bigram_counts = Counter(bigrams)
                most_common_bigram = bigram_counts.most_common(1)[0]
                if most_common_bigram[1] >= 3:  # If bigram appears 3+ times
                    final_keyword = most_common_bigram[0].strip()
                    # Mark as "REVISAR" if still contains suspicious terms
                    if any(term in final_keyword.lower() for term in suspicious_terms):
                        return f"âš ï¸ REVISAR: {final_keyword}"
                    return final_keyword

            # Mark main keyword for review if suspicious
            if main_keyword in suspicious_terms:
                return f"âš ï¸ REVISAR: {main_keyword}"

            return main_keyword
        else:
            # Fallback: use title words
            title_words = re.findall(r'\b[a-zÃ¡Ã©Ã­Ã³ÃºÃ±]{4,}\b', title.lower())
            title_filtered = [w for w in title_words if w not in stop_words]
            return title_filtered[0] if title_filtered else 'contenido'

    def _generate_seo_url_recommendation(self, url, keyword):
        """Generate SEO-optimized URL recommendation"""
        if not keyword:
            return url

        from urllib.parse import urlparse
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]

        # Get the last meaningful part of the URL
        if path_parts:
            last_part = path_parts[-1]
            # Remove file extension if exists
            if '.' in last_part:
                last_part = last_part.rsplit('.', 1)[0]

            # Clean keyword for URL (lowercase, replace spaces with hyphens)
            clean_keyword = keyword.lower().replace(' ', '-').replace('_', '-')

            # If keyword not in URL, suggest adding it
            if clean_keyword not in last_part.lower():
                # Create SEO-friendly slug
                suggested_slug = f"{clean_keyword}-{last_part}" if last_part else clean_keyword
                suggested_url = f"{parsed.scheme}://{parsed.netloc}/{'/'.join(path_parts[:-1])}/{suggested_slug}".rstrip('/')
                return suggested_url

        return url

    def _generate_seo_h1_recommendation(self, h1, keyword):
        """Generate SEO-optimized H1 recommendation"""
        if not keyword or not h1:
            return h1 if h1 else f"GuÃ­a Completa de {keyword.title()}" if keyword else ""

        keyword_lower = keyword.lower()
        h1_lower = h1.lower()

        # If keyword already in H1, optimize structure
        if keyword_lower in h1_lower:
            # Check if it's at the beginning (best practice)
            if not h1_lower.startswith(keyword_lower):
                return f"{keyword.title()}: {h1}"
            return h1
        else:
            # Add keyword naturally to H1
            power_words = ['GuÃ­a Completa', 'Todo lo que Necesitas Saber', 'Mejores PrÃ¡cticas', 'Expertos en']
            return f"{keyword.title()}: {h1} - GuÃ­a Completa"

    def _extract_real_topics_from_content(self, page, keyword):
        """Extract real topics from content using TF-IDF-like approach"""
        body_text = page.get('body_text', '')
        h2_list = page.get('h2', [])

        if not body_text:
            return []

        # Tokenize content into sentences
        sentences = re.split(r'[.!?]+', body_text)

        # Extended stop words
        stop_words = {
            'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'por', 'con', 'su',
            'para', 'como', 'estar', 'tener', 'lo', 'todo', 'mÃ¡s', 'hacer', 'este', 'muy', 'sin',
            'the', 'be', 'to', 'of', 'and', 'in', 'that', 'have', 'it', 'for', 'not', 'on', 'with',
            'inicio', 'contacto', 'servicios', 'nosotros', 'home', 'about', 'contact'
        }

        # Extract meaningful bigrams from content
        word_pattern = r'\b[a-zÃ¡Ã©Ã­Ã³ÃºÃ±]{4,}\b'
        bigrams = []

        for sentence in sentences[:30]:  # Analyze first 30 sentences
            words = [w.lower() for w in re.findall(word_pattern, sentence) if w.lower() not in stop_words]
            for i in range(len(words) - 1):
                bigram = f"{words[i]} {words[i+1]}"
                if len(bigram) > 8 and keyword and keyword.lower() in bigram:
                    bigrams.append(bigram)

        # Count bigram frequency
        bigram_counts = Counter(bigrams)

        # Extract topics from existing H2s
        topics_from_h2 = []
        if h2_list:
            for h2 in h2_list[:5]:
                if len(h2) > 10 and h2.lower().strip() not in ['', 'undefined']:
                    topics_from_h2.append(h2.strip())

        # Combine: prioritize existing H2s, then add discovered bigrams
        final_topics = topics_from_h2[:3]  # Keep up to 3 existing H2s

        for bigram, count in bigram_counts.most_common(5):
            if len(final_topics) < 5:
                # Convert bigram to question format
                topic = f"Â¿QuÃ© es {bigram}?" if 'quÃ©' not in bigram.lower() else bigram.title()
                if topic not in final_topics:
                    final_topics.append(topic)

        return final_topics

    def _generate_seo_h2_recommendation(self, h2_text, keyword, page=None):
        """Generate contextual, intelligent H2 recommendations based on industry and URL structure"""
        if not keyword:
            return h2_text

        # Skip if keyword is marked for review
        if keyword.startswith('âš ï¸ REVISAR'):
            keyword = keyword.replace('âš ï¸ REVISAR: ', '')

        if not page:
            # Fallback to generic if no page data
            return self._generate_generic_h2s(h2_text, keyword)

        # STEP 1: Detect business niche and URL structure
        niche, confidence = self._detect_business_niche(page)
        url_structure = self._parse_url_structure(page.get('url', ''))

        # STEP 2: Get industry knowledge bank
        industry_banks = self._get_industry_knowledge_banks()
        industry_data = industry_banks.get(niche, None)

        # STEP 3: Determine page type from URL structure
        page_type = url_structure['section']  # servicios, blog, about, etc.

        # STEP 4: Extract real topics from content
        real_topics = []
        if page:
            real_topics = self._extract_real_topics_from_content(page, keyword)

        # STEP 5: Build contextual H2s
        enhanced_h2s = []

        # Start with existing real H2s (max 2)
        if h2_text and h2_text.strip():
            h2_list = [h.strip() for h in h2_text.split(',') if h.strip()]
            enhanced_h2s.extend(h2_list[:2])

        # Add real topics from bigrams (max 2)
        for topic in real_topics[:2]:
            if topic not in enhanced_h2s:
                enhanced_h2s.append(topic)

        # STEP 6: Generate contextual H2s based on niche + page type
        if industry_data:
            h2_templates = industry_data['h2_templates']

            # Apply templates with keyword replacement
            for template in h2_templates:
                if len(enhanced_h2s) >= 5:
                    break

                # Replace {keyword} placeholder
                contextual_h2 = template.replace('{keyword}', keyword)

                # Skip if already added
                if contextual_h2 not in enhanced_h2s:
                    enhanced_h2s.append(contextual_h2)
        else:
            # Fallback to generic variations
            generic_variations = [
                f"Â¿QuÃ© es {keyword} y cÃ³mo funciona?",
                f"Beneficios principales de {keyword}",
                f"CÃ³mo elegir el mejor {keyword}",
                f"Precio y opciones de {keyword}",
                f"GuÃ­a completa sobre {keyword}"
            ]

            for variation in generic_variations:
                if len(enhanced_h2s) >= 5:
                    break
                if variation not in enhanced_h2s:
                    enhanced_h2s.append(variation)

        return ', '.join(enhanced_h2s[:5])

    def _generate_generic_h2s(self, h2_text, keyword):
        """Fallback generic H2 generation when no page data available"""
        keyword_variations = [
            f"Â¿QuÃ© es {keyword} y cÃ³mo funciona?",
            f"Beneficios principales de {keyword}",
            f"CÃ³mo elegir el mejor {keyword}",
            f"Precio y opciones de {keyword}",
            f"GuÃ­a completa sobre {keyword}"
        ]

        if not h2_text or h2_text.strip() == '':
            return ', '.join(keyword_variations[:5])

        h2_list = [h.strip() for h in h2_text.split(',') if h.strip()]
        enhanced_h2s = h2_list[:2]

        while len(enhanced_h2s) < 5:
            for suggestion in keyword_variations:
                if suggestion not in enhanced_h2s:
                    enhanced_h2s.append(suggestion)
                    break
            if len(enhanced_h2s) >= 5:
                break

        return ', '.join(enhanced_h2s[:5])

    def _generate_seo_faqs_recommendation(self, existing_faqs, keyword, page=None):
        """Generate contextual, intelligent FAQs based on detected industry"""
        if not keyword:
            return existing_faqs

        # Skip if keyword is marked for review
        if keyword.startswith('âš ï¸ REVISAR'):
            keyword = keyword.replace('âš ï¸ REVISAR: ', '')

        # STEP 1: Detect business niche if page available
        niche = 'general'
        if page:
            niche, confidence = self._detect_business_niche(page)

        # STEP 2: Get industry-specific questions
        industry_banks = self._get_industry_knowledge_banks()
        industry_data = industry_banks.get(niche, None)

        enhanced_faqs = []

        # STEP 3: Use industry-specific common questions
        if industry_data and 'common_questions' in industry_data:
            common_questions = industry_data['common_questions']

            for i, question in enumerate(common_questions[:5], 1):
                # Generate contextual answer based on niche
                answer = self._generate_contextual_answer(question, keyword, niche, industry_data)
                enhanced_faqs.append(f"P{i}: {question}\nR: {answer}")

        else:
            # Fallback to generic FAQs
            generic_faq_templates = [
                {
                    'q': f"Â¿QuÃ© es {keyword} y para quÃ© sirve?",
                    'a': f"{keyword.title()} es una soluciÃ³n profesional que permite optimizar procesos, mejorar resultados y aumentar la eficiencia. Se utiliza principalmente para alcanzar objetivos especÃ­ficos con calidad garantizada."
                },
                {
                    'q': f"Â¿CuÃ¡les son los principales beneficios de {keyword}?",
                    'a': f"Los beneficios de {keyword} incluyen: mejora en la productividad, optimizaciÃ³n de recursos, resultados medibles, atenciÃ³n personalizada y soporte continuo para garantizar el Ã©xito."
                },
                {
                    'q': f"Â¿CÃ³mo elegir el mejor servicio de {keyword}?",
                    'a': f"Para elegir el mejor servicio de {keyword}, considera: experiencia comprobada, casos de Ã©xito, opiniones de clientes, metodologÃ­a transparente y atenciÃ³n personalizada que se adapte a tus necesidades especÃ­ficas."
                },
                {
                    'q': f"Â¿CuÃ¡nto tiempo se necesita para ver resultados con {keyword}?",
                    'a': f"Los resultados con {keyword} pueden variar segÃºn el proyecto, pero generalmente se observan mejoras significativas en las primeras semanas. El compromiso y la implementaciÃ³n correcta son clave para el Ã©xito."
                },
                {
                    'q': f"Â¿DÃ³nde puedo encontrar servicios profesionales de {keyword}?",
                    'a': f"Puedes encontrar servicios profesionales de {keyword} contactando con especialistas certificados que ofrezcan garantÃ­as, casos de Ã©xito demostrados y atenciÃ³n personalizada para tu proyecto."
                }
            ]

            for i, faq in enumerate(generic_faq_templates, 1):
                enhanced_faqs.append(f"P{i}: {faq['q']}\nR: {faq['a']}")

        return "\n\n".join(enhanced_faqs)

    def _generate_contextual_answer(self, question, keyword, niche, industry_data):
        """Generate contextual answer based on industry and question"""
        question_lower = question.lower()

        # Detect question type and provide specific answers
        if 'cuÃ¡nto' in question_lower and ('cuesta' in question_lower or 'cobra' in question_lower or 'precio' in question_lower):
            # Price-related question
            if niche == 'fisioterapia':
                return f"El precio de {keyword} varÃ­a segÃºn el tipo de tratamiento y nÃºmero de sesiones necesarias. Una sesiÃ³n individual suele costar entre 35-60â‚¬. Muchas clÃ­nicas ofrecen bonos de 5 o 10 sesiones con descuento."
            elif niche == 'dentista':
                return f"El coste de {keyword} depende del tratamiento especÃ­fico. Una consulta de diagnÃ³stico ronda los 30-50â‚¬. Tratamientos mÃ¡s complejos pueden financiarse en cÃ³modas cuotas mensuales."
            elif niche == 'abogado':
                return f"Los honorarios para {keyword} pueden variar segÃºn la complejidad del caso. Algunos despachos ofrecen primera consulta gratuita y trabajan con tarifas planas o por Ã©xito."
            elif niche == 'restaurante':
                return f"El precio del {keyword} en nuestro restaurante es competitivo y ajustado a la calidad ofrecida. Consulta nuestra carta actualizada o menÃº del dÃ­a para conocer precios especÃ­ficos."
            elif niche == 'gimnasio':
                return f"La cuota mensual para {keyword} varÃ­a segÃºn el plan elegido. Ofrecemos desde 25â‚¬/mes en plan bÃ¡sico hasta 50â‚¬/mes con acceso completo y clases ilimitadas."
            else:
                return f"El precio de {keyword} depende de varios factores como la duraciÃ³n, complejidad y servicios incluidos. Contacta para recibir un presupuesto personalizado sin compromiso."

        elif 'cuÃ¡nto' in question_lower and ('dura' in question_lower or 'tiempo' in question_lower or 'tarda' in question_lower):
            # Duration-related question
            if niche == 'fisioterapia':
                return f"La duraciÃ³n del tratamiento de {keyword} depende del tipo y severidad de la lesiÃ³n. Las sesiones suelen durar 45-60 minutos. La recuperaciÃ³n completa puede tomar de 2-3 semanas a varios meses segÃºn el caso."
            elif niche == 'dentista':
                return f"El tiempo necesario para {keyword} varÃ­a segÃºn el procedimiento. Tratamientos simples pueden completarse en una visita, mientras que otros como ortodoncia pueden durar de 6 meses a 2 aÃ±os."
            elif niche == 'abogado':
                return f"La duraciÃ³n del proceso de {keyword} depende de la complejidad del caso y la vÃ­a judicial. Casos simples pueden resolverse en 3-6 meses, mientras que procedimientos complejos pueden extenderse hasta 1-2 aÃ±os."
            else:
                return f"El tiempo necesario para {keyword} varÃ­a segÃºn cada situaciÃ³n particular. Te proporcionaremos un cronograma detallado tras la evaluaciÃ³n inicial de tu caso especÃ­fico."

        elif 'sesiones' in question_lower or 'visitas' in question_lower or 'citas' in question_lower:
            # Sessions-related question
            if niche == 'fisioterapia':
                return f"El nÃºmero de sesiones de {keyword} necesarias depende de la condiciÃ³n tratada. Problemas agudos pueden mejorar en 3-5 sesiones, mientras que condiciones crÃ³nicas pueden requerir 10-15 sesiones de tratamiento."
            elif niche == 'psicologia':
                return f"La cantidad de sesiones de {keyword} varÃ­a segÃºn cada persona. Algunas situaciones se resuelven en 5-10 sesiones, mientras que procesos mÃ¡s profundos pueden requerir terapia a medio-largo plazo."
            elif niche == 'estetica':
                return f"El nÃºmero de sesiones de {keyword} necesarias depende del tipo de piel y objetivo deseado. Generalmente se recomiendan entre 6-10 sesiones para obtener resultados Ã³ptimos y duraderos."
            else:
                return f"La cantidad de sesiones necesarias para {keyword} se determina tras una evaluaciÃ³n inicial. Cada caso es Ãºnico y requiere un plan personalizado para garantizar los mejores resultados."

        elif 'horario' in question_lower or 'abierto' in question_lower or 'cerrado' in question_lower:
            # Schedule-related question
            return f"Nuestro horario de atenciÃ³n para {keyword} es de lunes a viernes de 9:00 a 20:00h y sÃ¡bados de 9:00 a 14:00h. TambiÃ©n ofrecemos citas con horarios flexibles para adaptarnos a tu disponibilidad."

        elif 'urgencias' in question_lower or 'emergencias' in question_lower:
            # Emergency-related question
            if niche in ['veterinario', 'fontanero', 'electricista']:
                return f"SÃ­, ofrecemos servicio de urgencias 24 horas para {keyword}. Contamos con un equipo disponible para atender cualquier emergencia de forma inmediata. Llama al telÃ©fono de urgencias."
            else:
                return f"Para casos urgentes de {keyword}, ofrecemos citas prioritarias el mismo dÃ­a o al dÃ­a siguiente. Contacta lo antes posible para evaluar tu situaciÃ³n y darte la atenciÃ³n necesaria."

        elif 'primera' in question_lower and ('consulta' in question_lower or 'cita' in question_lower or 'visita' in question_lower):
            # First visit question
            return f"SÃ­, ofrecemos primera consulta de {keyword} gratuita o con precio especial. Es una oportunidad para conocernos, evaluar tu situaciÃ³n y diseÃ±ar un plan personalizado sin compromiso."

        elif 'online' in question_lower or 'distancia' in question_lower or 'remoto' in question_lower:
            # Online service question
            return f"SÃ­, ofrecemos servicios de {keyword} online mediante videollamada. Es una opciÃ³n cÃ³moda y efectiva que te permite recibir atenciÃ³n profesional desde la comodidad de tu hogar."

        elif 'financiaciÃ³n' in question_lower or 'financiar' in question_lower or 'cuotas' in question_lower:
            # Financing question
            return f"SÃ­, para {keyword} ofrecemos opciones de financiaciÃ³n flexible en cÃ³modas cuotas mensuales sin intereses. Queremos que el precio no sea un impedimento para recibir la atenciÃ³n que necesitas."

        else:
            # Generic answer with industry context
            services = industry_data.get('services', [])
            services_text = ', '.join(services[:3]) if services else keyword
            return f"En {keyword}, ofrecemos servicios profesionales especializados incluyendo {services_text}, entre otros. Contamos con experiencia comprobada y metodologÃ­a efectiva para garantizar tu satisfacciÃ³n."

    def _extract_faqs_from_content(self, content_text):
        """Extract existing FAQs from page content"""
        import re
        faqs_found = []

        if not content_text:
            return []

        # Look for FAQ patterns in content
        # Pattern 1: Question followed by answer (common FAQ structure)
        question_patterns = [
            r'Â¿[^?]+\?',  # Spanish questions
            r'What [^?]+\?',  # English questions starting with What
            r'How [^?]+\?',   # English questions starting with How
            r'Why [^?]+\?',   # English questions starting with Why
            r'When [^?]+\?',  # English questions starting with When
            r'Where [^?]+\?', # English questions starting with Where
        ]

        for pattern in question_patterns:
            questions = re.findall(pattern, content_text, re.IGNORECASE)
            for question in questions[:10]:  # Limit to first 10 questions found
                # Try to find answer after question (next 200 chars)
                question_pos = content_text.find(question)
                if question_pos != -1:
                    answer_text = content_text[question_pos + len(question):question_pos + len(question) + 300]
                    # Clean answer: get sentences until we hit another question or end
                    sentences = answer_text.split('.')
                    answer = ''
                    for sentence in sentences[:3]:
                        if 'Â¿' not in sentence and '?' not in sentence or sentence.count('?') == 0:
                            answer += sentence.strip() + '. '
                        else:
                            break

                    if len(answer.strip()) > 20:
                        faqs_found.append({
                            'question': question.strip(),
                            'answer': answer.strip()[:250]
                        })

        return faqs_found[:5]  # Return max 5 FAQs

    def _generate_faqs_for_content(self, title, keyword, content_text):
        """Generate 5 FAQs with answers - first tries to extract from content, then generates"""
        faqs_list = []

        # STEP 1: Try to extract existing FAQs from content
        if content_text:
            existing_faqs = self._extract_faqs_from_content(content_text)
            if existing_faqs:
                for i, faq in enumerate(existing_faqs, 1):
                    faq_entry = f"P{i}: {faq['question']}\nR: {faq['answer']}"
                    faqs_list.append(faq_entry)

        # STEP 2: If no FAQs found or not enough, generate AI-based FAQs
        if len(faqs_list) < 5 and keyword:
            remaining_count = 5 - len(faqs_list)
            faq_templates = [
                f"Â¿QuÃ© es {keyword}?",
                f"Â¿CÃ³mo funciona {keyword}?",
                f"Â¿CuÃ¡les son los beneficios de {keyword}?",
                f"Â¿Por quÃ© elegir {keyword}?",
                f"Â¿DÃ³nde puedo obtener {keyword}?",
                f"Â¿CuÃ¡nto cuesta {keyword}?",
                f"Â¿CuÃ¡ndo es recomendable usar {keyword}?",
                f"Â¿QuiÃ©n puede usar {keyword}?",
            ]

            # Generate answers for remaining FAQs
            for i, question in enumerate(faq_templates[:remaining_count], len(faqs_list) + 1):
                answer = ""

                # Try to extract answer from content
                if content_text and len(content_text) > 100:
                    # Search for keyword context in content
                    keyword_lower = keyword.lower()
                    if keyword_lower in content_text.lower():
                        # Find paragraph containing keyword
                        paragraphs = content_text.split('\n')
                        for para in paragraphs:
                            if keyword_lower in para.lower() and len(para) > 50:
                                sentences = para.split('.')[:2]
                                answer = '. '.join([s.strip() for s in sentences if len(s.strip()) > 20])[:200]
                                if answer:
                                    break

                # Generate generic answer if no content-based answer found
                if not answer:
                    generic_answers = {
                        "Â¿QuÃ© es": f"{keyword.title()} es un servicio/producto que ofrece soluciones especializadas para satisfacer necesidades especÃ­ficas de nuestros clientes.",
                        "Â¿CÃ³mo funciona": f"{keyword.title()} funciona mediante un proceso estructurado que garantiza resultados efectivos y satisfactorios para todos nuestros usuarios.",
                        "Â¿CuÃ¡les son los beneficios": f"Los beneficios de {keyword} incluyen mejora en la eficiencia, resultados comprobados, y atenciÃ³n personalizada a cada cliente.",
                        "Â¿Por quÃ© elegir": f"Elegir {keyword} significa optar por calidad, experiencia y compromiso con la excelencia en cada servicio que ofrecemos.",
                        "Â¿DÃ³nde puedo obtener": f"Puedes obtener {keyword} contactando directamente con nosotros a travÃ©s de nuestro sitio web o medios de contacto disponibles.",
                        "Â¿CuÃ¡nto cuesta": f"El costo de {keyword} varÃ­a segÃºn las necesidades especÃ­ficas. Contacta con nuestro equipo para obtener una cotizaciÃ³n personalizada.",
                        "Â¿CuÃ¡ndo es recomendable": f"Es recomendable usar {keyword} cuando se busca optimizar procesos y obtener resultados profesionales de alta calidad.",
                        "Â¿QuiÃ©n puede usar": f"{keyword.title()} estÃ¡ diseÃ±ado para todas las personas y empresas que buscan soluciones profesionales y efectivas.",
                    }

                    for key, gen_answer in generic_answers.items():
                        if key in question:
                            answer = gen_answer
                            break

                    if not answer:
                        answer = f"Para mÃ¡s informaciÃ³n sobre {keyword}, te recomendamos contactar con nuestro equipo especializado que podrÃ¡ asesorarte de forma personalizada."

                # Format FAQ entry
                faq_entry = f"P{i}: {question}\nR: {answer}"
                faqs_list.append(faq_entry)

        # Combine all FAQs with separator
        return "\n\n".join(faqs_list) if faqs_list else ""

    def _create_link_building_sheet(self, wb):
        """Create Link Building sheet with internal and external links"""
        ws = wb.create_sheet("Link building")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        section_fill = PatternFill(start_color="D3D3D3", end_color="D3D3D3", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["Tipo de Enlace", "PÃ¡gina Origen", "URL Destino", "Dominio", "Texto Ancla", "Follow/NoFollow", "Estado"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        row = 2

        # Extract internal and external links from scraped content
        internal_links = []
        external_links = []
        base_domain = None

        if self.content_data:
            # Determine base domain from first URL
            first_url = self.content_data[0].get('url', '')
            if first_url:
                parsed = urlparse(first_url)
                base_domain = parsed.netloc

            # Process all pages to extract links
            for page in self.content_data:
                page_url = page.get('url', '')
                body_text = page.get('body_text', '')

                # Extract links from page (simplified - looks for href patterns)
                import re
                link_pattern = r'https?://[^\s<>"\']+|www\.[^\s<>"\']+'
                found_links = re.findall(link_pattern, body_text)

                for link in found_links:
                    if not link.startswith('http'):
                        link = 'https://' + link

                    parsed_link = urlparse(link)
                    link_domain = parsed_link.netloc

                    if base_domain and link_domain == base_domain:
                        internal_links.append({
                            'origin': page_url,
                            'destination': link,
                            'domain': link_domain
                        })
                    elif link_domain:
                        external_links.append({
                            'origin': page_url,
                            'destination': link,
                            'domain': link_domain
                        })

        # Remove duplicates
        seen_internal = set()
        unique_internal = []
        for link in internal_links:
            key = (link['destination'], link['origin'])
            if key not in seen_internal:
                seen_internal.add(key)
                unique_internal.append(link)

        seen_external = set()
        unique_external = []
        for link in external_links:
            key = (link['destination'], link['origin'])
            if key not in seen_external:
                seen_external.add(key)
                unique_external.append(link)

        # SECTION: Internal Links
        section_cell = ws.cell(row=row, column=1, value="ENLACES INTERNOS")
        section_cell.font = Font(bold=True)
        section_cell.fill = section_fill
        section_cell.border = border
        for col in range(2, 8):
            ws.cell(row=row, column=col).fill = section_fill
            ws.cell(row=row, column=col).border = border
        row += 1

        for link in unique_internal[:50]:  # Limit to 50 internal links
            ws.cell(row=row, column=1, value="Interno").border = border
            ws.cell(row=row, column=2, value=link['origin']).border = border
            ws.cell(row=row, column=3, value=link['destination']).border = border
            ws.cell(row=row, column=4, value=link['domain']).border = border
            ws.cell(row=row, column=5, value="").border = border
            ws.cell(row=row, column=6, value="Follow").border = border
            ws.cell(row=row, column=7, value="Activo").border = border
            row += 1

        # SECTION: External Links
        row += 1
        section_cell = ws.cell(row=row, column=1, value="ENLACES EXTERNOS")
        section_cell.font = Font(bold=True)
        section_cell.fill = section_fill
        section_cell.border = border
        for col in range(2, 8):
            ws.cell(row=row, column=col).fill = section_fill
            ws.cell(row=row, column=col).border = border
        row += 1

        for link in unique_external[:50]:  # Limit to 50 external links
            ws.cell(row=row, column=1, value="Externo").border = border
            ws.cell(row=row, column=2, value=link['origin']).border = border
            ws.cell(row=row, column=3, value=link['destination']).border = border
            ws.cell(row=row, column=4, value=link['domain']).border = border
            ws.cell(row=row, column=5, value="").border = border
            ws.cell(row=row, column=6, value="Follow").border = border
            ws.cell(row=row, column=7, value="Verificar").border = border
            row += 1

        # SECTION: Oportunidades de Link Building (from competitive analysis)
        if self.competitive_analysis:
            row += 1
            section_cell = ws.cell(row=row, column=1, value="OPORTUNIDADES DE BACKLINKS")
            section_cell.font = Font(bold=True)
            section_cell.fill = section_fill
            section_cell.border = border
            for col in range(2, 8):
                ws.cell(row=row, column=col).fill = section_fill
                ws.cell(row=row, column=col).border = border
            row += 1

            for search_key, competitors in self.competitive_analysis.items():
                keyword = search_key.split('_', 1)[1] if '_' in search_key else search_key

                for competitor in competitors[:5]:  # Top 5 competitors per keyword
                    url = competitor.get('url', '')
                    if url:
                        domain = urlparse(url).netloc
                        ws.cell(row=row, column=1, value="Oportunidad").border = border
                        ws.cell(row=row, column=2, value="").border = border
                        ws.cell(row=row, column=3, value=url).border = border
                        ws.cell(row=row, column=4, value=domain).border = border
                        ws.cell(row=row, column=5, value=keyword).border = border
                        ws.cell(row=row, column=6, value="").border = border
                        ws.cell(row=row, column=7, value="Pendiente").border = border
                        row += 1

        self.auto_adjust_columns(ws, max_width=60)

    def _create_blog_sheet(self, wb):
        """Create Blog sheet with SEO recommendations"""
        ws = wb.create_sheet("Blog")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        recommendation_fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")  # Light green
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers with recommendations columns
        headers = [
            "Fecha", "Tipo", "Reponsable", "Estado",
            "URL", "âœ… URL Optimizada (RecomendaciÃ³n SEO)",
            "Palabra clave principal",
            "H1", "âœ… H1 Optimizado (RecomendaciÃ³n SEO)",
            "H2", "âœ… H2 Optimizados (RecomendaciÃ³n SEO)",
            "FAQs (Preguntas Frecuentes)", "âœ… FAQs Optimizadas (RecomendaciÃ³n SEO)"
        ]

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            # Green background for recommendation columns
            if "âœ…" in header or "RecomendaciÃ³n" in header:
                cell.fill = recommendation_fill
            else:
                cell.fill = header_fill
            cell.border = border

        # Populate with blog articles from content_data
        row = 2
        if self.content_data:
            for page in self.content_data:
                url = page.get('url', '')
                # Identify blog posts (URLs containing 'blog', 'articulo', or date patterns)
                if any(indicator in url.lower() for indicator in ['blog', 'articulo', '/20']):
                    title = page.get('title', '')
                    h1 = page.get('h1', [''])[0] if page.get('h1') else title
                    h2_list = page.get('h2', [])
                    h2 = ', '.join(h2_list[:3]) if h2_list else ''

                    # Use AI to detect main keyword from title, meta description, and content
                    main_keyword = self._detect_main_keyword_with_ai(page)

                    # Generate FAQs for this blog post
                    faqs = self._generate_faqs_for_content(title, main_keyword, page.get('body_text', ''))

                    # Generate SEO recommendations
                    url_recommendation = self._generate_seo_url_recommendation(url, main_keyword)
                    h1_recommendation = self._generate_seo_h1_recommendation(h1, main_keyword)
                    h2_recommendation = self._generate_seo_h2_recommendation(h2, main_keyword, page)
                    faqs_recommendation = self._generate_seo_faqs_recommendation(faqs, main_keyword, page)

                    # Write data
                    ws.cell(row=row, column=1, value=datetime.now().strftime('%Y-%m-%d')).border = border
                    ws.cell(row=row, column=2, value="ArtÃ­culo").border = border
                    ws.cell(row=row, column=3, value="").border = border
                    ws.cell(row=row, column=4, value="Publicado").border = border
                    ws.cell(row=row, column=5, value=url).border = border
                    cell = ws.cell(row=row, column=6, value=url_recommendation)
                    cell.border = border
                    cell.fill = recommendation_fill
                    ws.cell(row=row, column=7, value=main_keyword).border = border
                    ws.cell(row=row, column=8, value=h1).border = border
                    cell = ws.cell(row=row, column=9, value=h1_recommendation)
                    cell.border = border
                    cell.fill = recommendation_fill
                    ws.cell(row=row, column=10, value=h2).border = border
                    cell = ws.cell(row=row, column=11, value=h2_recommendation)
                    cell.border = border
                    cell.fill = recommendation_fill
                    ws.cell(row=row, column=12, value=faqs).border = border
                    cell = ws.cell(row=row, column=13, value=faqs_recommendation)
                    cell.border = border
                    cell.fill = recommendation_fill
                    row += 1

        # Add generated blog content if available
        if self.generated_content.get('blog_articles'):
            for article in self.generated_content['blog_articles']:
                title = article.get('title', '')
                keyword = article.get('keyword', '')

                # Generate FAQs and recommendations
                faqs = self._generate_faqs_for_content(title, keyword, '')
                h1_recommendation = self._generate_seo_h1_recommendation(title, keyword)
                h2_recommendation = self._generate_seo_h2_recommendation('', keyword, None)
                faqs_recommendation = self._generate_seo_faqs_recommendation(faqs, keyword, None)

                ws.cell(row=row, column=1, value=datetime.now().strftime('%Y-%m-%d')).border = border
                ws.cell(row=row, column=2, value="ArtÃ­culo").border = border
                ws.cell(row=row, column=3, value="").border = border
                ws.cell(row=row, column=4, value="Planificado").border = border
                ws.cell(row=row, column=5, value="").border = border
                ws.cell(row=row, column=6, value="").border = border
                ws.cell(row=row, column=7, value=keyword).border = border
                ws.cell(row=row, column=8, value=title).border = border
                cell = ws.cell(row=row, column=9, value=h1_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=10, value="").border = border
                cell = ws.cell(row=row, column=11, value=h2_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=12, value=faqs).border = border
                cell = ws.cell(row=row, column=13, value=faqs_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_seo_onpage_sheet(self, wb):
        """Create SEO On-Page sheet with SEO recommendations"""
        ws = wb.create_sheet("SEO On-Page")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        recommendation_fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")  # Light green
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers with recommendations columns
        headers = [
            "Fecha", "Tipo", "Reponsable", "Estado",
            "URL", "âœ… URL Optimizada (RecomendaciÃ³n SEO)",
            "Palabra clave principal",
            "H1", "âœ… H1 Optimizado (RecomendaciÃ³n SEO)",
            "H2", "âœ… H2 Optimizados (RecomendaciÃ³n SEO)",
            "FAQs (Preguntas Frecuentes)", "âœ… FAQs Optimizadas (RecomendaciÃ³n SEO)"
        ]

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            # Green background for recommendation columns
            if "âœ…" in header or "RecomendaciÃ³n" in header:
                cell.fill = recommendation_fill
            else:
                cell.fill = header_fill
            cell.border = border

        # Populate with all pages from content_data
        row = 2
        if self.content_data:
            for page in self.content_data:
                url = page.get('url', '')
                title = page.get('title', '')
                h1 = page.get('h1', [''])[0] if page.get('h1') else title
                h2_list = page.get('h2', [])
                h2 = ', '.join(h2_list[:3]) if h2_list else ''

                # Determine page type
                page_type = "PÃ¡gina"
                if any(indicator in url.lower() for indicator in ['blog', 'articulo', '/20']):
                    continue  # Skip blog posts (they go in Blog sheet)

                # Use AI to detect main keyword from title, meta description, and content
                main_keyword = self._detect_main_keyword_with_ai(page)

                # Generate FAQs for this page
                faqs = self._generate_faqs_for_content(title, main_keyword, page.get('body_text', ''))

                # Generate SEO recommendations
                url_recommendation = self._generate_seo_url_recommendation(url, main_keyword)
                h1_recommendation = self._generate_seo_h1_recommendation(h1, main_keyword)
                h2_recommendation = self._generate_seo_h2_recommendation(h2, main_keyword, page)
                faqs_recommendation = self._generate_seo_faqs_recommendation(faqs, main_keyword, page)

                # Write data
                ws.cell(row=row, column=1, value=datetime.now().strftime('%Y-%m-%d')).border = border
                ws.cell(row=row, column=2, value=page_type).border = border
                ws.cell(row=row, column=3, value="").border = border
                ws.cell(row=row, column=4, value="Publicado").border = border
                ws.cell(row=row, column=5, value=url).border = border
                cell = ws.cell(row=row, column=6, value=url_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=7, value=main_keyword).border = border
                ws.cell(row=row, column=8, value=h1).border = border
                cell = ws.cell(row=row, column=9, value=h1_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=10, value=h2).border = border
                cell = ws.cell(row=row, column=11, value=h2_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=12, value=faqs).border = border
                cell = ws.cell(row=row, column=13, value=faqs_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                row += 1

        # Add generated pages if available
        if self.generated_content.get('pages'):
            for page_type, content in self.generated_content['pages'].items():
                page_name = page_type.replace('_', ' ').title()
                main_keyword = content.get('main_keyword', '')

                # Generate FAQs and recommendations
                faqs = self._generate_faqs_for_content(page_name, main_keyword, '')
                h1_recommendation = self._generate_seo_h1_recommendation(page_name, main_keyword)
                h2_recommendation = self._generate_seo_h2_recommendation('', main_keyword, None)
                faqs_recommendation = self._generate_seo_faqs_recommendation(faqs, main_keyword, None)

                ws.cell(row=row, column=1, value=datetime.now().strftime('%Y-%m-%d')).border = border
                ws.cell(row=row, column=2, value="PÃ¡gina").border = border
                ws.cell(row=row, column=3, value="").border = border
                ws.cell(row=row, column=4, value="Planificado").border = border
                ws.cell(row=row, column=5, value="").border = border
                ws.cell(row=row, column=6, value="").border = border
                ws.cell(row=row, column=7, value=main_keyword).border = border
                ws.cell(row=row, column=8, value=page_name).border = border
                cell = ws.cell(row=row, column=9, value=h1_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=10, value="").border = border
                cell = ws.cell(row=row, column=11, value=h2_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                ws.cell(row=row, column=12, value=faqs).border = border
                cell = ws.cell(row=row, column=13, value=faqs_recommendation)
                cell.border = border
                cell.fill = recommendation_fill
                row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_blog_sheet_phase1(self, wb):
        """
        FASE 1: Create Blog sheet WITHOUT AI recommendations
        Only extracts and displays existing data from scraped pages
        """
        ws = wb.create_sheet("Blog")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        recommendation_fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = [
            "Fecha", "Tipo", "Reponsable", "Estado",
            "URL", "âœ… URL Optimizada (RecomendaciÃ³n SEO)",
            "Palabra clave principal",
            "H1", "âœ… H1 Optimizado (RecomendaciÃ³n SEO)",
            "H2", "âœ… H2 Optimizados (RecomendaciÃ³n SEO)",
            "FAQs (Preguntas Frecuentes)", "âœ… FAQs Optimizadas (RecomendaciÃ³n SEO)"
        ]

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            if "âœ…" in header or "RecomendaciÃ³n" in header:
                cell.fill = recommendation_fill
            else:
                cell.fill = header_fill
            cell.border = border

        # Populate with scraped blog data ONLY (NO AI recommendations)
        row = 2
        if self.content_data:
            for page in self.content_data:
                url = page.get('url', '')
                # Identify blog posts
                if any(indicator in url.lower() for indicator in ['blog', 'articulo', '/20']):
                    title = page.get('title', '')
                    h1 = page.get('h1', [''])[0] if page.get('h1') else title
                    h2_list = page.get('h2', [])
                    h2 = ', '.join(h2_list[:3]) if h2_list else ''

                    # Extract existing FAQs if present
                    faqs = self._extract_existing_faqs_from_page(page)

                    # Write data (NO keyword detection, NO AI recommendations)
                    ws.cell(row=row, column=1, value=datetime.now().strftime('%Y-%m-%d')).border = border
                    ws.cell(row=row, column=2, value="ArtÃ­culo").border = border
                    ws.cell(row=row, column=3, value="").border = border
                    ws.cell(row=row, column=4, value="Publicado").border = border
                    ws.cell(row=row, column=5, value=url).border = border

                    # Empty recommendation columns (Phase 2 will fill these)
                    cell = ws.cell(row=row, column=6, value="")
                    cell.border = border
                    cell.fill = recommendation_fill

                    # Empty keyword column (USER will fill this)
                    ws.cell(row=row, column=7, value="").border = border

                    ws.cell(row=row, column=8, value=h1).border = border

                    cell = ws.cell(row=row, column=9, value="")
                    cell.border = border
                    cell.fill = recommendation_fill

                    ws.cell(row=row, column=10, value=h2).border = border

                    cell = ws.cell(row=row, column=11, value="")
                    cell.border = border
                    cell.fill = recommendation_fill

                    ws.cell(row=row, column=12, value=faqs).border = border

                    cell = ws.cell(row=row, column=13, value="")
                    cell.border = border
                    cell.fill = recommendation_fill

                    row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_seo_onpage_sheet_phase1(self, wb):
        """
        FASE 1: Create SEO On-Page sheet WITHOUT AI recommendations
        Only extracts and displays existing data from scraped pages
        """
        ws = wb.create_sheet("SEO On-Page")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        recommendation_fill = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = [
            "Fecha", "Tipo", "Reponsable", "Estado",
            "URL", "âœ… URL Optimizada (RecomendaciÃ³n SEO)",
            "Palabra clave principal",
            "H1", "âœ… H1 Optimizado (RecomendaciÃ³n SEO)",
            "H2", "âœ… H2 Optimizados (RecomendaciÃ³n SEO)",
            "FAQs (Preguntas Frecuentes)", "âœ… FAQs Optimizadas (RecomendaciÃ³n SEO)"
        ]

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            if "âœ…" in header or "RecomendaciÃ³n" in header:
                cell.fill = recommendation_fill
            else:
                cell.fill = header_fill
            cell.border = border

        # Populate with scraped pages ONLY (NO AI recommendations)
        row = 2
        if self.content_data:
            for page in self.content_data:
                url = page.get('url', '')

                # Skip blog posts (they go in Blog sheet)
                if any(indicator in url.lower() for indicator in ['blog', 'articulo', '/20']):
                    continue

                title = page.get('title', '')
                h1 = page.get('h1', [''])[0] if page.get('h1') else title
                h2_list = page.get('h2', [])
                h2 = ', '.join(h2_list[:3]) if h2_list else ''

                # Extract existing FAQs if present
                faqs = self._extract_existing_faqs_from_page(page)

                page_type = "PÃ¡gina"

                # Write data (NO keyword detection, NO AI recommendations)
                ws.cell(row=row, column=1, value=datetime.now().strftime('%Y-%m-%d')).border = border
                ws.cell(row=row, column=2, value=page_type).border = border
                ws.cell(row=row, column=3, value="").border = border
                ws.cell(row=row, column=4, value="Publicado").border = border
                ws.cell(row=row, column=5, value=url).border = border

                # Empty recommendation columns (Phase 2 will fill these)
                cell = ws.cell(row=row, column=6, value="")
                cell.border = border
                cell.fill = recommendation_fill

                # Empty keyword column (USER will fill this)
                ws.cell(row=row, column=7, value="").border = border

                ws.cell(row=row, column=8, value=h1).border = border

                cell = ws.cell(row=row, column=9, value="")
                cell.border = border
                cell.fill = recommendation_fill

                ws.cell(row=row, column=10, value=h2).border = border

                cell = ws.cell(row=row, column=11, value="")
                cell.border = border
                cell.fill = recommendation_fill

                ws.cell(row=row, column=12, value=faqs).border = border

                cell = ws.cell(row=row, column=13, value="")
                cell.border = border
                cell.fill = recommendation_fill

                row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _extract_existing_faqs_from_page(self, page):
        """Extract existing FAQs from page if present"""
        body_text = page.get('body_text', '')

        # Simple FAQ detection patterns
        faq_patterns = [
            r'Â¿[^?]+\?[^\n]+',  # Spanish questions with answers
            r'Q:[^\n]+A:[^\n]+',  # Q&A format
        ]

        faqs = []
        for pattern in faq_patterns:
            matches = re.findall(pattern, body_text)
            faqs.extend(matches[:3])  # Limit to first 3

        return '\n'.join(faqs) if faqs else ''

    def _create_business_brief_sheet(self, wb):
        """Create Business Brief sheet with comprehensive scraped data"""
        ws = wb.create_sheet("Brief del Negocio")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        section_fill = PatternFill(start_color="D3D3D3", end_color="D3D3D3", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["Campo", "Valor"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        row = 2

        # SECCIÃ“N: InformaciÃ³n General
        section_cell = ws.cell(row=row, column=1, value="INFORMACIÃ“N GENERAL")
        section_cell.font = Font(bold=True)
        section_cell.fill = section_fill
        section_cell.border = border
        for col in range(2, 3):
            ws.cell(row=row, column=col).fill = section_fill
            ws.cell(row=row, column=col).border = border
        row += 1

        general_fields = [
            ("Nombre comercial", self.business_data.get('general_info', {}).get('nombre_comercial', '')),
            ("DescripciÃ³n corta", self.business_data.get('general_info', {}).get('descripcion_corta', '')),
            ("Actividad principal", self.business_data.get('general_info', {}).get('actividad_principal', '')),
            ("Zonas de servicio", self.business_data.get('general_info', {}).get('zonas_servicio', '')),
            ("Medios de pago", self.business_data.get('general_info', {}).get('medios_pago', '')),
            ("Objetivo principal", self.business_data.get('general_info', {}).get('objetivo_principal', '')),
            ("MisiÃ³n", self.business_data.get('general_info', {}).get('mision', '')),
            ("VisiÃ³n", self.business_data.get('general_info', {}).get('vision', '')),
            ("Valores", self.business_data.get('general_info', {}).get('valores', '')),
        ]

        # Handle differentiators separately (could be list)
        differentiators = self.business_data.get('general_info', {}).get('diferenciales', [])
        if isinstance(differentiators, list):
            diff_text = '; '.join(differentiators) if differentiators else ''
        else:
            diff_text = differentiators
        general_fields.append(("Diferenciales", diff_text))

        for field, value in general_fields:
            ws.cell(row=row, column=1, value=field).border = border
            ws.cell(row=row, column=2, value=value).border = border
            row += 1

        # SECCIÃ“N: Redes Sociales
        row += 1
        section_cell = ws.cell(row=row, column=1, value="REDES SOCIALES")
        section_cell.font = Font(bold=True)
        section_cell.fill = section_fill
        section_cell.border = border
        for col in range(2, 3):
            ws.cell(row=row, column=col).fill = section_fill
            ws.cell(row=row, column=col).border = border
        row += 1

        social_fields = [
            ("Instagram", self.business_data.get('social_media', {}).get('instagram', '')),
            ("Facebook", self.business_data.get('social_media', {}).get('facebook', '')),
            ("LinkedIn", self.business_data.get('social_media', {}).get('linkedin', '')),
            ("Twitter", self.business_data.get('social_media', {}).get('twitter', '')),
            ("YouTube", self.business_data.get('social_media', {}).get('youtube', '')),
        ]

        for field, value in social_fields:
            ws.cell(row=row, column=1, value=field).border = border
            ws.cell(row=row, column=2, value=value).border = border
            row += 1

        # SECCIÃ“N: Contacto
        row += 1
        section_cell = ws.cell(row=row, column=1, value="CONTACTO")
        section_cell.font = Font(bold=True)
        section_cell.fill = section_fill
        section_cell.border = border
        for col in range(2, 3):
            ws.cell(row=row, column=col).fill = section_fill
            ws.cell(row=row, column=col).border = border
        row += 1

        contact_fields = [
            ("Email", self.business_data.get('contact', {}).get('email', '')),
            ("TelÃ©fono", self.business_data.get('contact', {}).get('telefono', '')),
            ("DirecciÃ³n", self.business_data.get('contact', {}).get('direccion', '')),
        ]

        for field, value in contact_fields:
            ws.cell(row=row, column=1, value=field).border = border
            ws.cell(row=row, column=2, value=value).border = border
            row += 1

        # SECCIÃ“N: Servicios/Productos
        row += 1
        section_cell = ws.cell(row=row, column=1, value="SERVICIOS/PRODUCTOS")
        section_cell.font = Font(bold=True)
        section_cell.fill = section_fill
        section_cell.border = border
        for col in range(2, 3):
            ws.cell(row=row, column=col).fill = section_fill
            ws.cell(row=row, column=col).border = border
        row += 1

        services = self.business_data.get('services', {}).get('lista_servicios', [])
        if services:
            for i, service in enumerate(services[:10], 1):
                ws.cell(row=row, column=1, value=f"Servicio/Producto {i}").border = border
                ws.cell(row=row, column=2, value=service).border = border
                row += 1
        else:
            ws.cell(row=row, column=1, value="Lista de servicios").border = border
            ws.cell(row=row, column=2, value="").border = border
            row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_competitive_analysis_sheet(self, wb):
        """Create Competitive Analysis sheet (Sheet 2)"""
        ws = wb.create_sheet("AnÃ¡lisis Competitivo")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["Keyword", "Motor", "PosiciÃ³n", "URL Competidor", "TÃ­tulo", "DescripciÃ³n"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # Competitive data
        row = 2
        for search_key, competitors in self.competitive_analysis.items():
            keyword = search_key.split('_', 1)[1] if '_' in search_key else search_key
            engine = search_key.split('_')[0] if '_' in search_key else 'unknown'

            for pos, competitor in enumerate(competitors, 1):
                ws.cell(row=row, column=1, value=keyword).border = border
                ws.cell(row=row, column=2, value=engine.upper()).border = border
                ws.cell(row=row, column=3, value=pos).border = border
                ws.cell(row=row, column=4, value=competitor.get('url', '')).border = border
                ws.cell(row=row, column=5, value=competitor.get('title', '')).border = border
                ws.cell(row=row, column=6, value=competitor.get('description', '')).border = border
                row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_keywords_assignment_sheet(self, wb):
        """Create Keywords Assignment sheet (Sheet 3)"""
        ws = wb.create_sheet("Keywords por PÃ¡gina")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["PÃ¡gina", "Keyword Principal", "Keywords Secundarias", "Volumen Est.", "Dificultad", "Oportunidad"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # Page keyword assignments
        row = 2
        for page_type, content in self.generated_content.get('pages', {}).items():
            page_name = page_type.replace('_', ' ').title()
            main_keyword = content.get('main_keyword', '')

            # Get secondary keywords from SERP analysis
            secondary_keywords = []
            if self.serp_data.get('common_keywords'):
                secondary_keywords = [kw for kw, count in self.serp_data['common_keywords'][:3] if kw != main_keyword]

            ws.cell(row=row, column=1, value=page_name).border = border
            ws.cell(row=row, column=2, value=main_keyword).border = border
            ws.cell(row=row, column=3, value=", ".join(secondary_keywords[:3])).border = border
            ws.cell(row=row, column=4, value="Medio").border = border  # Placeholder
            ws.cell(row=row, column=5, value="Media").border = border  # Placeholder
            ws.cell(row=row, column=6, value="Alta").border = border   # Placeholder
            row += 1

        self.auto_adjust_columns(ws, max_width=60)

    def _create_generated_content_sheet(self, wb):
        """Create Generated Content sheet (Sheet 4)"""
        ws = wb.create_sheet("Contenido SEO Generado")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["PÃ¡gina", "TÃ­tulo SEO", "Long. TÃ­tulo", "Meta Description", "Long. Meta", "H1", "Slug"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # Generated content data
        row = 2
        for page_type, content in self.generated_content.get('pages', {}).items():
            page_name = page_type.replace('_', ' ').title()
            title_seo = content.get('title_seo', '')
            meta_desc = content.get('meta_description', '')

            ws.cell(row=row, column=1, value=page_name).border = border
            ws.cell(row=row, column=2, value=title_seo).border = border
            ws.cell(row=row, column=3, value=len(title_seo)).border = border
            ws.cell(row=row, column=4, value=meta_desc).border = border
            ws.cell(row=row, column=5, value=len(meta_desc)).border = border
            ws.cell(row=row, column=6, value=content.get('h1', '')).border = border
            ws.cell(row=row, column=7, value=content.get('slug', '')).border = border

            # Color coding
            title_len = len(title_seo)
            if title_len == 0 or title_len > 60:
                ws.cell(row=row, column=3).fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            elif title_len < 30:
                ws.cell(row=row, column=3).fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")

            meta_len = len(meta_desc)
            if meta_len == 0 or meta_len > 160:
                ws.cell(row=row, column=5).fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            elif meta_len < 120:
                ws.cell(row=row, column=5).fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")

            row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_content_structure_sheet(self, wb):
        """Create Content Structure sheet (Sheet 5)"""
        ws = wb.create_sheet("Estructura de Contenido")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["PÃ¡gina", "SecciÃ³n (H2)", "Orden", "Keyword Focus", "CTA Incluido"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # Content structure data
        row = 2
        for page_type, content in self.generated_content.get('pages', {}).items():
            page_name = page_type.replace('_', ' ').title()
            h2_structure = content.get('h2_structure', [])
            main_keyword = content.get('main_keyword', '')

            for order, h2 in enumerate(h2_structure, 1):
                ws.cell(row=row, column=1, value=page_name).border = border
                ws.cell(row=row, column=2, value=h2).border = border
                ws.cell(row=row, column=3, value=order).border = border

                # Check if H2 contains main keyword
                keyword_included = "âœ…" if main_keyword.lower() in h2.lower() else "âŒ"
                ws.cell(row=row, column=4, value=keyword_included).border = border

                # Check if it's FAQ section (always has CTA)
                cta_included = "âœ…" if "frecuentes" in h2.lower() else "âš ï¸"
                ws.cell(row=row, column=5, value=cta_included).border = border

                row += 1

        self.auto_adjust_columns(ws, max_width=70)

    def _create_blog_content_sheet(self, wb):
        """Create Blog Content sheet (Sheet 6)"""
        ws = wb.create_sheet("Contenido de Blog")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["#", "Tipo", "TÃ­tulo SEO", "Meta Description", "Keyword Principal", "Slug", "Estado"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # Blog posts data
        row = 2
        for i, post in enumerate(self.generated_content.get('blog_posts', []), 1):
            ws.cell(row=row, column=1, value=i).border = border
            ws.cell(row=row, column=2, value=post.get('type', '').title()).border = border
            ws.cell(row=row, column=3, value=post.get('title_seo', '')).border = border
            ws.cell(row=row, column=4, value=post.get('meta_description', '')).border = border
            ws.cell(row=row, column=5, value=post.get('main_keyword', '')).border = border
            ws.cell(row=row, column=6, value=post.get('slug', '')).border = border
            ws.cell(row=row, column=7, value="Pendiente").border = border
            row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_faqs_sheet(self, wb):
        """Create FAQs sheet (Sheet 7)"""
        ws = wb.create_sheet("FAQs Generadas")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["PÃ¡gina", "Pregunta (H3)", "Respuesta", "Keyword Incluida", "Optimizada"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # FAQs data
        row = 2
        for page_type, faqs in self.generated_content.get('faqs', {}).items():
            page_name = page_type.replace('_', ' ').title()

            for faq in faqs:
                ws.cell(row=row, column=1, value=page_name).border = border
                ws.cell(row=row, column=2, value=faq.get('question', '')).border = border
                ws.cell(row=row, column=3, value=faq.get('answer', '')).border = border

                # Check if answer contains business activity
                activity = self.business_data.get('general_info', {}).get('actividad_principal', '')
                keyword_included = "âœ…" if activity.lower() in faq.get('answer', '').lower() else "âŒ"
                ws.cell(row=row, column=4, value=keyword_included).border = border
                ws.cell(row=row, column=5, value="âœ…").border = border

                row += 1

        self.auto_adjust_columns(ws, max_width=80)

    def _create_url_tracking_sheet(self, wb):
        """Create URL Tracking sheet (Sheet 8)"""
        ws = wb.create_sheet("Seguimiento de URLs")

        # Header styles
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))

        # Headers
        headers = ["URL/PÃ¡gina", "Estado Actual", "Prioridad", "Contenido Generado", "Implementado", "Fecha LÃ­mite", "Notas"]
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.border = border

        # URL tracking data
        row = 2
        for page_type, content in self.generated_content.get('pages', {}).items():
            page_name = page_type.replace('_', ' ').title()

            ws.cell(row=row, column=1, value=f"{page_name} ({content.get('slug', '')})").border = border
            ws.cell(row=row, column=2, value="Nuevo").border = border

            # Set priority based on page type
            priority = "Alta" if page_type in ['home', 'servicios', 'contacto'] else "Media"
            ws.cell(row=row, column=3, value=priority).border = border
            ws.cell(row=row, column=4, value="âœ…").border = border
            ws.cell(row=row, column=5, value="âŒ").border = border
            ws.cell(row=row, column=6, value="").border = border
            ws.cell(row=row, column=7, value="").border = border

            # Color code priority
            if priority == "Alta":
                ws.cell(row=row, column=3).fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            elif priority == "Media":
                ws.cell(row=row, column=3).fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")

            row += 1

        self.auto_adjust_columns(ws, max_width=60)

    def _create_original_analysis_sheets(self, wb):
        """Create the original 6 analysis sheets with current data"""
        # Use existing generate_excel_report logic but only create sheets
        try:
            # Create individual sheets using existing methods
            keyword_analysis = {'top_keywords': self.keywords.most_common(50), 'top_phrases': []}
            recommendations = self.generate_seo_recommendations(self.content_data, keyword_analysis)

            # Create sheets manually without saving
            self._add_summary_sheet(wb, keyword_analysis, recommendations)
            self._add_pages_analysis_sheet(wb)
            self._add_keywords_sheet(wb, keyword_analysis)
            self._add_phrases_sheet(wb, keyword_analysis)
            self._add_recommendations_sheet(wb, recommendations)
            self._add_technical_issues_sheet(wb)

        except Exception as e:
            logger.warning(f"Could not create original analysis sheets: {e}")

    def _add_summary_sheet(self, wb, keyword_analysis, recommendations):
        """Add summary sheet to workbook"""
        ws = wb.create_sheet("Resumen Ejecutivo Original")
        # Add summary data similar to original implementation
        # This is a simplified version
        pass

    def _add_pages_analysis_sheet(self, wb):
        """Add pages analysis sheet"""
        pass  # Simplified for now

    def _add_keywords_sheet(self, wb, keyword_analysis):
        """Add keywords sheet"""
        pass  # Simplified for now

    def _add_phrases_sheet(self, wb, keyword_analysis):
        """Add phrases sheet"""
        pass  # Simplified for now

    def _add_recommendations_sheet(self, wb, recommendations):
        """Add recommendations sheet"""
        pass  # Simplified for now

    def _add_technical_issues_sheet(self, wb):
        """Add technical issues sheet"""
        pass  # Simplified for now

    def parse_sitemap(self, sitemap_path):
        """Parse sitemap.xml and extract URLs, handling both sitemap index and regular sitemaps"""
        urls = []
        try:
            if sitemap_path.startswith('http'):
                response = self.session.get(sitemap_path, verify=False)
                response.raise_for_status()
                root = ET.fromstring(response.content)
            else:
                tree = ET.parse(sitemap_path)
                root = tree.getroot()

            # Handle namespace
            namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

            # Check if this is a sitemap index (contains <sitemap> elements)
            sitemap_elements = root.findall('.//ns:sitemap', namespace)
            if not sitemap_elements:
                sitemap_elements = root.findall('.//sitemap')

            if sitemap_elements:
                logger.info(f"Found sitemap index with {len(sitemap_elements)} sitemaps")
                # This is a sitemap index, parse each individual sitemap
                for sitemap_element in sitemap_elements:
                    loc_element = sitemap_element.find('ns:loc', namespace)
                    if loc_element is None:
                        loc_element = sitemap_element.find('loc')

                    if loc_element is not None:
                        sitemap_url = loc_element.text.strip()
                        logger.info(f"Parsing individual sitemap: {sitemap_url}")

                        # Recursively parse each sitemap
                        sitemap_urls = self.parse_individual_sitemap(sitemap_url)
                        urls.extend(sitemap_urls)

                        # Add small delay between sitemap requests
                        if self.delay > 0:
                            time.sleep(0.2)
            else:
                # This is a regular sitemap, parse URLs directly
                urls = self.parse_individual_sitemap(sitemap_path)

        except Exception as e:
            logger.error(f"Error parsing sitemap: {e}")

        # Filter out excluded URLs
        original_count = len(urls)
        filtered_urls = [url for url in urls if not self._should_exclude_url(url)]
        excluded_count = original_count - len(filtered_urls)

        if excluded_count > 0:
            logger.info(f"ðŸ” Filtered out {excluded_count} non-content URLs (WordPress/Elementor/taxonomies)")

        logger.info(f"Found {len(filtered_urls)} valid content URLs from sitemap(s)")
        return filtered_urls

    def parse_individual_sitemap(self, sitemap_path):
        """Parse an individual sitemap and extract URLs"""
        urls = []
        try:
            if sitemap_path.startswith('http'):
                response = self.session.get(sitemap_path, timeout=30, verify=False)
                response.raise_for_status()
                root = ET.fromstring(response.content)
            else:
                tree = ET.parse(sitemap_path)
                root = tree.getroot()

            # Handle namespace
            namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

            # Extract URLs from this sitemap
            for url_element in root.findall('.//ns:url', namespace):
                loc = url_element.find('ns:loc', namespace)
                if loc is not None:
                    urls.append(loc.text.strip())

            # Fallback without namespace
            if not urls:
                for url_element in root.findall('.//url'):
                    loc = url_element.find('loc')
                    if loc is not None:
                        urls.append(loc.text.strip())

        except Exception as e:
            logger.error(f"Error parsing individual sitemap {sitemap_path}: {e}")

        return urls

    def load_url_list(self, url_list_path):
        """Load URLs from text file"""
        urls = []
        try:
            with open(url_list_path, 'r', encoding='utf-8') as f:
                for line in f:
                    url = line.strip()
                    if url and not url.startswith('#'):
                        urls.append(url)
        except Exception as e:
            logger.error(f"Error loading URL list: {e}")
            
        logger.info(f"Loaded {len(urls)} URLs from list")
        return urls

    def _get_domain_from_url(self, url):
        """Extract domain from URL for cache organization"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        # Clean domain for filesystem
        domain_clean = domain.replace(':', '_').replace('/', '_')
        return domain_clean

    def _setup_domain_cache(self, first_url):
        """Setup cache directory for the current domain"""
        domain = self._get_domain_from_url(first_url)
        self.current_domain = domain
        self.domain_cache_dir = self.cache_dir / domain
        self.domain_cache_dir.mkdir(exist_ok=True, parents=True)
        logger.info(f"ðŸ“ Cache directory: .seo_cache/{domain}/")

    def _get_domain_cache_metadata_file(self):
        """Get metadata file path for current domain cache"""
        if not self.domain_cache_dir:
            return None
        return self.domain_cache_dir / "_metadata.json"

    def _needs_rescraping(self):
        """Check if domain needs re-scraping (>= 60 minutes since last scrape)"""
        metadata_file = self._get_domain_cache_metadata_file()
        if not metadata_file or not metadata_file.exists():
            return True  # No cache = needs scraping

        try:
            import json
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                last_scrape = datetime.fromisoformat(metadata.get('last_scrape', ''))
                age_minutes = (datetime.now() - last_scrape).total_seconds() / 60

                # OBLIGATORIO re-scrapear cada 60 minutos
                return age_minutes >= self.cache_rescrape_minutes
        except Exception as e:
            logger.warning(f"Failed to read cache metadata: {e}")
            return True  # Error = needs scraping

    def _should_delete_domain_cache(self):
        """Check if domain cache should be deleted (> 24 hours)"""
        metadata_file = self._get_domain_cache_metadata_file()
        if not metadata_file or not metadata_file.exists():
            return False

        try:
            import json
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                last_scrape = datetime.fromisoformat(metadata.get('last_scrape', ''))
                age_hours = (datetime.now() - last_scrape).total_seconds() / 3600
                return age_hours >= self.cache_cleanup_hours
        except Exception:
            return False

    def _save_domain_cache_metadata(self, url_count):
        """Save metadata for domain cache"""
        metadata_file = self._get_domain_cache_metadata_file()
        if not metadata_file:
            return

        try:
            import json
            metadata = {
                'domain': self.current_domain,
                'last_scrape': datetime.now().isoformat(),
                'url_count': url_count,
                'cache_version': '2.0'
            }
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)
            logger.info(f"ðŸ’¾ Saved cache metadata for {self.current_domain}")
        except Exception as e:
            logger.warning(f"Failed to save cache metadata: {e}")

    def _get_cache_filename(self, url):
        """Generate cache filename from URL within domain directory"""
        import hashlib
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if self.domain_cache_dir:
            return self.domain_cache_dir / f"{url_hash}.json"
        return self.cache_dir / f"{url_hash}.json"

    def _is_cache_valid(self, cache_file):
        """Check if cache file exists and is less than 60 minutes old for scraping"""
        if not cache_file.exists():
            return False

        file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
        current_time = datetime.now()
        age_minutes = (current_time - file_time).total_seconds() / 60

        return age_minutes < self.cache_duration_minutes

    def _should_delete_cache(self, cache_file):
        """Check if cache file should be deleted (older than 24 hours)"""
        if not cache_file.exists():
            return False

        file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
        current_time = datetime.now()
        age_hours = (current_time - file_time).total_seconds() / 3600

        return age_hours >= self.cache_cleanup_hours

    def _save_to_cache(self, url, content_data):
        """Save scraped content to cache"""
        import json
        cache_file = self._get_cache_filename(url)

        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'url': url,
                    'timestamp': datetime.now().isoformat(),
                    'data': content_data
                }, f, ensure_ascii=False, indent=2)
            logger.info(f"ðŸ’¾ Cached data for: {url}")
        except Exception as e:
            logger.warning(f"Failed to cache data for {url}: {e}")

    def _load_from_cache(self, url):
        """Load content from cache if valid"""
        import json
        cache_file = self._get_cache_filename(url)

        if not self._is_cache_valid(cache_file):
            return None

        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
                cached_time = datetime.fromisoformat(cached['timestamp'])
                age_minutes = (datetime.now() - cached_time).total_seconds() / 60
                logger.info(f"ðŸ“¦ Using cached data for: {url} (age: {age_minutes:.1f} minutes)")
                return cached['data']
        except Exception as e:
            logger.warning(f"Failed to load cache for {url}: {e}")
            return None

    def _get_cache_stats(self):
        """Get cache statistics by domain"""
        domain_dirs = [d for d in self.cache_dir.iterdir() if d.is_dir()]

        stats = {
            'total_domains': len(domain_dirs),
            'valid_domains': 0,
            'old_domains': 0,
            'expired_domains': 0
        }

        for domain_dir in domain_dirs:
            # Temporarily set domain cache dir to read metadata
            temp_domain_cache = self.domain_cache_dir
            self.domain_cache_dir = domain_dir

            metadata_file = domain_dir / "_metadata.json"
            if not metadata_file.exists():
                continue

            try:
                import json
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    last_scrape = datetime.fromisoformat(metadata.get('last_scrape', ''))
                    age_minutes = (datetime.now() - last_scrape).total_seconds() / 60
                    age_hours = age_minutes / 60

                    if age_minutes < self.cache_rescrape_minutes:
                        stats['valid_domains'] += 1
                    elif age_hours < self.cache_cleanup_hours:
                        stats['old_domains'] += 1
                    else:
                        stats['expired_domains'] += 1
            except:
                pass

            # Restore domain cache dir
            self.domain_cache_dir = temp_domain_cache

        return stats

    def _clear_expired_cache(self):
        """Remove domain caches older than 24 hours"""
        import shutil
        domain_dirs = [d for d in self.cache_dir.iterdir() if d.is_dir()]
        removed = 0
        kept_old = 0

        for domain_dir in domain_dirs:
            metadata_file = domain_dir / "_metadata.json"
            if not metadata_file.exists():
                continue

            try:
                import json
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    last_scrape = datetime.fromisoformat(metadata.get('last_scrape', ''))
                    age_hours = (datetime.now() - last_scrape).total_seconds() / 3600
                    age_minutes = age_hours * 60

                    if age_hours >= self.cache_cleanup_hours:
                        # Delete domain cache older than 24 hours
                        shutil.rmtree(domain_dir)
                        removed += 1
                        logger.info(f"ðŸ—‘ï¸  Deleted cache for {metadata.get('domain')} (age: {age_hours:.1f}h)")
                    elif age_minutes >= self.cache_duration_minutes:
                        # Keep domain cache between 60 min and 24 hours
                        kept_old += 1
            except Exception as e:
                logger.warning(f"Failed to process cache for {domain_dir.name}: {e}")

        if removed > 0:
            logger.info(f"ðŸ—‘ï¸  Removed {removed} domain caches older than 24 hours")
        if kept_old > 0:
            logger.info(f"ðŸ“¦ Keeping {kept_old} old domain caches (1h - 24h) for potential reuse")

    def _normalize_content_structure(self, content_info):
        """Normalize content structure to flatten headings for easier access"""
        if 'headings' in content_info:
            headings = content_info.get('headings', {})
            # Add flattened heading access
            for i in range(1, 7):
                key = f'h{i}'
                if key in headings:
                    content_info[key] = headings[key]
        return content_info

    def _extract_main_content_wp(self, soup):
        """Extract main content from WordPress site using common selectors"""
        # Try WordPress and Elementor content selectors (in priority order)
        main_content_selectors = [
            'article .entry-content',
            '.entry-content',
            'article .post-content',
            '.post-content',
            '.elementor-widget-theme-post-content',
            '.elementor-post__excerpt',
            'article',
            'main',
            '.content',
            '#content',
            '.main-content',
            '#main-content',
            '.page-content',
            '#page-content'
        ]

        for selector in main_content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                logger.debug(f"âœ… Main content found using selector: {selector}")
                return main_content

        # Fallback: return None (will use full soup)
        logger.debug("âš ï¸ Main content selector not found, using full page")
        return None

    def extract_content(self, url):
        """Extract content from a single URL with cache support"""
        # Try to load from cache first
        cached_content = self._load_from_cache(url)
        if cached_content is not None:
            return cached_content

        # If not in cache or expired, scrape fresh data
        try:
            logger.info(f"ðŸŒ Scraping fresh data from: {url}")
            response = self.session.get(url, timeout=30, verify=False)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()

            # Remove navigation, sidebar, footer (common noise)
            for noise in soup.select('nav, header, footer, aside, .sidebar, .menu, .navigation, .footer, .header'):
                noise.decompose()

            # Extract key elements
            title = soup.find('title')
            title_text = title.get_text().strip() if title else ""

            meta_desc = soup.find('meta', attrs={'name': 'description'})
            meta_description = meta_desc.get('content', '').strip() if meta_desc else ""

            # Extract main content using WordPress common selectors
            main_content = self._extract_main_content_wp(soup)

            # Extract headings from main content only (if found), otherwise from full page
            content_source = main_content if main_content else soup
            headings = {}
            for i in range(1, 7):
                h_tags = content_source.find_all(f'h{i}')
                headings[f'h{i}'] = [h.get_text().strip() for h in h_tags if h.get_text().strip()]

            # Extract body text from main content
            if main_content:
                body_text = main_content.get_text()
            else:
                body_text = soup.get_text()
            clean_text = re.sub(r'\s+', ' ', body_text).strip()
            
            # Extract links
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text().strip()
                if href and text:
                    links.append({'href': href, 'text': text})
            
            # Word count
            word_count = len(clean_text.split())
            
            content_info = {
                'url': url,
                'title': title_text,
                'meta_description': meta_description,
                'headings': headings,
                'body_text': clean_text[:5000],  # Truncate for storage
                'word_count': word_count,
                'links': links[:50],  # Limit links
                'status': response.status_code
            }

            # Normalize headings structure for easier access
            content_info = self._normalize_content_structure(content_info)

            # Save to cache for future use
            self._save_to_cache(url, content_info)

            # Log H2 count for debugging
            h2_count = len(content_info.get('h2', []))
            logger.info(f"âœ… Extracted content from: {url} (H1: {len(content_info.get('h1', []))}, H2: {h2_count}, H3: {len(content_info.get('h3', []))})")
            return content_info
            
        except Exception as e:
            logger.error(f"Error extracting content from {url}: {e}")
            return {
                'url': url,
                'error': str(e),
                'status': 'failed'
            }

    def analyze_keywords(self, content_data):
        """Analyze keywords and topics from content"""
        all_text = ""
        
        for content in content_data:
            if content.get('status') == 'failed':
                continue
                
            text_parts = [
                content.get('title', ''),
                content.get('meta_description', ''),
                content.get('body_text', '')
            ]
            
            # Add heading text
            headings = content.get('headings', {})
            for h_level, h_list in headings.items():
                text_parts.extend(h_list)
            
            all_text += " " + " ".join(text_parts)
        
        # Clean and tokenize
        clean_text = re.sub(r'[^\w\s]', ' ', all_text.lower())
        words = clean_text.split()
        
        # Filter common stop words (basic list)
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 
            'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 
            'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 
            'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 
            'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their', 'el', 'la', 
            'los', 'las', 'un', 'una', 'y', 'o', 'pero', 'en', 'de', 'con', 'por', 'para', 'que'
        }
        
        filtered_words = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # Count keywords
        self.keywords = Counter(filtered_words)
        
        # Extract common phrases (2-3 words)
        phrases = []
        for i in range(len(filtered_words) - 1):
            phrase = " ".join(filtered_words[i:i+2])
            if len(phrase) > 6:
                phrases.append(phrase)
        
        phrase_counter = Counter(phrases)
        
        logger.info(f"Analyzed {len(filtered_words)} words, found {len(self.keywords)} unique keywords")
        
        return {
            'top_keywords': self.keywords.most_common(50),
            'top_phrases': phrase_counter.most_common(25)
        }

    def generate_seo_recommendations(self, content_data, keyword_analysis):
        """Generate SEO recommendations based on analysis"""
        recommendations = []
        
        # Analyze content quality
        pages_analyzed = len([c for c in content_data if c.get('status') != 'failed'])
        pages_failed = len([c for c in content_data if c.get('status') == 'failed'])
        
        recommendations.append({
            'category': 'AnÃ¡lisis General',
            'priority': 'Alta',
            'action': f'Se analizaron {pages_analyzed} pÃ¡ginas exitosamente y {pages_failed} fallaron',
            'details': 'Revisar URLs que fallaron en el anÃ¡lisis'
        })
        
        # Title analysis
        no_title = len([c for c in content_data if not c.get('title')])
        short_titles = len([c for c in content_data if c.get('title') and len(c['title']) < 30])
        long_titles = len([c for c in content_data if c.get('title') and len(c['title']) > 60])
        
        if no_title > 0:
            recommendations.append({
                'category': 'TÃ­tulos',
                'priority': 'Alta',
                'action': f'Agregar tÃ­tulos a {no_title} pÃ¡ginas que no tienen title tag',
                'details': 'Cada pÃ¡gina debe tener un tÃ­tulo Ãºnico y descriptivo'
            })
            
        if short_titles > 0:
            recommendations.append({
                'category': 'TÃ­tulos',
                'priority': 'Media',
                'action': f'Optimizar {short_titles} tÃ­tulos que son muy cortos (menos de 30 caracteres)',
                'details': 'Los tÃ­tulos deben ser descriptivos entre 30-60 caracteres'
            })
            
        if long_titles > 0:
            recommendations.append({
                'category': 'TÃ­tulos',
                'priority': 'Media',
                'action': f'Acortar {long_titles} tÃ­tulos que exceden 60 caracteres',
                'details': 'TÃ­tulos muy largos se truncan en los resultados de bÃºsqueda'
            })
        
        # Meta description analysis
        no_meta_desc = len([c for c in content_data if not c.get('meta_description')])
        short_meta = len([c for c in content_data if c.get('meta_description') and len(c['meta_description']) < 120])
        long_meta = len([c for c in content_data if c.get('meta_description') and len(c['meta_description']) > 160])
        
        if no_meta_desc > 0:
            recommendations.append({
                'category': 'Meta Descriptions',
                'priority': 'Alta',
                'action': f'Agregar meta descriptions a {no_meta_desc} pÃ¡ginas',
                'details': 'Las meta descriptions mejoran el CTR en resultados de bÃºsqueda'
            })
            
        # Content analysis
        thin_content = len([c for c in content_data if c.get('word_count', 0) < 300])
        if thin_content > 0:
            recommendations.append({
                'category': 'Contenido',
                'priority': 'Alta',
                'action': f'Expandir contenido de {thin_content} pÃ¡ginas con menos de 300 palabras',
                'details': 'Contenido mÃ¡s extenso y valioso mejora el posicionamiento'
            })
        
        # Keyword opportunities
        top_keywords = keyword_analysis.get('top_keywords', [])[:10]
        if top_keywords:
            keyword_list = [f"{kw[0]} ({kw[1]})" for kw in top_keywords]
            recommendations.append({
                'category': 'Palabras Clave',
                'priority': 'Media',
                'action': 'Optimizar contenido para palabras clave principales identificadas',
                'details': f"Keywords principales: {', '.join(keyword_list[:5])}"
            })
        
        # Internal linking
        avg_links = sum([len(c.get('links', [])) for c in content_data]) / pages_analyzed if pages_analyzed > 0 else 0
        if avg_links < 3:
            recommendations.append({
                'category': 'Enlaces Internos',
                'priority': 'Media',
                'action': 'Mejorar estructura de enlaces internos',
                'details': f'Promedio actual: {avg_links:.1f} enlaces por pÃ¡gina. Objetivo: 3-10 enlaces relevantes'
            })
        
        return recommendations

    def generate_report(self, content_data, keyword_analysis, recommendations, output_file):
        """Generate markdown SEO report"""
        report = f"""# Plan de AcciÃ³n SEO - {datetime.now().strftime('%Y-%m-%d')}

## Resumen Ejecutivo

### PÃ¡ginas Analizadas
- **Total de URLs procesadas**: {len(content_data)}
- **PÃ¡ginas analizadas exitosamente**: {len([c for c in content_data if c.get('status') != 'failed'])}
- **Errores de acceso**: {len([c for c in content_data if c.get('status') == 'failed'])}

### Palabras Clave Principales
"""
        
        top_keywords = keyword_analysis.get('top_keywords', [])[:15]
        for i, (keyword, count) in enumerate(top_keywords, 1):
            report += f"{i}. **{keyword}** ({count} menciones)\n"
        
        report += "\n### Frases Relevantes\n"
        top_phrases = keyword_analysis.get('top_phrases', [])[:10]
        for i, (phrase, count) in enumerate(top_phrases, 1):
            report += f"{i}. \"{phrase}\" ({count} menciones)\n"
        
        report += "\n## Recomendaciones de AcciÃ³n\n\n"
        
        # Group recommendations by priority
        high_priority = [r for r in recommendations if r['priority'] == 'Alta']
        medium_priority = [r for r in recommendations if r['priority'] == 'Media']
        low_priority = [r for r in recommendations if r['priority'] == 'Baja']
        
        if high_priority:
            report += "### ðŸ”´ Prioridad Alta (Implementar Inmediatamente)\n\n"
            for i, rec in enumerate(high_priority, 1):
                report += f"**{i}. {rec['category']}: {rec['action']}**\n"
                report += f"- *Detalles*: {rec['details']}\n"
                report += f"- *Impacto*: Alto impacto en SEO\n"
                report += f"- *Esfuerzo*: TÃ©cnico\n\n"
        
        if medium_priority:
            report += "### ðŸŸ¡ Prioridad Media (Implementar en 2-4 semanas)\n\n"
            for i, rec in enumerate(medium_priority, 1):
                report += f"**{i}. {rec['category']}: {rec['action']}**\n"
                report += f"- *Detalles*: {rec['details']}\n"
                report += f"- *Impacto*: Medio impacto en SEO\n"
                report += f"- *Esfuerzo*: Moderado\n\n"
        
        if low_priority:
            report += "### ðŸŸ¢ Prioridad Baja (Implementar en 1-2 meses)\n\n"
            for i, rec in enumerate(low_priority, 1):
                report += f"**{i}. {rec['category']}: {rec['action']}**\n"
                report += f"- *Detalles*: {rec['details']}\n"
                report += f"- *Impacto*: Bajo impacto en SEO\n"
                report += f"- *Esfuerzo*: MÃ­nimo\n\n"
        
        report += """## Estrategia de Link Building

### 1. Enlaces Internos
- Crear una arquitectura de enlaces internos coherente
- Utilizar anchor text descriptivo con palabras clave relevantes
- Enlazar pÃ¡ginas relacionadas temÃ¡ticamente
- Implementar breadcrumbs para mejorar la navegaciÃ³n

### 2. Contenido de Calidad
- Crear contenido valioso que genere enlaces naturales
- Desarrollar guÃ­as completas sobre temas de nicho
- Publicar estudios de caso y anÃ¡lisis tÃ©cnicos
- Mantener el contenido actualizado y relevante

### 3. Outreach y Relaciones PÃºblicas
- Identificar sitios web relevantes en el sector
- Contactar con bloggers y periodistas del sector
- Participar en foros y comunidades especializadas
- Colaborar con influencers del sector

### 4. Directorios y Listados
- Registrarse en directorios de calidad del sector
- Completar perfiles en plataformas B2B relevantes
- Mantener informaciÃ³n consistente (NAP) en todos los listados

## MÃ©tricas de Seguimiento

### KPIs Principales
1. **Posiciones de palabras clave objetivo**
2. **TrÃ¡fico orgÃ¡nico mensual**
3. **NÃºmero de pÃ¡ginas indexadas**
4. **Tiempo de permanencia y bounce rate**
5. **NÃºmero y calidad de enlaces entrantes**
6. **Conversiones desde trÃ¡fico orgÃ¡nico**

### Herramientas Recomendadas
- Google Search Console (imprescindible)
- Google Analytics 4
- SEMrush o Ahrefs (anÃ¡lisis de competencia)
- Screaming Frog (auditorÃ­as tÃ©cnicas)

## Cronograma de ImplementaciÃ³n

### Semana 1-2: Optimizaciones CrÃ­ticas
- Corregir tÃ­tulos y meta descriptions faltantes
- Solucionar errores tÃ©cnicos identificados
- Implementar estructura de datos

### Semana 3-4: Mejoras de Contenido  
- Expandir pÃ¡ginas con contenido delgado
- Optimizar contenido existente para palabras clave
- Mejorar estructura de headings

### Mes 2: Estrategia de Enlaces
- Implementar estrategia de enlaces internos
- Inicio de campaÃ±a de outreach
- Crear contenido linkable

### Mes 3+: OptimizaciÃ³n Continua
- Monitoreo y ajuste de estrategias
- AnÃ¡lisis de resultados y iteraciÃ³n
- ExpansiÃ³n de contenido basada en datos

---
*Reporte generado automÃ¡ticamente el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"Report saved to: {output_file}")
        except Exception as e:
            logger.error(f"Error saving report: {e}")

    def auto_adjust_columns(self, worksheet, max_width=80):
        """Safely auto-adjust column widths, handling merged cells"""
        try:
            for column in worksheet.columns:
                max_length = 0
                # Get column letter from first non-merged cell
                column_letter = None

                for cell in column:
                    # Skip merged cells
                    if isinstance(cell, MergedCell):
                        continue

                    # Get column letter from first valid cell
                    if column_letter is None:
                        column_letter = cell.column_letter

                    # Calculate max length
                    try:
                        cell_length = len(str(cell.value)) if cell.value is not None else 0
                        if cell_length > max_length:
                            max_length = cell_length
                    except:
                        pass

                # Apply width adjustment if we found a valid column letter
                if column_letter and max_length > 0:
                    adjusted_width = min(max_length + 2, max_width)
                    worksheet.column_dimensions[column_letter].width = adjusted_width

        except Exception as e:
            logger.warning(f"Could not auto-adjust columns: {e}")

    def generate_excel_report(self, content_data, keyword_analysis, recommendations, output_file):
        """Generate comprehensive Excel SEO report with multiple sheets"""
        try:
            wb = Workbook()

            # Remove default sheet
            if "Sheet" in wb.sheetnames:
                wb.remove(wb["Sheet"])

            # Define styles
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            subheader_font = Font(bold=True, color="000000")
            subheader_fill = PatternFill(start_color="DCE6F1", end_color="DCE6F1", fill_type="solid")
            center_alignment = Alignment(horizontal="center", vertical="center")
            wrap_alignment = Alignment(horizontal="left", vertical="top", wrap_text=True)
            border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )

            # 1. RESUMEN EJECUTIVO SHEET
            ws_summary = wb.create_sheet("Resumen Ejecutivo")

            # Title
            ws_summary.merge_cells('A1:F1')
            ws_summary['A1'] = f"ANÃLISIS SEO - {datetime.now().strftime('%Y-%m-%d')}"
            ws_summary['A1'].font = Font(size=16, bold=True)
            ws_summary['A1'].alignment = center_alignment

            # Summary stats
            successful_pages = len([c for c in content_data if c.get('status') != 'failed'])
            failed_pages = len([c for c in content_data if c.get('status') == 'failed'])

            row = 3
            summary_data = [
                ["MÃ©trica", "Valor"],
                ["Total URLs procesadas", len(content_data)],
                ["PÃ¡ginas analizadas exitosamente", successful_pages],
                ["Errores de acceso", failed_pages],
                ["Porcentaje de Ã©xito", f"{(successful_pages/len(content_data)*100):.1f}%" if content_data else "0%"],
                ["Palabras clave Ãºnicas identificadas", len(keyword_analysis.get('top_keywords', []))],
                ["Recomendaciones generadas", len(recommendations)]
            ]

            for row_data in summary_data:
                for col, value in enumerate(row_data, 1):
                    cell = ws_summary.cell(row=row, column=col, value=value)
                    if row == 3:  # Header row
                        cell.font = header_font
                        cell.fill = header_fill
                    cell.alignment = center_alignment
                    cell.border = border
                row += 1

            # Auto-adjust column widths
            self.auto_adjust_columns(ws_summary, max_width=50)

            # 2. ANÃLISIS DE PÃGINAS SHEET
            ws_pages = wb.create_sheet("AnÃ¡lisis de PÃ¡ginas")

            # Headers
            page_headers = [
                "URL", "Estado", "TÃ­tulo", "Longitud TÃ­tulo", "Meta Description",
                "Longitud Meta Desc", "Palabras", "Enlaces Internos", "H1", "H2", "H3"
            ]

            for col, header in enumerate(page_headers, 1):
                cell = ws_pages.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            # Page data
            for row, content in enumerate(content_data, 2):
                if content.get('status') == 'failed':
                    status = "ERROR"
                    title = content.get('error', 'Error desconocido')
                    title_length = 0
                    meta_desc = ""
                    meta_length = 0
                    word_count = 0
                    links_count = 0
                    h1_count = 0
                    h2_count = 0
                    h3_count = 0
                else:
                    status = "OK"
                    title = content.get('title', '')
                    title_length = len(title)
                    meta_desc = content.get('meta_description', '')
                    meta_length = len(meta_desc)
                    word_count = content.get('word_count', 0)
                    links_count = len(content.get('links', []))
                    headings = content.get('headings', {})
                    h1_count = len(headings.get('h1', []))
                    h2_count = len(headings.get('h2', []))
                    h3_count = len(headings.get('h3', []))

                row_data = [
                    content.get('url', ''),
                    status,
                    title,
                    title_length,
                    meta_desc,
                    meta_length,
                    word_count,
                    links_count,
                    h1_count,
                    h2_count,
                    h3_count
                ]

                for col, value in enumerate(row_data, 1):
                    cell = ws_pages.cell(row=row, column=col, value=value)
                    cell.border = border
                    if col in [1, 3, 5]:  # URL, Title, Meta Description columns
                        cell.alignment = wrap_alignment
                    else:
                        cell.alignment = center_alignment

                    # Color coding for issues
                    if col == 2 and value == "ERROR":  # Status column
                        cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                    elif col == 4:  # Title length
                        if value == 0:
                            cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                        elif value < 30 or value > 60:
                            cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                    elif col == 6:  # Meta description length
                        if value == 0:
                            cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                        elif value < 120 or value > 160:
                            cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                    elif col == 7 and value < 300:  # Word count
                        cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")

            # Auto-adjust column widths
            self.auto_adjust_columns(ws_pages, max_width=80)

            # 3. PALABRAS CLAVE SHEET
            ws_keywords = wb.create_sheet("Palabras Clave")

            # Headers
            kw_headers = ["PosiciÃ³n", "Palabra Clave", "Frecuencia", "Relevancia"]
            for col, header in enumerate(kw_headers, 1):
                cell = ws_keywords.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            # Keyword data
            top_keywords = keyword_analysis.get('top_keywords', [])[:50]
            for row, (keyword, count) in enumerate(top_keywords, 2):
                relevance = "Alta" if count >= 10 else "Media" if count >= 5 else "Baja"
                row_data = [row-1, keyword, count, relevance]

                for col, value in enumerate(row_data, 1):
                    cell = ws_keywords.cell(row=row, column=col, value=value)
                    cell.border = border
                    cell.alignment = center_alignment

                    # Color coding by relevance
                    if col == 4:  # Relevance column
                        if value == "Alta":
                            cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
                        elif value == "Media":
                            cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                        else:
                            cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")

            # Auto-adjust column widths
            self.auto_adjust_columns(ws_keywords, max_width=50)

            # 4. FRASES RELEVANTES SHEET
            ws_phrases = wb.create_sheet("Frases Relevantes")

            # Headers
            phrase_headers = ["PosiciÃ³n", "Frase", "Frecuencia", "Longitud"]
            for col, header in enumerate(phrase_headers, 1):
                cell = ws_phrases.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            # Phrase data
            top_phrases = keyword_analysis.get('top_phrases', [])[:30]
            for row, (phrase, count) in enumerate(top_phrases, 2):
                row_data = [row-1, phrase, count, len(phrase.split())]

                for col, value in enumerate(row_data, 1):
                    cell = ws_phrases.cell(row=row, column=col, value=value)
                    cell.border = border
                    if col == 2:  # Phrase column
                        cell.alignment = wrap_alignment
                    else:
                        cell.alignment = center_alignment

            # Auto-adjust column widths
            self.auto_adjust_columns(ws_phrases, max_width=60)

            # 5. RECOMENDACIONES SHEET
            ws_recommendations = wb.create_sheet("Recomendaciones")

            # Headers
            rec_headers = ["Prioridad", "CategorÃ­a", "AcciÃ³n Requerida", "Detalles", "Impacto Estimado"]
            for col, header in enumerate(rec_headers, 1):
                cell = ws_recommendations.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            # Recommendations data
            for row, rec in enumerate(recommendations, 2):
                impact = "Alto" if rec['priority'] == 'Alta' else "Medio" if rec['priority'] == 'Media' else "Bajo"
                row_data = [
                    rec['priority'],
                    rec['category'],
                    rec['action'],
                    rec['details'],
                    impact
                ]

                for col, value in enumerate(row_data, 1):
                    cell = ws_recommendations.cell(row=row, column=col, value=value)
                    cell.border = border
                    if col in [3, 4]:  # Action and Details columns
                        cell.alignment = wrap_alignment
                    else:
                        cell.alignment = center_alignment

                    # Color coding by priority
                    if col == 1:  # Priority column
                        if value == "Alta":
                            cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                        elif value == "Media":
                            cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                        else:
                            cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")

            # Auto-adjust column widths
            self.auto_adjust_columns(ws_recommendations, max_width=80)

            # 6. PROBLEMAS TÃ‰CNICOS SHEET
            ws_issues = wb.create_sheet("Problemas TÃ©cnicos")

            # Headers
            issue_headers = ["URL", "Problema", "Tipo", "Severidad", "SoluciÃ³n Recomendada"]
            for col, header in enumerate(issue_headers, 1):
                cell = ws_issues.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = center_alignment
                cell.border = border

            # Technical issues data
            row = 2
            for content in content_data:
                url = content.get('url', '')

                # Check for various technical issues
                issues = []

                if content.get('status') == 'failed':
                    issues.append({
                        'problema': 'Error de acceso',
                        'tipo': 'TÃ©cnico',
                        'severidad': 'Alta',
                        'solucion': 'Verificar URL y accesibilidad del servidor'
                    })
                else:
                    # Title issues
                    title = content.get('title', '')
                    if not title:
                        issues.append({
                            'problema': 'TÃ­tulo faltante',
                            'tipo': 'SEO',
                            'severidad': 'Alta',
                            'solucion': 'Agregar tÃ­tulo Ãºnico y descriptivo'
                        })
                    elif len(title) < 30:
                        issues.append({
                            'problema': 'TÃ­tulo muy corto',
                            'tipo': 'SEO',
                            'severidad': 'Media',
                            'solucion': 'Expandir tÃ­tulo a 30-60 caracteres'
                        })
                    elif len(title) > 60:
                        issues.append({
                            'problema': 'TÃ­tulo muy largo',
                            'tipo': 'SEO',
                            'severidad': 'Media',
                            'solucion': 'Reducir tÃ­tulo a mÃ¡ximo 60 caracteres'
                        })

                    # Meta description issues
                    meta_desc = content.get('meta_description', '')
                    if not meta_desc:
                        issues.append({
                            'problema': 'Meta description faltante',
                            'tipo': 'SEO',
                            'severidad': 'Alta',
                            'solucion': 'Agregar meta description de 120-160 caracteres'
                        })
                    elif len(meta_desc) < 120:
                        issues.append({
                            'problema': 'Meta description muy corta',
                            'tipo': 'SEO',
                            'severidad': 'Media',
                            'solucion': 'Expandir meta description a 120-160 caracteres'
                        })
                    elif len(meta_desc) > 160:
                        issues.append({
                            'problema': 'Meta description muy larga',
                            'tipo': 'SEO',
                            'severidad': 'Media',
                            'solucion': 'Reducir meta description a mÃ¡ximo 160 caracteres'
                        })

                    # Content issues
                    word_count = content.get('word_count', 0)
                    if word_count < 300:
                        issues.append({
                            'problema': 'Contenido escaso',
                            'tipo': 'Contenido',
                            'severidad': 'Media',
                            'solucion': 'Expandir contenido a mÃ­nimo 300 palabras'
                        })

                    # Heading issues
                    headings = content.get('headings', {})
                    if not headings.get('h1'):
                        issues.append({
                            'problema': 'H1 faltante',
                            'tipo': 'SEO',
                            'severidad': 'Alta',
                            'solucion': 'Agregar etiqueta H1 Ãºnica por pÃ¡gina'
                        })

                # Add issues to sheet
                for issue in issues:
                    row_data = [url, issue['problema'], issue['tipo'], issue['severidad'], issue['solucion']]

                    for col, value in enumerate(row_data, 1):
                        cell = ws_issues.cell(row=row, column=col, value=value)
                        cell.border = border
                        if col in [1, 5]:  # URL and Solution columns
                            cell.alignment = wrap_alignment
                        else:
                            cell.alignment = center_alignment

                        # Color coding by severity
                        if col == 4:  # Severity column
                            if value == "Alta":
                                cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                            elif value == "Media":
                                cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                            else:
                                cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")

                    row += 1

            # Auto-adjust column widths
            self.auto_adjust_columns(ws_issues, max_width=80)

            # Save the workbook
            wb.save(output_file)
            logger.info(f"Excel report saved to: {output_file}")

        except Exception as e:
            logger.error(f"Error generating Excel report: {e}")

    def run_analysis(self, sitemap_path=None, url_list_path=None, output_file="plan-de-accion-seo.md", excel_output=None):
        """Run complete SEO analysis"""
        urls = []

        # Collect URLs from sources
        if sitemap_path:
            urls.extend(self.parse_sitemap(sitemap_path))

        if url_list_path:
            urls.extend(self.load_url_list(url_list_path))

        if not urls:
            logger.error("No URLs found to analyze")
            return

        # Remove duplicates
        urls = list(set(urls))
        logger.info(f"Starting analysis of {len(urls)} unique URLs")

        # Setup domain-based cache
        if urls:
            self._setup_domain_cache(urls[0])

        # Clean expired cache (older than 24 hours) and show statistics
        self._clear_expired_cache()
        cache_stats = self._get_cache_stats()
        logger.info(f"ðŸ“Š Cache stats: {cache_stats['valid_domains']} valid domains, {cache_stats['old_domains']} old domains, {cache_stats['total_domains']} total")

        # Check if domain needs re-scraping (>= 60 minutes)
        needs_scraping = self._needs_rescraping()

        if not needs_scraping:
            # Cache reciente (< 60 min), mostrar info pero NO usar
            metadata_file = self._get_domain_cache_metadata_file()
            try:
                import json
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                    age_minutes = (datetime.now() - datetime.fromisoformat(metadata['last_scrape'])).total_seconds() / 60

                logger.info(f"")
                logger.info(f"{'='*60}")
                logger.info(f"âš¡ USING CACHED DATA - {self.current_domain}")
                logger.info(f"{'='*60}")
                logger.info(f"ðŸ“¦ Last scraped: {age_minutes:.1f} minutes ago")
                logger.info(f"ðŸ“„ Cached URLs: {metadata.get('url_count', 0)}")
                logger.info(f"â±ï¸  Cache valid for: {self.cache_rescrape_minutes - age_minutes:.0f} more minutes")
                logger.info(f"{'='*60}")
                logger.info(f"")
                logger.info(f"âš¡ Loading data from cache to generate Excel report...")

                # Load all cached data instead of scraping
                for i, url in enumerate(urls, 1):
                    logger.info(f"ðŸ“¦ Loading from cache {i}/{len(urls)}: {url}")
                    content = self.extract_content(url)  # Will use cache automatically
                    if content:
                        self.content_data.append(content)

                logger.info(f"âœ… Loaded {len(self.content_data)} pages from cache")
                logger.info(f"")

                # Continue to generate Excel with cached data (don't return here)
                needs_scraping = False
            except:
                needs_scraping = True

        # Si necesitamos scrapear (cachÃ© expirado o no existe)
        if needs_scraping:
            logger.info(f"ðŸŒ Starting fresh scraping for {self.current_domain} (required every 60 minutes)")

            # Extract content from URLs (using cache when available)
            cached_count = 0
            scraped_count = 0

            for i, url in enumerate(urls, 1):
                logger.info(f"Processing URL {i}/{len(urls)}: {url}")

                # Check if coming from cache
                is_cached = self._load_from_cache(url) is not None

                content = self.extract_content(url)
                self.content_data.append(content)

                if is_cached:
                    cached_count += 1
                else:
                    scraped_count += 1

                if self.delay > 0 and not is_cached:  # No delay for cached content
                    time.sleep(self.delay)

            # Save domain cache metadata after scraping
            if scraped_count > 0:
                self._save_domain_cache_metadata(len(urls))
        else:
            # Already loaded from cache above
            cached_count = len(self.content_data)
            scraped_count = 0

        # Show cache performance summary
        logger.info(f"")
        logger.info(f"{'='*60}")
        logger.info(f"ðŸ“ˆ CACHE PERFORMANCE SUMMARY - {self.current_domain}")
        logger.info(f"{'='*60}")
        logger.info(f"âœ… URLs loaded from cache: {cached_count} ({cached_count/len(urls)*100:.1f}%)")
        logger.info(f"ðŸŒ URLs scraped fresh: {scraped_count} ({scraped_count/len(urls)*100:.1f}%)")
        logger.info(f"âš¡ Time saved: ~{cached_count * 2:.0f} seconds (approx)")
        logger.info(f"ðŸ’¡ Next scrape required in: {self.cache_rescrape_minutes} minutes")
        logger.info(f"ðŸ—‘ï¸  Files auto-delete after: {self.cache_cleanup_hours} hours")
        logger.info(f"{'='*60}")
        logger.info(f"")

        # Analyze keywords
        keyword_analysis = self.analyze_keywords(self.content_data)

        # Generate recommendations
        recommendations = self.generate_seo_recommendations(self.content_data, keyword_analysis)

        # Generate markdown report
        self.generate_report(self.content_data, keyword_analysis, recommendations, output_file)

        # Generate Excel report if requested
        if excel_output:
            self.generate_excel_report(self.content_data, keyword_analysis, recommendations, excel_output)
            logger.info(f"Analysis complete! Reports saved to: {output_file} and {excel_output}")
        else:
            logger.info(f"Analysis complete! Report saved to: {output_file}")

def main():
    """Main function with dual operation mode support"""
    parser = argparse.ArgumentParser(description='SEO Content Generator - Advanced SEO Analysis and Content Creation')
    parser.add_argument('--brief', help='Path to business brief file (.txt, .docx, .pdf) for new site mode')
    parser.add_argument('--sitemap', help='Path to sitemap.xml file or URL for existing site mode')
    parser.add_argument('--urls', help='Path to text file with URLs to analyze (existing site mode)')
    parser.add_argument('--output', default='brief-seo-completo.md', help='Output markdown file name')
    parser.add_argument('--excel', default='analisis-seo-completo.xlsx', help='Output Excel file name (.xlsx)')
    parser.add_argument('--delay', type=float, default=1.0, help='Delay between requests (seconds)')
    parser.add_argument('--mode', choices=['new', 'existing'], help='Operation mode: new (brief) or existing (sitemap)')
    parser.add_argument('--clear-cache', action='store_true', help='Clear all cached data before analysis')
    parser.add_argument('--cache-duration', type=int, default=60, help='Cache duration in minutes (default: 60)')
    parser.add_argument('--phase', type=int, choices=[1, 2], help='Execution phase: 1=Generate basic Excel (user fills keywords), 2=Generate AI content using user-defined keywords')

    args = parser.parse_args()

    generator = SEOContentGenerator(delay=args.delay)

    # Set cache duration from arguments
    generator.cache_duration_minutes = args.cache_duration

    # Clear cache if requested
    if args.clear_cache:
        import shutil
        if generator.cache_dir.exists():
            shutil.rmtree(generator.cache_dir)
            generator.cache_dir.mkdir(exist_ok=True)
            logger.info("ðŸ—‘ï¸  Cache cleared successfully")

    try:
        # Check if using 2-phase workflow
        if args.phase:
            if args.phase == 1:
                # PHASE 1: Generate basic Excel without AI recommendations
                print(f"\nðŸ“Š FASE 1: GENERACIÃ“N EXCEL BÃSICO")
                print("="*60)
                print("ðŸ“ Se generarÃ¡ Excel con datos extraÃ­dos del sitio web")
                print("âš ï¸  La columna 'Palabra clave principal' quedarÃ¡ VACÃA para que la completes manualmente")
                print("âš ï¸  Las columnas de recomendaciones IA quedarÃ¡n VACÃAS")
                print("")

                if not args.sitemap and not args.urls:
                    print("âŒ Error: Para Fase 1 debe proporcionar --sitemap o --urls")
                    return

                # Run analysis and scraping
                generator.analyze_website_from_sitemap(
                    args.sitemap,
                    args.urls,
                    args.output,
                    args.excel
                )

                # Generate Phase 1 Excel (without AI recommendations)
                generator.generate_phase1_excel(args.excel)

                print(f"\nâœ… FASE 1 COMPLETADA")
                print("="*60)
                print(f"ðŸ“Š Excel bÃ¡sico generado: {args.excel}")
                print("")
                print("ðŸ“ PRÃ“XIMOS PASOS:")
                print("   1. Abre el archivo Excel generado")
                print("   2. Edita la columna 7 'Palabra clave principal' en hojas Blog y SEO On-Page")
                print("   3. Define keywords especÃ­ficas (ej: 'fisioterapia deportiva Madrid')")
                print("   4. Guarda el archivo Excel")
                print("")
                print("ðŸš€ EJECUTA FASE 2:")
                print(f"   python seo_analyzer.py --excel {args.excel} --phase 2")
                print("")
                return

            elif args.phase == 2:
                # PHASE 2: Read Excel with user-defined keywords and generate AI content
                print(f"\nðŸ¤– FASE 2: GENERACIÃ“N CONTENIDO IA")
                print("="*60)

                if not args.excel:
                    print("âŒ Error: Para Fase 2 debe proporcionar --excel con el archivo de Fase 1")
                    return

                if not os.path.exists(args.excel):
                    print(f"âŒ Error: Archivo Excel no encontrado: {args.excel}")
                    print(f"   Verifica que el archivo existe y la ruta es correcta")
                    return

                # Generate Phase 2 Excel (AI recommendations based on user keywords)
                generator.generate_phase2_excel(args.excel)

                print(f"\nâœ… FASE 2 COMPLETADA")
                print("="*60)
                print(f"ðŸ“Š Excel con recomendaciones IA: {args.excel}")
                print("")
                return

        # Original workflow (non-phased)
        # Determine operation mode
        if args.mode:
            mode = 'new_site' if args.mode == 'new' else 'existing_site'
        elif args.brief:
            mode = 'new_site'
        elif args.sitemap or args.urls:
            mode = 'existing_site'
        else:
            # Interactive mode selection
            mode = generator.select_operation_mode()

        if mode == 'new_site':
            # New Site Mode - Process brief and generate content
            print(f"\nðŸš€ MODO: SITIO NUEVO")
            print("="*60)

            # Process brief file
            brief_content = generator.process_brief_file(args.brief)
            if not brief_content:
                return

            # Validate and complete brief
            generator.validate_brief_completeness()

            # Extract keywords for competitive research
            activity = generator.business_data.get('general_info', {}).get('actividad_principal', '')
            location = generator.business_data.get('general_info', {}).get('cobertura_geografica', '')

            research_keywords = [activity]
            if location:
                research_keywords.append(f"{activity} {location}")

            # Perform competitive research
            if research_keywords[0]:  # Only if we have business activity
                generator.search_competitors(research_keywords)
                generator.analyze_serp_patterns()

            # Generate SEO content
            generator.generate_seo_content()

            # Generate outputs
            generator.generate_complete_brief_markdown(args.output)
            generator.generate_expanded_excel_report(args.excel)

            print(f"\nðŸŽ‰ PROCESO COMPLETADO")
            print(f"ðŸ“ Brief completo: {args.output}")
            print(f"ðŸ“Š AnÃ¡lisis Excel: {args.excel}")

        else:
            # Existing Site Mode - Enhanced analysis
            print(f"\nðŸ” MODO: SITIO EXISTENTE")
            print("="*60)

            if not args.sitemap and not args.urls:
                print("âŒ Error: Para sitio existente debe proporcionar --sitemap o --urls")
                return

            # Run enhanced analysis
            generator.run_enhanced_analysis(
                sitemap_path=args.sitemap,
                url_list_path=args.urls,
                output_file=args.output,
                excel_output=args.excel
            )

    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ Proceso cancelado por el usuario.")
    except Exception as e:
        logger.error(f"Error during execution: {e}")
        print(f"\nâŒ Error durante la ejecuciÃ³n: {e}")
    finally:
        # Close WebDriver if initialized
        if generator.driver:
            try:
                generator.driver.quit()
                logger.info("WebDriver closed successfully")
            except:
                pass

if __name__ == "__main__":
    main()